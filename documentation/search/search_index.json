{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"AI Software Development Kit AI Software Development Kit is a Python library that allows you to create, package, and test AI inference pipelines for the AI Inference Server. The AI SDK is accompanied by project templates that provide notebook-based workflows for training models, packaging them for deployment and testing these packages. Some of the AI SDK functionalities require pip version 21.3.1 or greater, so installing the library will upgrade your pip in the given Python environment.","title":"Home"},{"location":"index.html#ai-software-development-kit","text":"AI Software Development Kit is a Python library that allows you to create, package, and test AI inference pipelines for the AI Inference Server. The AI SDK is accompanied by project templates that provide notebook-based workflows for training models, packaging them for deployment and testing these packages. Some of the AI SDK functionalities require pip version 21.3.1 or greater, so installing the library will upgrade your pip in the given Python environment.","title":"AI Software Development Kit"},{"location":"CHANGELOG.html","text":"Version History AI Software Development Kit Known issues: Python 3.8.10 is the final regular bugfix release of Python 3.8 with binary installers. We recommend you to use the most recent bugfix release of Python 3.8 for productive use. For non-productive use, you can attempt using AI SDK with Python 3.8.10. AI SDK has only been tested on 64-bit platforms. We do not recommend using AI SDK on 32-bit platforms. The local pipeline runner might exceed the maximum path length allowed on Windows by default. To resolve this, please see the following article: https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry As no TensorFlow Lite 2.7.0 installer was published for Windows systems, you cannot use the local pipeline runner on Windows to execute the TensorFlow Lite based pipeline packages, like the one provided in the Image Classification project template. Markuppy is a new dependency in AI SDK 1.4.1 which is available as a source only wheel. As a consequence, you cannot simply include AI SDK 1.4.1 in a pipeline package, like you could in previous versions of AI SDK. As a workaround, you can include earlier version of AI SDK or include a manually created wheel of Markuppy along with AI SDK 1.4.1 in the pipeline package. Python 3.7.x \u2264 3.11.2 - Remote Security Bypass Vulnerability - CVE-2023-24329 - AI SDK is not using blocklisting and hence is not affected 2.4.0 New features: New Class ComponentRunner introduced for testing Component object before packaging them. _package_component_dependencies in deployment.py returns a set containing (name,version) tuples of all the dependencies. Pipeline.export(...) generates a report md file with pipeline information, structure, direct and transitive dependencies, package vulnerabilities and warnings. The report file is placed next to the final generated zip package. LocalPipelineRunner generates a report md file with the execution information, zip folder structure, list of python packages installed, input/output payload counts, and warnings. The report file is placed next to the package. Error is raised when pipeline input variable names are present also in pipeline outputs. VCAStream class can convert a folder of images into ImageSet input for LocalPipelineRunner TimeSeriesStream class can feed a csv file as input into LocalPipelineRunner Dependencies with inline URLs in requirements.txt are downloaded and the URL is removed at package creation Deprecated features: - PythonComponent.add_resources(..) method does NOT exclude hidden files or files in hidden folder when packaging the Pipeline. Fixed issues: - Files from hidden folders can be added to PythonComponent resources, which were excluded in previous versions of AI SDK. - Fixing pip report.json character encoding incompatibility on Windows. - Python dependencies can be specified with URIs in the requirements.txt file and assigned to a Python Component. 2.3.0 New features: Optimizing dependency list to reduce package size Pipeline.export(...) checks the size of the final generated zip package, throws error if it exceeds the limit of 2.2 GB PythonComponent gives warning when TensorFlow dependency is detected Warning about unused dependencies during LocalPipelineRunner execution Incomplete input payload, output or metric does not throw AssertionError anymore; instead, a warning message will be issued about the missing variables Deprecated features: Fixed issues: - Python 3.10.x \u2264 3.10.14 - Multiple Vulnerabilities - 3.10.15 - Python 3.11.x \u2264 3.11.9 - Multiple Vulnerabilities - 3.11.10 2.2.0 New features: Adding new property of 'batch' to class 'Component' LocalPipelineRunner can be used to run pipeline components with batch input or output Enabled Python versions for using AI SDK is '>=3.10.0' . With Python 3.8, simaticai >=2.2.0 cannot be installed. Enabled Python versions for a PythonComponent is 3.10 and 3.11 . Telemetry data is saved in the edge package Deprecated features: Python 3.8 is removed from supported Python versions. Fixed issues: Typo in Boolean type name 2.1.0 New Features: TensorRTOptimization class is introduced ModelConfig class is updated to include TensorRTOptimization instance as optimization and to add the config.pbtxt of GPURuntimeComponent pipeline step LocalPipelineRunner is able to execute both pipeline configuration and edge packages LocalPipelineRunner gives warnings about output variables so mismatching variable names can be detected Pipeline.export(...) method, for directly generating an edge runtime package Deprecated features: Pipeline.save(...) and deployment.convert_package(...) should not be used separately, instead Pipeline.export(...) should be used. Fixed issues: Python 3.8.x \u2264 3.8.18 - Multiple Vulnerabilities - 3.8.19 Python 3.10.x \u2264 3.10.13 - Multiple Vulnerabilities - 3.10.14 Python Package: pip \u2264 23.2.1 - Local Security Bypass Vulnerability - 24.0 Python Package: idna < 3.7 - Local Denial of Service Vulnerability - GHSA-jjg7-2v4v-x38h Python Package: onnx \u2264 1.15.0 - Remote Path Traversal Vulnerability - CVE-2024-27318 Input variable 'timestamp' is not allowed for Components (Pipeline Steps) as the AI Inference Server always put the message creation timestamp into the payload with the reserved name 'timestamp. Same AssertionError is raised when the user tries to add an input variable with this name. GPURuntime.use_model(..) is able to handle onnx models with different name than 'model.onnx'. The model will be copied into the package with name 'model.onnx'. 2.0.0 New features: New GPURuntimeComponent introduced for supporting ONNX models running on GPU enabled AI Inference Server GPURuntimeComponent can be created with name and version ONNX model can be added via GPURuntimeComponent.use_model(..) Default config.pbtxt is created Specific config.pbtxt can be added via GPURuntimeComponent.use_config(..) LocalPipelineRunner is able to execute GPURuntimeComponent with run_component(..), and run_pipeline(..) The edge package's SHA256 hash is saved in a text file Warning message in case pipeline components use different Python versions Warning message in case no dependencies are added to the pipeline package Updated features: Component.add_metrics(..) is moved to PythonComponent.add_metrics(..), as GPURuntimeComponent is not able to produce a formatted output. Supported AI Inference Server variable Types are extended with \"UInt8Array\", \"UInt16Array\", \"UInt32Array\", \"UInt64Array\", \"Int8Array\", \"Int16Array\", \"Int32Array\", \"Int64Array\", \"Float16Array\", \"Float32Array\", \"Float64Array\", \"ImageSet\" Removed features: Removed function: simaticai.deployment.from_saved_model(). Removed module: simaticai.pipeline Removed module: progress.py Fixed issues: GPU runtime configuration option max_batch_size should behave the same for values 0 or 1. 1.6.0 New features: Mathilda packages pipeline with image input and output Support source only packages Security fixes Documentation Revamp 1.5.0 New features: Version pipeline packages with UUID Binary data type 1.4.1 New features: Define custom metrics for a pipeline. Enable adding a description for a pipeline and its elements. Support for binary output from a pipeline or pipeline component. Improved guideline for creating pipeline components. Deprecated features: Using function simaticai.deployment.from_saved_model() is deprecated. The file format is proprietary and will not be supported in future major versions of AI SDK. Please use explicit calls to add inputs, outputs and the model to the pipeline. Fixed issues: Unexpected scheme validation error during edge package creation convert_package() includes wheel not supported on AI Inference Server Python 3.8.x \u2264 3.8.15 - Remote Denial of Service Vulnerability - CVE-2022-45061 Python Package: certifi 2017.11.05 \u2264 2022.9.24 - Remote Improper Input Validation Vulnerability - GHSA-43fp-rhv2-5gv8 1.3.0 New features: Full PEP 508 support in requirements.txt Configure pipeline components to be executed in parallel in multiple instances. Deprecated features: Using module simaticai.pipeline is deprecated. Please use the source module pipeline provided as part of project template State Identifier. Using AI Model Deployer to convert pipeline configuration packages to edge configuration packages is deprecated. Fixed issues: Python 3.8.x \u2264 3.8.13 - Open Redirect Vulnerability Python 3.8.x \u2264 3.8.14 - Multiple Vulnerabilities Python Package: joblib \u2264 1.1.0 - Remote Code Execution Vulnerability 1.2.0 New features: Define and use pipeline parameters to adapt the behavior of a pipeline at runtime without redeployment. Create Delta Configuration Packages to reduce the upload time for small changes in the Pipeline Package. Streamline the entrypoint of the pipeline with the new entrypoint interface signature. Use input type Object to receive Vision Connector payload via high-performance ZMQ connection. Fixed issues: deployment.Component.add_dependencies() does not duplicate packages in requirements.txt with different letter cases. deployment.convert_package() method returns with pathlib.Path instead of string representation of the generated zip path. testing.pipeline_runner.LocalPipelineRunner and testing.pipeline_validator.validate_pipeline_dependencies() can correctly handle components without requirements.txt. 1.1.0 New features: Create Edge Configuration Package from a Pipeline Configuration Package. Added generated reference manual. Fixed issues: Error stack trace is now complete under Windows when using LocalPipelineRunner. deployment.find_dependencies() is not restricted anymore to versioning scheme X.Y.Z LocalPipelineRunner updates pip to required version in the test venv 1.0.0 Initial released version. Main features include: Basic scikit-learn pipeline elements for building machine learning models that process time series of aligned signals. Python API for packaging trained models into pipeline configuration packages to be executed on AI Inference Server. Supports pipeline configurations with multiple components running in isolated Python environments. Supports PEP 440 public version identifiers to specify required inference serving environment. Supports providing required Python packages as wheels. Python API to check availability of required Python packages for AI Inference Server. Python API to drive pipeline configuration packages with test data. Creates required inference serving environment automatically. Supports test driving the pipeline as a whole or component-by-component. Provides a local mock of the logging interface on AI Inference Server. API reference manual. Tested on Linux and Windows.","title":"Changelog"},{"location":"CHANGELOG.html#version-history","text":"AI Software Development Kit Known issues: Python 3.8.10 is the final regular bugfix release of Python 3.8 with binary installers. We recommend you to use the most recent bugfix release of Python 3.8 for productive use. For non-productive use, you can attempt using AI SDK with Python 3.8.10. AI SDK has only been tested on 64-bit platforms. We do not recommend using AI SDK on 32-bit platforms. The local pipeline runner might exceed the maximum path length allowed on Windows by default. To resolve this, please see the following article: https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry As no TensorFlow Lite 2.7.0 installer was published for Windows systems, you cannot use the local pipeline runner on Windows to execute the TensorFlow Lite based pipeline packages, like the one provided in the Image Classification project template. Markuppy is a new dependency in AI SDK 1.4.1 which is available as a source only wheel. As a consequence, you cannot simply include AI SDK 1.4.1 in a pipeline package, like you could in previous versions of AI SDK. As a workaround, you can include earlier version of AI SDK or include a manually created wheel of Markuppy along with AI SDK 1.4.1 in the pipeline package. Python 3.7.x \u2264 3.11.2 - Remote Security Bypass Vulnerability - CVE-2023-24329 - AI SDK is not using blocklisting and hence is not affected","title":"Version History"},{"location":"CHANGELOG.html#240","text":"New features: New Class ComponentRunner introduced for testing Component object before packaging them. _package_component_dependencies in deployment.py returns a set containing (name,version) tuples of all the dependencies. Pipeline.export(...) generates a report md file with pipeline information, structure, direct and transitive dependencies, package vulnerabilities and warnings. The report file is placed next to the final generated zip package. LocalPipelineRunner generates a report md file with the execution information, zip folder structure, list of python packages installed, input/output payload counts, and warnings. The report file is placed next to the package. Error is raised when pipeline input variable names are present also in pipeline outputs. VCAStream class can convert a folder of images into ImageSet input for LocalPipelineRunner TimeSeriesStream class can feed a csv file as input into LocalPipelineRunner Dependencies with inline URLs in requirements.txt are downloaded and the URL is removed at package creation Deprecated features: - PythonComponent.add_resources(..) method does NOT exclude hidden files or files in hidden folder when packaging the Pipeline. Fixed issues: - Files from hidden folders can be added to PythonComponent resources, which were excluded in previous versions of AI SDK. - Fixing pip report.json character encoding incompatibility on Windows. - Python dependencies can be specified with URIs in the requirements.txt file and assigned to a Python Component.","title":"2.4.0"},{"location":"CHANGELOG.html#230","text":"New features: Optimizing dependency list to reduce package size Pipeline.export(...) checks the size of the final generated zip package, throws error if it exceeds the limit of 2.2 GB PythonComponent gives warning when TensorFlow dependency is detected Warning about unused dependencies during LocalPipelineRunner execution Incomplete input payload, output or metric does not throw AssertionError anymore; instead, a warning message will be issued about the missing variables Deprecated features: Fixed issues: - Python 3.10.x \u2264 3.10.14 - Multiple Vulnerabilities - 3.10.15 - Python 3.11.x \u2264 3.11.9 - Multiple Vulnerabilities - 3.11.10","title":"2.3.0"},{"location":"CHANGELOG.html#220","text":"New features: Adding new property of 'batch' to class 'Component' LocalPipelineRunner can be used to run pipeline components with batch input or output Enabled Python versions for using AI SDK is '>=3.10.0' . With Python 3.8, simaticai >=2.2.0 cannot be installed. Enabled Python versions for a PythonComponent is 3.10 and 3.11 . Telemetry data is saved in the edge package Deprecated features: Python 3.8 is removed from supported Python versions. Fixed issues: Typo in Boolean type name","title":"2.2.0"},{"location":"CHANGELOG.html#210","text":"New Features: TensorRTOptimization class is introduced ModelConfig class is updated to include TensorRTOptimization instance as optimization and to add the config.pbtxt of GPURuntimeComponent pipeline step LocalPipelineRunner is able to execute both pipeline configuration and edge packages LocalPipelineRunner gives warnings about output variables so mismatching variable names can be detected Pipeline.export(...) method, for directly generating an edge runtime package Deprecated features: Pipeline.save(...) and deployment.convert_package(...) should not be used separately, instead Pipeline.export(...) should be used. Fixed issues: Python 3.8.x \u2264 3.8.18 - Multiple Vulnerabilities - 3.8.19 Python 3.10.x \u2264 3.10.13 - Multiple Vulnerabilities - 3.10.14 Python Package: pip \u2264 23.2.1 - Local Security Bypass Vulnerability - 24.0 Python Package: idna < 3.7 - Local Denial of Service Vulnerability - GHSA-jjg7-2v4v-x38h Python Package: onnx \u2264 1.15.0 - Remote Path Traversal Vulnerability - CVE-2024-27318 Input variable 'timestamp' is not allowed for Components (Pipeline Steps) as the AI Inference Server always put the message creation timestamp into the payload with the reserved name 'timestamp. Same AssertionError is raised when the user tries to add an input variable with this name. GPURuntime.use_model(..) is able to handle onnx models with different name than 'model.onnx'. The model will be copied into the package with name 'model.onnx'.","title":"2.1.0"},{"location":"CHANGELOG.html#200","text":"New features: New GPURuntimeComponent introduced for supporting ONNX models running on GPU enabled AI Inference Server GPURuntimeComponent can be created with name and version ONNX model can be added via GPURuntimeComponent.use_model(..) Default config.pbtxt is created Specific config.pbtxt can be added via GPURuntimeComponent.use_config(..) LocalPipelineRunner is able to execute GPURuntimeComponent with run_component(..), and run_pipeline(..) The edge package's SHA256 hash is saved in a text file Warning message in case pipeline components use different Python versions Warning message in case no dependencies are added to the pipeline package Updated features: Component.add_metrics(..) is moved to PythonComponent.add_metrics(..), as GPURuntimeComponent is not able to produce a formatted output. Supported AI Inference Server variable Types are extended with \"UInt8Array\", \"UInt16Array\", \"UInt32Array\", \"UInt64Array\", \"Int8Array\", \"Int16Array\", \"Int32Array\", \"Int64Array\", \"Float16Array\", \"Float32Array\", \"Float64Array\", \"ImageSet\" Removed features: Removed function: simaticai.deployment.from_saved_model(). Removed module: simaticai.pipeline Removed module: progress.py Fixed issues: GPU runtime configuration option max_batch_size should behave the same for values 0 or 1.","title":"2.0.0"},{"location":"CHANGELOG.html#160","text":"New features: Mathilda packages pipeline with image input and output Support source only packages Security fixes Documentation Revamp","title":"1.6.0"},{"location":"CHANGELOG.html#150","text":"New features: Version pipeline packages with UUID Binary data type","title":"1.5.0"},{"location":"CHANGELOG.html#141","text":"New features: Define custom metrics for a pipeline. Enable adding a description for a pipeline and its elements. Support for binary output from a pipeline or pipeline component. Improved guideline for creating pipeline components. Deprecated features: Using function simaticai.deployment.from_saved_model() is deprecated. The file format is proprietary and will not be supported in future major versions of AI SDK. Please use explicit calls to add inputs, outputs and the model to the pipeline. Fixed issues: Unexpected scheme validation error during edge package creation convert_package() includes wheel not supported on AI Inference Server Python 3.8.x \u2264 3.8.15 - Remote Denial of Service Vulnerability - CVE-2022-45061 Python Package: certifi 2017.11.05 \u2264 2022.9.24 - Remote Improper Input Validation Vulnerability - GHSA-43fp-rhv2-5gv8","title":"1.4.1"},{"location":"CHANGELOG.html#130","text":"New features: Full PEP 508 support in requirements.txt Configure pipeline components to be executed in parallel in multiple instances. Deprecated features: Using module simaticai.pipeline is deprecated. Please use the source module pipeline provided as part of project template State Identifier. Using AI Model Deployer to convert pipeline configuration packages to edge configuration packages is deprecated. Fixed issues: Python 3.8.x \u2264 3.8.13 - Open Redirect Vulnerability Python 3.8.x \u2264 3.8.14 - Multiple Vulnerabilities Python Package: joblib \u2264 1.1.0 - Remote Code Execution Vulnerability","title":"1.3.0"},{"location":"CHANGELOG.html#120","text":"New features: Define and use pipeline parameters to adapt the behavior of a pipeline at runtime without redeployment. Create Delta Configuration Packages to reduce the upload time for small changes in the Pipeline Package. Streamline the entrypoint of the pipeline with the new entrypoint interface signature. Use input type Object to receive Vision Connector payload via high-performance ZMQ connection. Fixed issues: deployment.Component.add_dependencies() does not duplicate packages in requirements.txt with different letter cases. deployment.convert_package() method returns with pathlib.Path instead of string representation of the generated zip path. testing.pipeline_runner.LocalPipelineRunner and testing.pipeline_validator.validate_pipeline_dependencies() can correctly handle components without requirements.txt.","title":"1.2.0"},{"location":"CHANGELOG.html#110","text":"New features: Create Edge Configuration Package from a Pipeline Configuration Package. Added generated reference manual. Fixed issues: Error stack trace is now complete under Windows when using LocalPipelineRunner. deployment.find_dependencies() is not restricted anymore to versioning scheme X.Y.Z LocalPipelineRunner updates pip to required version in the test venv","title":"1.1.0"},{"location":"CHANGELOG.html#100","text":"Initial released version. Main features include: Basic scikit-learn pipeline elements for building machine learning models that process time series of aligned signals. Python API for packaging trained models into pipeline configuration packages to be executed on AI Inference Server. Supports pipeline configurations with multiple components running in isolated Python environments. Supports PEP 440 public version identifiers to specify required inference serving environment. Supports providing required Python packages as wheels. Python API to check availability of required Python packages for AI Inference Server. Python API to drive pipeline configuration packages with test data. Creates required inference serving environment automatically. Supports test driving the pipeline as a whole or component-by-component. Provides a local mock of the logging interface on AI Inference Server. API reference manual. Tested on Linux and Windows.","title":"1.0.0"},{"location":"user_manual.html","text":"The AI SDK user manual is available at Docs for Industrial Operations X .","title":"User Manual"},{"location":"reference/log_module/index.html","text":"Module log_module Logging in AI Inference Server The AI Inference Server's Python runtime provides a logger module which is embedded in the runtime and not available as a separate Python module. The AI SDK's log_module mimics the behavior of this embedded runtime logger by offering the same methods backed by the standard Python logging framework. This module is not a real logging framework, it only exists to help you write code against the logging framework on the Edge device. Using the AI SDK's LocalPipelineRunner the log_module wheel is automatically provided during the test run. If you want to run your code containing calls to log_module without the LocalPipelineRunner, you have to install the log_module wheel in your local execution environment. You should not install log_module on the Edge device itself. The log level can be set through the environment variable LOGLEVEL. Possible values are: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL. Please note that this setting only applies for local testing. For running on the Edge device, you can set the log level in the AI Inference Server application. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" ## Logging in AI Inference Server The AI Inference Server's Python runtime provides a logger module which is embedded in the runtime and not available as a separate Python module. The AI SDK's log_module mimics the behavior of this embedded runtime logger by offering the same methods backed by the standard Python logging framework. This module is not a real logging framework, it only exists to help you write code against the logging framework on the Edge device. Using the AI SDK's LocalPipelineRunner the log_module wheel is automatically provided during the test run. If you want to run your code containing calls to log_module without the LocalPipelineRunner, you have to install the log_module wheel in your local execution environment. You should not install log_module on the Edge device itself. The log level can be set through the environment variable LOGLEVEL. Possible values are: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL. Please note that this setting only applies for local testing. For running on the Edge device, you can set the log level in the AI Inference Server application. \"\"\" from . mock_logger import LogModule __all__ = ( \"LogModule\" ,) Sub-modules log_module.mock_logger","title":"Index"},{"location":"reference/log_module/index.html#module-log_module","text":"","title":"Module log_module"},{"location":"reference/log_module/index.html#logging-in-ai-inference-server","text":"The AI Inference Server's Python runtime provides a logger module which is embedded in the runtime and not available as a separate Python module. The AI SDK's log_module mimics the behavior of this embedded runtime logger by offering the same methods backed by the standard Python logging framework. This module is not a real logging framework, it only exists to help you write code against the logging framework on the Edge device. Using the AI SDK's LocalPipelineRunner the log_module wheel is automatically provided during the test run. If you want to run your code containing calls to log_module without the LocalPipelineRunner, you have to install the log_module wheel in your local execution environment. You should not install log_module on the Edge device itself. The log level can be set through the environment variable LOGLEVEL. Possible values are: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL. Please note that this setting only applies for local testing. For running on the Edge device, you can set the log level in the AI Inference Server application. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" ## Logging in AI Inference Server The AI Inference Server's Python runtime provides a logger module which is embedded in the runtime and not available as a separate Python module. The AI SDK's log_module mimics the behavior of this embedded runtime logger by offering the same methods backed by the standard Python logging framework. This module is not a real logging framework, it only exists to help you write code against the logging framework on the Edge device. Using the AI SDK's LocalPipelineRunner the log_module wheel is automatically provided during the test run. If you want to run your code containing calls to log_module without the LocalPipelineRunner, you have to install the log_module wheel in your local execution environment. You should not install log_module on the Edge device itself. The log level can be set through the environment variable LOGLEVEL. Possible values are: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL. Please note that this setting only applies for local testing. For running on the Edge device, you can set the log level in the AI Inference Server application. \"\"\" from . mock_logger import LogModule __all__ = ( \"LogModule\" ,)","title":"Logging in AI Inference Server"},{"location":"reference/log_module/index.html#sub-modules","text":"log_module.mock_logger","title":"Sub-modules"},{"location":"reference/log_module/mock_logger.html","text":"Module log_module.mock_logger A logger facade for testing. This module can substitute the AI Inference Server's logger in development and testing environments. Each method prefixes the message with '[PY] ' and delegates to the standard Python logger. Limitation: The log methods can only accept a single string argument. As composition from multiple arguments is not supported on the Edge runtime, it is not supported in this module either. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" A logger facade for testing. This module can substitute the AI Inference Server's logger in development and testing environments. Each method prefixes the message with '[PY] ' and delegates to the standard Python logger. Limitation: The log methods can only accept a single string argument. As composition from multiple arguments is not supported on the Edge runtime, it is not supported in this module either. \"\"\" import logging import os class LogModule : \"\"\" Facade class for exposing the AI Inference Server's log methods. This class can be imported and used the same way as on the Edge device. Example usage:: from log_module import LogModule logger = LogModule() def run(data: str): logger.trace(\"trace from EMBEDDED Python\") logger.info(\"info from EMBEDDED Python\") logger.warning(\"warning from EMBEDDED Python\") logger.warn(\"warn from EMBEDDED Python\") logger.debug(\"debug from EMBEDDED Python\") logger.error(\"error from EMBEDDED Python\") logger.critical(\"critical from EMBEDDED Python\") return {\"ready\": False, \"output\": None} \"\"\" def __init__ ( self ): self . TRACE_LEVEL = 5 logging . addLevelName ( self . TRACE_LEVEL , \"TRACE\" ) loglevel = os . environ . get ( \"LOGLEVEL\" , \"DEBUG\" ) . upper () if loglevel not in [ \"TRACE\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ]: loglevel = \"DEBUG\" logging . basicConfig ( level = loglevel ) self . logger = logging . getLogger ( __name__ ) self . logger . setLevel ( loglevel ) def trace ( self , message ): \"\"\" Logs a message on TRACE level. Args: message (str): The log message \"\"\" if self . logger . isEnabledFor ( self . TRACE_LEVEL ): self . logger . log ( self . TRACE_LEVEL , _prefix ( message )) def debug ( self , message ): \"\"\" Logs a message on DEBUG level. Args: message (str): The log message \"\"\" self . logger . debug ( _prefix ( message )) def info ( self , message ): \"\"\" Logs a message on INFO level. Args: message (str): The log message \"\"\" self . logger . info ( _prefix ( message )) def warning ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . logger . warning ( _prefix ( message )) def warn ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . warning ( message ) def error ( self , message ): \"\"\" Logs a message on ERROR level. Args: message (str): The log message \"\"\" self . logger . error ( _prefix ( message )) def critical ( self , message ): \"\"\" Logs a message on CRITICAL level. Args: message (str): The log message \"\"\" self . logger . critical ( _prefix ( message )) def _prefix ( message ): return f \"[PY] { message } \" Classes LogModule Facade class for exposing the AI Inference Server's log methods. This class can be imported and used the same way as on the Edge device. Example usage:: from log_module import LogModule logger = LogModule () def run ( data : str ): logger . trace ( \"trace from EMBEDDED Python\" ) logger . info ( \"info from EMBEDDED Python\" ) logger . warning ( \"warning from EMBEDDED Python\" ) logger . warn ( \"warn from EMBEDDED Python\" ) logger . debug ( \"debug from EMBEDDED Python\" ) logger . error ( \"error from EMBEDDED Python\" ) logger . critical ( \"critical from EMBEDDED Python\" ) return { \"ready\" : False , \"output\" : None } class LogModule ( ) View Source class LogModule : \"\"\" Facade class for exposing the AI Inference Server's log methods. This class can be imported and used the same way as on the Edge device. Example usage:: from log_module import LogModule logger = LogModule() def run(data: str): logger.trace(\"trace from EMBEDDED Python\") logger.info(\"info from EMBEDDED Python\") logger.warning(\"warning from EMBEDDED Python\") logger.warn(\"warn from EMBEDDED Python\") logger.debug(\"debug from EMBEDDED Python\") logger.error(\"error from EMBEDDED Python\") logger.critical(\"critical from EMBEDDED Python\") return {\"ready\": False, \"output\": None} \"\"\" def __init__ ( self ): self . TRACE_LEVEL = 5 logging . addLevelName ( self . TRACE_LEVEL , \"TRACE\" ) loglevel = os . environ . get ( \"LOGLEVEL\" , \"DEBUG\" ) . upper () if loglevel not in [ \"TRACE\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ]: loglevel = \"DEBUG\" logging . basicConfig ( level = loglevel ) self . logger = logging . getLogger ( __name__ ) self . logger . setLevel ( loglevel ) def trace ( self , message ): \"\"\" Logs a message on TRACE level. Args: message (str): The log message \"\"\" if self . logger . isEnabledFor ( self . TRACE_LEVEL ): self . logger . log ( self . TRACE_LEVEL , _prefix ( message )) def debug ( self , message ): \"\"\" Logs a message on DEBUG level. Args: message (str): The log message \"\"\" self . logger . debug ( _prefix ( message )) def info ( self , message ): \"\"\" Logs a message on INFO level. Args: message (str): The log message \"\"\" self . logger . info ( _prefix ( message )) def warning ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . logger . warning ( _prefix ( message )) def warn ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . warning ( message ) def error ( self , message ): \"\"\" Logs a message on ERROR level. Args: message (str): The log message \"\"\" self . logger . error ( _prefix ( message )) def critical ( self , message ): \"\"\" Logs a message on CRITICAL level. Args: message (str): The log message \"\"\" self . logger . critical ( _prefix ( message )) Methods critical def critical ( self , message ) Logs a message on CRITICAL level. Parameters: Name Type Description Default message str The log message None View Source def critical(self, message): \"\"\" Logs a message on CRITICAL level. Args: message (str): The log message \"\"\" self.logger.critical(_prefix(message)) debug def debug ( self , message ) Logs a message on DEBUG level. Parameters: Name Type Description Default message str The log message None View Source def debug(self, message): \"\"\" Logs a message on DEBUG level. Args: message (str): The log message \"\"\" self.logger.debug(_prefix(message)) error def error ( self , message ) Logs a message on ERROR level. Parameters: Name Type Description Default message str The log message None View Source def error(self, message): \"\"\" Logs a message on ERROR level. Args: message (str): The log message \"\"\" self.logger.error(_prefix(message)) info def info ( self , message ) Logs a message on INFO level. Parameters: Name Type Description Default message str The log message None View Source def info(self, message): \"\"\" Logs a message on INFO level. Args: message (str): The log message \"\"\" self.logger.info(_prefix(message)) trace def trace ( self , message ) Logs a message on TRACE level. Parameters: Name Type Description Default message str The log message None View Source def trace(self, message): \"\"\" Logs a message on TRACE level. Args: message (str): The log message \"\"\" if self.logger.isEnabledFor(self.TRACE_LEVEL): self.logger.log(self.TRACE_LEVEL, _prefix(message)) warn def warn ( self , message ) Logs a message on WARNING level. Parameters: Name Type Description Default message str The log message None View Source def warn(self, message): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self.warning(message) warning def warning ( self , message ) Logs a message on WARNING level. Parameters: Name Type Description Default message str The log message None View Source def warning(self, message): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self.logger.warning(_prefix(message))","title":"Mock Logger"},{"location":"reference/log_module/mock_logger.html#module-log_modulemock_logger","text":"A logger facade for testing. This module can substitute the AI Inference Server's logger in development and testing environments. Each method prefixes the message with '[PY] ' and delegates to the standard Python logger. Limitation: The log methods can only accept a single string argument. As composition from multiple arguments is not supported on the Edge runtime, it is not supported in this module either. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" A logger facade for testing. This module can substitute the AI Inference Server's logger in development and testing environments. Each method prefixes the message with '[PY] ' and delegates to the standard Python logger. Limitation: The log methods can only accept a single string argument. As composition from multiple arguments is not supported on the Edge runtime, it is not supported in this module either. \"\"\" import logging import os class LogModule : \"\"\" Facade class for exposing the AI Inference Server's log methods. This class can be imported and used the same way as on the Edge device. Example usage:: from log_module import LogModule logger = LogModule() def run(data: str): logger.trace(\"trace from EMBEDDED Python\") logger.info(\"info from EMBEDDED Python\") logger.warning(\"warning from EMBEDDED Python\") logger.warn(\"warn from EMBEDDED Python\") logger.debug(\"debug from EMBEDDED Python\") logger.error(\"error from EMBEDDED Python\") logger.critical(\"critical from EMBEDDED Python\") return {\"ready\": False, \"output\": None} \"\"\" def __init__ ( self ): self . TRACE_LEVEL = 5 logging . addLevelName ( self . TRACE_LEVEL , \"TRACE\" ) loglevel = os . environ . get ( \"LOGLEVEL\" , \"DEBUG\" ) . upper () if loglevel not in [ \"TRACE\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ]: loglevel = \"DEBUG\" logging . basicConfig ( level = loglevel ) self . logger = logging . getLogger ( __name__ ) self . logger . setLevel ( loglevel ) def trace ( self , message ): \"\"\" Logs a message on TRACE level. Args: message (str): The log message \"\"\" if self . logger . isEnabledFor ( self . TRACE_LEVEL ): self . logger . log ( self . TRACE_LEVEL , _prefix ( message )) def debug ( self , message ): \"\"\" Logs a message on DEBUG level. Args: message (str): The log message \"\"\" self . logger . debug ( _prefix ( message )) def info ( self , message ): \"\"\" Logs a message on INFO level. Args: message (str): The log message \"\"\" self . logger . info ( _prefix ( message )) def warning ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . logger . warning ( _prefix ( message )) def warn ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . warning ( message ) def error ( self , message ): \"\"\" Logs a message on ERROR level. Args: message (str): The log message \"\"\" self . logger . error ( _prefix ( message )) def critical ( self , message ): \"\"\" Logs a message on CRITICAL level. Args: message (str): The log message \"\"\" self . logger . critical ( _prefix ( message )) def _prefix ( message ): return f \"[PY] { message } \"","title":"Module log_module.mock_logger"},{"location":"reference/log_module/mock_logger.html#classes","text":"","title":"Classes"},{"location":"reference/log_module/mock_logger.html#logmodule","text":"Facade class for exposing the AI Inference Server's log methods. This class can be imported and used the same way as on the Edge device. Example usage:: from log_module import LogModule logger = LogModule () def run ( data : str ): logger . trace ( \"trace from EMBEDDED Python\" ) logger . info ( \"info from EMBEDDED Python\" ) logger . warning ( \"warning from EMBEDDED Python\" ) logger . warn ( \"warn from EMBEDDED Python\" ) logger . debug ( \"debug from EMBEDDED Python\" ) logger . error ( \"error from EMBEDDED Python\" ) logger . critical ( \"critical from EMBEDDED Python\" ) return { \"ready\" : False , \"output\" : None } class LogModule ( ) View Source class LogModule : \"\"\" Facade class for exposing the AI Inference Server's log methods. This class can be imported and used the same way as on the Edge device. Example usage:: from log_module import LogModule logger = LogModule() def run(data: str): logger.trace(\"trace from EMBEDDED Python\") logger.info(\"info from EMBEDDED Python\") logger.warning(\"warning from EMBEDDED Python\") logger.warn(\"warn from EMBEDDED Python\") logger.debug(\"debug from EMBEDDED Python\") logger.error(\"error from EMBEDDED Python\") logger.critical(\"critical from EMBEDDED Python\") return {\"ready\": False, \"output\": None} \"\"\" def __init__ ( self ): self . TRACE_LEVEL = 5 logging . addLevelName ( self . TRACE_LEVEL , \"TRACE\" ) loglevel = os . environ . get ( \"LOGLEVEL\" , \"DEBUG\" ) . upper () if loglevel not in [ \"TRACE\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ]: loglevel = \"DEBUG\" logging . basicConfig ( level = loglevel ) self . logger = logging . getLogger ( __name__ ) self . logger . setLevel ( loglevel ) def trace ( self , message ): \"\"\" Logs a message on TRACE level. Args: message (str): The log message \"\"\" if self . logger . isEnabledFor ( self . TRACE_LEVEL ): self . logger . log ( self . TRACE_LEVEL , _prefix ( message )) def debug ( self , message ): \"\"\" Logs a message on DEBUG level. Args: message (str): The log message \"\"\" self . logger . debug ( _prefix ( message )) def info ( self , message ): \"\"\" Logs a message on INFO level. Args: message (str): The log message \"\"\" self . logger . info ( _prefix ( message )) def warning ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . logger . warning ( _prefix ( message )) def warn ( self , message ): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self . warning ( message ) def error ( self , message ): \"\"\" Logs a message on ERROR level. Args: message (str): The log message \"\"\" self . logger . error ( _prefix ( message )) def critical ( self , message ): \"\"\" Logs a message on CRITICAL level. Args: message (str): The log message \"\"\" self . logger . critical ( _prefix ( message ))","title":"LogModule"},{"location":"reference/log_module/mock_logger.html#methods","text":"","title":"Methods"},{"location":"reference/log_module/mock_logger.html#critical","text":"def critical ( self , message ) Logs a message on CRITICAL level. Parameters: Name Type Description Default message str The log message None View Source def critical(self, message): \"\"\" Logs a message on CRITICAL level. Args: message (str): The log message \"\"\" self.logger.critical(_prefix(message))","title":"critical"},{"location":"reference/log_module/mock_logger.html#debug","text":"def debug ( self , message ) Logs a message on DEBUG level. Parameters: Name Type Description Default message str The log message None View Source def debug(self, message): \"\"\" Logs a message on DEBUG level. Args: message (str): The log message \"\"\" self.logger.debug(_prefix(message))","title":"debug"},{"location":"reference/log_module/mock_logger.html#error","text":"def error ( self , message ) Logs a message on ERROR level. Parameters: Name Type Description Default message str The log message None View Source def error(self, message): \"\"\" Logs a message on ERROR level. Args: message (str): The log message \"\"\" self.logger.error(_prefix(message))","title":"error"},{"location":"reference/log_module/mock_logger.html#info","text":"def info ( self , message ) Logs a message on INFO level. Parameters: Name Type Description Default message str The log message None View Source def info(self, message): \"\"\" Logs a message on INFO level. Args: message (str): The log message \"\"\" self.logger.info(_prefix(message))","title":"info"},{"location":"reference/log_module/mock_logger.html#trace","text":"def trace ( self , message ) Logs a message on TRACE level. Parameters: Name Type Description Default message str The log message None View Source def trace(self, message): \"\"\" Logs a message on TRACE level. Args: message (str): The log message \"\"\" if self.logger.isEnabledFor(self.TRACE_LEVEL): self.logger.log(self.TRACE_LEVEL, _prefix(message))","title":"trace"},{"location":"reference/log_module/mock_logger.html#warn","text":"def warn ( self , message ) Logs a message on WARNING level. Parameters: Name Type Description Default message str The log message None View Source def warn(self, message): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self.warning(message)","title":"warn"},{"location":"reference/log_module/mock_logger.html#warning","text":"def warning ( self , message ) Logs a message on WARNING level. Parameters: Name Type Description Default message str The log message None View Source def warning(self, message): \"\"\" Logs a message on WARNING level. Args: message (str): The log message \"\"\" self.logger.warning(_prefix(message))","title":"warning"},{"location":"reference/simaticai/index.html","text":"Namespace simaticai The simaticai namespace combines the AI SDK components. The simaticai python package provides the core modules listed below. Other simaticai_* python packages extend this namespace with their own submodules. Sub-modules simaticai.data simaticai.deployment simaticai.helpers simaticai.model_config_pb2 simaticai.packaging simaticai.testing","title":"Index"},{"location":"reference/simaticai/index.html#namespace-simaticai","text":"The simaticai namespace combines the AI SDK components. The simaticai python package provides the core modules listed below. Other simaticai_* python packages extend this namespace with their own submodules.","title":"Namespace simaticai"},{"location":"reference/simaticai/index.html#sub-modules","text":"simaticai.data simaticai.deployment simaticai.helpers simaticai.model_config_pb2 simaticai.packaging simaticai.testing","title":"Sub-modules"},{"location":"reference/simaticai/deployment.html","text":"Module simaticai.deployment Packaging ML models for deployment on the AI Inference Server. The AI SDK offers the functionality to create a pipeline configuration package and wrap trained models, which can be converted to an edge configuration package and then uploaded and run on an AI Inference Server on an Industrial Edge device. From a deployment perspective, the inference pipeline can consist of one or more components. This is independent of the logical structure of the inference pipeline. For example, a typical time series pipeline that consists of multiple Scikit Learn pipeline elements can be packaged into a single pipeline component, which includes both a feature extractor and a classifier. Alternatively, you can deploy the same pipeline split into two components, one for the feature extractor and another for the classifier. To keep things simple and less error-prone, a pipeline should have as few components as possible. In many cases, a single component will be sufficient. However, there might be reasons why you might consider using separate components, such as: You need a different Python environment for different parts of your processing, e.g., you have components requiring conflicting package versions. You want to exploit parallelism between components without implementing multithreading. You want to modularize and build your pipeline from a pool of component variants, which you can combine flexibly. The AI SDK allows you to create pipeline components implemented in Python and compose linear pipelines of one or multiple of such components. The API is designed to anticipate future possible types of components that might be based on a different technology than Python, e.g. ONNX or native TensorFlow Serving. Currently, only Python is supported. For a comprehensive overview on how to package ML models in the context of a machine learning workflow, we recommend you refer to the AI SDK User Manual, especially the chapter concerning packaging models into an inference pipeline. We also recommend you follow the project templates for the AI SDK, which provide packaging notebooks as examples, and where source code and saved trained models are organized into a given folder structure. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Packaging ML models for deployment on the AI Inference Server. The AI SDK offers the functionality to create a pipeline configuration package and wrap trained models, which can be converted to an edge configuration package and then uploaded and run on an AI Inference Server on an Industrial Edge device. From a deployment perspective, the inference pipeline can consist of one or more components. This is independent of the logical structure of the inference pipeline. For example, a typical time series pipeline that consists of multiple Scikit Learn pipeline elements can be packaged into a single pipeline component, which includes both a feature extractor and a classifier. Alternatively, you can deploy the same pipeline split into two components, one for the feature extractor and another for the classifier. To keep things simple and less error-prone, a pipeline should have as few components as possible. In many cases, a single component will be sufficient. However, there might be reasons why you might consider using separate components, such as: - You need a different Python environment for different parts of your processing, e.g., you have components requiring conflicting package versions. - You want to exploit parallelism between components without implementing multithreading. - You want to modularize and build your pipeline from a pool of component variants, which you can combine flexibly. The AI SDK allows you to create pipeline components implemented in Python and compose linear pipelines of one or multiple of such components. The API is designed to anticipate future possible types of components that might be based on a different technology than Python, e.g. ONNX or native TensorFlow Serving. Currently, only Python is supported. For a comprehensive overview on how to package ML models in the context of a machine learning workflow, we recommend you refer to the AI SDK User Manual, especially the chapter concerning packaging models into an inference pipeline. We also recommend you follow the project templates for the AI SDK, which provide packaging notebooks as examples, and where source code and saved trained models are organized into a given folder structure. \"\"\" from dataclasses import dataclass import json import logging import math import os import uuid import platform import re import shutil import subprocess import sys import tempfile import zipfile from datetime import datetime from importlib import resources as module_resources from pathlib import Path , PurePath from typing import Optional , Tuple , Union import jsonschema import jsonschema . exceptions import pkg_resources import yaml from MarkupPy import markup from google . protobuf import text_format from simaticai import model_config_pb2 from simaticai . helpers import pep508 , tempfiles , yaml_helper , model_config , calc_sha from simaticai . helpers . tempfiles import OpenZipInTemp from simaticai . packaging . constants import ( PIPELINE_CONFIG , RUNTIME_CONFIG , DATALINK_METADATA , # pipeline configuration files TELEMETRY_YAML , README_HTML , # additional pipeline information files REQUIREMENTS_TXT , PYTHON_PACKAGES_ZIP , # component dependency configuration PYTHON_PACKAGES , supported_types , MSG_NOT_FOUND , # additional constants PIPELINE_SIZE_LIMIT ) from simaticai . packaging . python_dependencies import PythonDependencies from simaticai . packaging . wheelhouse import create_wheelhouse from simaticai . helpers . reporter import PipelineReportWriter , ReportWriterHandler from simaticai . packaging . python_dependencies import _logger as _python_dependencies_logger from simaticai . packaging . wheelhouse import _logger as _wheelhouse_logger logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _version_matcher = re . compile ( 'Version: ([^ ]+).*' ) _transitive_matcher = re . compile ( 'Requires: (.+)' ) def find_dependencies ( name : str , dependencies : dict ): \"\"\" @Deprecated, reason: uses 'pip show' which only works for installed packages on the current platform. Collects all dependencies of the Python module given with its `name` in the current Python environment. All inherited dependencies will be added to the `dependencies` dictionary with the installed version of the module. The method executes an OS command like `python -m pip show scikit-learn`. Args: name (str): Name of the Python module to be searched through for its dependencies. dependencies (dict): Dictionary to collect the dependencies with the module name as key, and the installed version as value. Returns: dict: The `dependencies` dictionary with the collected module names and versions. \"\"\" cmd_line = [ sys . executable , '-m' , 'pip' , 'show' , name ] result = subprocess . run ( cmd_line , stdout = subprocess . PIPE , text = True ) if result . returncode != 0 : print ( f \"Dependency {name} is not found and cannot be added.\" ) return dependencies version = None for line in result . stdout . splitlines (): version_matches = _version_matcher . match ( line ) if version_matches : version = version_matches . groups ()[ 0 ] . strip () transitive_matches = _transitive_matcher . match ( line ) if transitive_matches : transitives = transitive_matches . groups ()[ 0 ] . split ( \", \" ) for dependency in transitives : if dependency not in dependencies : find_dependencies ( dependency , dependencies ) if name not in dependencies : spec = pep508 . Spec ( name , [], [( '==' , version )] if version else [], None ) dependencies [ name ] = spec print ( \"Found:\" , spec ) return dependencies def python_version_validator ( version : str ): \"\"\" Checks if Python version string is valid and describes supported version. Only version 3.10 and 3.11 is supported. A patch version is optional and accepted but logs a warning. Accepted syntaxes are: - {major}.{minor} - {major}.{minor}.{patch} Args: version (str): Python version string Raises: ValueError: if the provided version is not supported \"\"\" supported_versions = [ \"3.10\" , \"3.11\" ] error_message = \"The defined python version is not supported. Currently supported Python versions are 3.10 and 3.11. Python version must be specified only with major and minor version, e.g. '3.10'.\" warning_message = \"\"\"Required Python version was specified with patch version. Please note that the patch digit of the required Python version is often not taken into account by the Python ecosystem, so there is no guarantee it has the desired effect.\"\"\" python_version_matcher = re . match ( r '^(3)\\.(0|[1-9][0-9]*)\\.?(0|[1-9][0-9]*)?$' , str ( version )) major_minor_version = \"0.0\" has_patch_version = False if python_version_matcher is not None : major_minor_version = f \"{python_version_matcher.group(1)}.{python_version_matcher.group(2)}\" has_patch_version = python_version_matcher . group ( 3 ) is not None if major_minor_version not in supported_versions : raise ValueError ( error_message ) if has_patch_version : _logger . warning ( warning_message ) class Component : \"\"\" Base class for pipeline components, with name, description, and a list of inputs and outputs. A new component is created with the given name and an empty input and output list. Args: name (str): Name of the component desc (str): Optional description of the component inputs (dict): Dictionary of (name, type) pairs, which describe the input variables outputs (dict): Dictionary of (name, type) pairs, which describe the output variables \"\"\" reserved_names = [ \"timestamp\" ] @ dataclass class BatchInfo : \"\"\" Batch information for the component. This attribute specifies whether the component can handle batch input or output data. When set to True, the component will receive data in the form of a list of dictionaries instead of a single dictionary. It is important to note that the input and output variables on the component should still be defined as if they are single variables. If the input of the pipeline is configured for batch processing, it is recommended not to configure timeshifting, as the list will have the same timestamp for all elements, potentially resulting in data loss. \"\"\" inputBatch : bool = False outputBatch : bool = False def dict ( self ): return { 'inputBatch' : 'Yes' if self . inputBatch is True else 'No' , 'outputBatch' : 'Yes' if self . outputBatch is True else 'No' } def __init__ ( self , name : str , desc : str = \"\" ): \"\"\" Creates a new component with the given name and an empty input and output list. Args: name (str): Name of the component. desc (str): Optional description of the component \"\"\" self . name = name self . desc = desc self . inputs = {} self . outputs = {} self . batch = self . BatchInfo ( False , False ) def __repr__ ( self ) -> str : text = f \"[{self.__class__.__name__}] {self.name} \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Component Inputs: \\n \" for name , input in self . inputs . items (): text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Component Outputs: \\n \" for name , output in self . outputs . items (): text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" return text def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name ) def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name ) def _to_dict ( self ): inputs = [] inputs += [{ 'name' : name , 'type' : self . inputs [ name ][ 'type' ], } for name in self . inputs ] outputs = [] outputs += [{ 'name' : name , 'type' : self . outputs [ name ][ 'type' ], 'metric' : False , } for name in self . outputs ] return { 'name' : self . name , 'description' : self . desc , 'batch' : self . batch . dict (), 'inputType' : inputs , 'outputType' : outputs , } def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass def save ( self , destination , validate ): \"\"\" Empty method for child classess to implement. \"\"\" pass class PythonComponent ( Component ): \"\"\" A pipeline component implemented using Python scripts and libraries. A `PythonComponent` wraps Python code resource files such as saved models into a structured folder, which can be added to a pipeline configuration package. For a comprehensive overview on how to wrap ML models into Python components, we recommend you refer to the AI SDK User Manual, especially the guideline for writing pipeline components. We also recommend you study the example Python components in the E2E Tutorials for the AI SDK. A new `PythonComponent` is empty. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, the current version supports Python 3.10 and 3.11. \"\"\" def __init__ ( self , name = \"inference\" , version = \"0.0.1\" , python_version = '3.10' , desc : str = \"\" ): \"\"\" Creates a new, empty Python component. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, AI Inference Server supports Python 3.10 and 3.11. \"\"\" super () . __init__ ( name = name , desc = desc ) try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) self . python_version = python_version self . version = version self . metrics = {} self . entrypoint : Optional [ Path ] = None self . resources = {} self . python_dependencies = PythonDependencies ( python_version ) self . _replicas = 1 self . is_valid = False def __repr__ ( self ) -> str : text = super () . __repr__ () if len ( self . metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric in self . metrics . items (): text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . resources ): text += \" \\n Resources: \\n \" for path , base in self . resources . items (): text += f \" {base}/{path.name} \\n \" . replace ( './' , '' ) if self . entrypoint is not None : text += f \"Entrypoint: {self.entrypoint} \\n \" return text def set_entrypoint ( self , entrypoint : str ): \"\"\" Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. ```python import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys.path.insert(0, str(Path('./src').resolve())) from my_module import processor # then the processor module can be imported def run(data: str): input_data = json.loads(data) # incoming JSON string is loaded as a dictionary result = processor.process_data(input_data) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None: answer = {\"ready\": False, \"output\": None} else: answer = {\"ready\": True, \"output\": json.dumps(result)} return answer ``` Args: entrypoint (str): Name of the new entrypoint script to be copied \"\"\" self . is_valid = False if not any ( key . name for key , value in self . resources . items () if key . name == entrypoint and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) self . entrypoint = Path ( entrypoint ) def add_resources ( self , base_dir : os . PathLike , resources : Union [ os . PathLike , list ]): \"\"\" Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in '__pycache__' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Args: base_dir (path-like): Root folder of your code from which the resources are referred resources (os.PathLike or List): A single path or list of relative paths to resource files \"\"\" self . is_valid = False base_dir = Path ( base_dir ) . resolve () . absolute () if not base_dir . is_dir (): raise AssertionError ( f \"Parameter 'base_dir' must be a directory and available in path {base_dir}.\" ) resources = resources if type ( resources ) is list else [ resources ] for resource in resources : self . _add_resource ( base_dir , resource ) def _add_resource ( self , base_dir : Path , resource : os . PathLike ): self . is_valid = False if Path ( resource ) . is_absolute () or '..' in resource : raise AssertionError ( \"The resource path must be relative and cannot contain '/../' elements.\" ) resource_path = base_dir / resource if resource_path . is_file (): self . _add_resource_file ( base_dir , resource_path ) return if resource_path . is_dir (): for glob_path in resource_path . rglob ( \"*\" ): if glob_path . is_file (): self . _add_resource_file ( base_dir , glob_path ) return raise AssertionError ( f \"Specified resource is not a file or directory: '{resource}'\" ) def _add_resource_file ( self , base_dir : Path , resource_path : Path ): self . is_valid = False for parent in resource_path . parents : if parent . name == '__pycache__' : return if resource_path in self . resources . keys (): _logger . warning ( f \"Resource '{resource_path}' is already added to target directory '{self.resources[resource_path]}'\" ) return self . resources [ resource_path ] = f \"{resource_path.parent.relative_to(base_dir)}\" def add_dependencies ( self , packages : list ): \"\"\" Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Args: packages (list): Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution \"\"\" self . is_valid = False self . python_dependencies . add_dependencies ( packages ) def set_requirements ( self , requirements_path : os . PathLike ): \"\"\" Reads the defined dependencies from the given `requirements.txt` file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of `--extra-index-url=my.repo.example.com`. Args: requirements_path (str): Path of the given `requirements.txt` file \"\"\" self . is_valid = False self . python_dependencies . set_requirements ( requirements_path ) def add_python_packages ( self , path : str ) -> None : \"\"\" Adds Python package(s) to the `PythonPackages.zip` file of the component. The `path` parameter can refer to either a `whl`, a `zip` or a `tar.gz` file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the `tempfile.tempdir` folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Args: path (str): Path of the distribution file Examples: `component.add_python_packages('../resources/my_package-0.0.1-py3-none-any.wheel')` adds the wheel file to `PythonPackages.zip` and adds dictionary item `component.dependencies['my_package'] = '0.0.1'` `component.add_python_packages('../resources/inference-wheels.zip')` adds all the wheel files in the zip to `PythonPackages.zip` and `component.dependencies` \"\"\" self . is_valid = False self . python_dependencies . add_python_packages ( path ) def set_parallel_steps ( self , replicas ): \"\"\" Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Args: replicas (int): Number of parallel executors. Default is 1. Raises: ValueError: if the given argument is not a positive integer. \"\"\" self . is_valid = False if ( not isinstance ( replicas , int )) or replicas < 1 : raise ValueError ( \"Replica count must be a positive integer.\" ) if 8 < replicas : _logger . warning ( \"The current maximum of parallel executors is 8.\" ) self . _replicas = replicas def add_metric ( self , name : str , desc : Optional [ str ] = None ): \"\"\" Adds a metric that will be automatically used as a pipeline output. Args: name (str): Name of the metric. desc (str): Description of the metric. (optional) \"\"\" if \"_\" not in name : raise AssertionError ( \"The metric name must contain at least one underscore\" ) if self . metrics is None : self . metrics = {} if name in self . metrics : raise AssertionError ( f \"Metric '{name}' already exists\" ) self . metrics [ name ] = {} if desc is not None : self . metrics [ name ][ 'desc' ] = desc def delete_metric ( self , name : str ): \"\"\" Remove a previously added metric. Args: name (str): Name of the metric to be deleted. \"\"\" if name not in self . metrics : raise AssertionError ( f \"Component '{self.name}' has no metric '{name}'\" ) self . metrics . pop ( name ) def _to_dict ( self ): component_dict = { ** super () . _to_dict (), 'version' : self . version , 'entrypoint' : f \"./{self.entrypoint.name}\" , 'hwType' : 'CPU' , 'runtime' : { 'type' : 'python' , 'version' : self . python_version }, 'replicas' : self . _replicas } component_dict [ \"outputType\" ] += [{ 'name' : name , 'type' : 'String' , 'metric' : True , } for name in self . metrics . keys ()] return component_dict def enable_dependency_optimization ( self ): \"\"\" Allows changing repository URLs to optimize the package size Allows the replacement of the `--index-url` argument during `pip download` to download CPU runtime optimized dependencies only. Enabling this optimization, the present `--index-url` will be prepended to the `--extra-index-url` list, and the Pytorch CPU only repository will be set as the `--index-url`. A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . enable_dependency_optimization () def disable_dependency_optimization ( self ): \"\"\" Disables any modification to repository URLs Disables the replacement of the `--index-url` argument during `pip download`. This way all `--index-url` or `--extra-index-url` arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . disable_dependency_optimization () def validate ( self ): \"\"\" Validates that the component is ready to be serialized and packaged as part of a pipeline. \"\"\" if not self . is_valid : if self . entrypoint is None : raise AssertionError ( \"Entrypoint must be defined\" ) if not any ( key . name for key , value in self . resources . items () if key . name == self . entrypoint . name and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) if len ( self . python_dependencies . dependencies ) < 1 : _logger . warning ( f \"WARNING! There are no dependencies defined for component '{self.name}'. Please make sure that all necessary dependencies have been added.\" ) self . python_dependencies . validate () self . is_valid = True _logger . info ( f \"Component '{self.name}' is valid and ready to use.\" ) def save ( self , destination , validate = True ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter `validate` to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: - `requirements.txt` with a list of Python dependencies - Entry point script defined by the `entrypoint` attribute of the component - Extra files as added to the specified folders - `PythonPackages.zip` with the wheel binaries for the environment to be installed Args: destination (path-like): Target directory to which the component will be saved. validate (bool): With value True, triggers component validation. Defaults to True. \"\"\" if validate : self . validate () folder_path = Path ( destination ) / self . name folder_path . mkdir ( parents = True , exist_ok = True ) for file_path in self . resources : dir_path = folder_path / self . resources [ file_path ] os . makedirs ( dir_path , exist_ok = True ) shutil . copy ( file_path , dir_path / file_path . name ) self . python_dependencies . save ( folder_path ) class Pipeline : \"\"\" `Pipeline` represents a pipeline configuration package with `Components` and wires to provide a data flow on the AI Inference Server. The `Components` have inputs and outputs to transfer data to each other and the wires describe this data flow between them. The package also contains configuration files required to deploy a pipeline on an Industrial Edge device. A newly initialized `Pipeline` does not contain any `Component` or wire, only its name and version will be set. The name and version together will define the name of the zip file when the package is saved. Args: name (str): Name of the package version (str): Version of the package \"\"\" _wire_hash_string = \"{}.{} -> {}.{}\" def __init__ ( self , name : str , version : Optional [ str ] = None , desc : str = \"\" ): \"\"\" A newly initialized `Pipeline` will contain no `Component` or wire, just its name and version will be set. The name and version will define together the name of the zip file when the package is saved. Args: name (str): Name of the package desc (str): Package description (optional) version (str): Version of the package \"\"\" self . name = name self . desc = desc self . version = version self . package_id : Optional [ uuid . UUID ] = None self . save_version = None self . save_package_id : Optional [ uuid . UUID ] = None self . author = 'AI SDK' self . components = {} self . wiring = {} self . parameters = {} self . periodicity = None self . timeshift_reference = [] self . inputs = [] self . outputs = [] self . log_level = logging . INFO self . report_writer = PipelineReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) _python_dependencies_logger . addHandler ( report_writer_handler ) _wheelhouse_logger . addHandler ( report_writer_handler ) def _set_log_level ( self , log_level : int ): self . log_level = log_level _logger . setLevel ( self . log_level ) @ staticmethod def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = \"\" ) -> \"Pipeline\" : \"\"\" Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Args: components (list): List of PythonComponents name (str): Name of the pipeline version (str): Version information of the pipeline. (Optional) Returns: Pipeline: Pipeline object with the auto-wired components \"\"\" pipeline = Pipeline ( name , version , desc = desc ) first_component = components [ 0 ] pipeline . add_component ( first_component ) pipeline . inputs = [( first_component . name , component_input ) for component_input in first_component . inputs ] pipeline . outputs = [( first_component . name , output ) for output in first_component . outputs ] for component in components [ 1 :]: pipeline . add_component ( component ) for ( wire_component , wire_name ) in pipeline . outputs : try : pipeline . add_wiring ( wire_component , wire_name , component . name , wire_name ) except Exception as e : _logger . warning ( f \"Output variable {wire_component}.{wire_name} couldn't be auto-wired. \\n Cause: {e}\" ) unwired_variables = [ f '{component.name}.{x}' for x in component . inputs if not any ( s . endswith ( f '{component.name}.{x}' ) for s in pipeline . wiring )] if len ( unwired_variables ) > 0 : for variable in unwired_variables : _logger . warning ( f \"Input variable {variable} couldn't be auto-wired. \\n \" ) pipeline . outputs = [( component . name , output ) for output in component . outputs ] return pipeline def __repr__ ( self ) -> str : \"\"\" Textual representation of the configured package. The method shows the `Components` with their inputs, outputs and parameters as well as the wiring between these `Components`. Returns: [str]: Textual representation of the package \"\"\" text = f \"[{self.__class__.__name__}] {self.name} ({self.version}) \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . parameters ) > 0 : text += \" \\n Pipeline Parameters: \\n \" for name , parameter in self . parameters . items (): text += f \"- {name} ({parameter['type']}, default: '{parameter['defaultValue']}'){(': ' + parameter['desc']) if parameter.get('desc') is not None else ''} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Pipeline Inputs: \\n \" for component , name in self . inputs : input = self . components [ component ] . inputs [ name ] text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Pipeline Outputs: \\n \" for component , name in self . outputs : output = self . components [ component ] . outputs [ name ] text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" metrics = [( name , metric , component_name ) for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name , metric in component . metrics . items ()] if len ( metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric , _ in metrics : text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . wiring ) > 0 : text += \" \\n I/O Wiring: \\n \" for component , name in self . inputs : text += f \" {name} -> {component}.{name} \\n \" for wire_hash in self . wiring : text += f \" {wire_hash} \\n \" for component , name in self . outputs : text += f \" {component}.{name} -> {name} \\n \" for name , metric , component_name in metrics : text += f \" {component_name}.{name} -> {name} \\n \" if self . periodicity is not None : text += \" \\n Timeshifting: \\n \" text += f \" Periodicity: {self.periodicity} ms \\n \" if len ( self . timeshift_reference ) > 0 : text += \" References: \\n \" for ref in self . timeshift_reference : text += f \" - {ref} \\n \" for component in self . components . values (): text += \" \\n \" + component . __repr__ () return text def add_input ( self , component , variable ): \"\"\" Defines an input variable on the given component as a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" try : _ = self . components [ component ] . inputs [ variable ] except KeyError : raise AssertionError ( \"The component with input variable must exist in the pipeline.\" ) if self . inputs is None : self . inputs = [] if ( component , variable ) in self . inputs : raise AssertionError ( \"The pipeline input already exists.\" ) self . inputs . append (( component , variable )) def delete_input ( self , component : str , variable : str ): \"\"\" Deletes a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . inputs : raise AssertionError ( \"The pipeline input does not exist.\" ) self . inputs . remove (( component , variable )) def add_output ( self , component , variable ): \"\"\" Defines an output variable on the given component as a pipeline output. Args: component (str): Name of the component variable (str): Name of the output variable \"\"\" try : _ = self . components [ component ] . outputs [ variable ] except KeyError : raise AssertionError ( \"The component with output variable must exist in the pipeline.\" ) if self . outputs is None : self . outputs = [] if ( component , variable ) in self . outputs : raise AssertionError ( \"The pipeline output already exists.\" ) self . outputs . append (( component , variable )) def delete_output ( self , component : str , variable : str ): \"\"\" Deletes a pipeline output. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . outputs : raise AssertionError ( \"The pipeline output does not exist.\" ) self . outputs . remove (( component , variable )) def add_component ( self , component : Component ): \"\"\" Adds a `Component` to the pipeline configuration without any connection. The `Component` can be marked as an input or output component of the pipeline. When these parameters are True, the `Component` is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Args: component (Component): `Component` to be added \"\"\" if component . name in self . components : raise AssertionError ( f \"Component with name {component.name} already exists. Please rename the component.\" ) self . components [ component . name ] = component def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ): \"\"\" Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: - The components exist with the given inputs/outputs - The given inputs and outputs are not connected to any wire - The types of the connected input and output are compatible Args: from_component (str): Name of the component which provides data to the `to_component` from_output (str): Name of the output variable of the `from_component` to_component (str): Name of the component which consumes data from the `from_component` to_input (str): Name of the input variable of the `to_component` \"\"\" if from_component not in self . components : raise AssertionError ( f \"No component named '{from_component}'\" ) if to_component not in self . components : raise AssertionError ( f \"No component named '{to_component}'\" ) if from_output not in self . components [ from_component ] . outputs : raise AssertionError ( f \"Component '{from_component}' has no output named '{from_output}'\" ) if to_input not in self . components [ to_component ] . inputs : raise AssertionError ( f \"Component '{to_component}' has no input named '{to_input}'\" ) if self . get_wire_for_input ( to_component , to_input ) is not None : raise AssertionError ( f \"Input '{to_input}' of component '{to_component}' is already wired\" ) _output_type = self . components [ from_component ] . outputs [ from_output ][ \"type\" ] _input_type = self . components [ to_component ] . inputs [ to_input ][ \"type\" ] if _output_type != _input_type : raise AssertionError ( \"Output and input types do not match\" ) wire_hash = self . _wire_hash_string . format ( from_component , from_output , to_component , to_input ) self . wiring [ wire_hash ] = { \"fromComponent\" : from_component , \"fromOutput\" : from_output , \"toComponent\" : to_component , \"toInput\" : to_input , } def get_wire_for_output ( self , component_name : str , output_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data provider through its output with name output_name. Args: component_name (str): Name of the data provider component. output_name (str): Name of the output variable of `component_name`. Returns: [dict]: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"fromComponent\" ] == component_name and x [ \"fromOutput\" ] == output_name ] return wires [ 0 ] if wires else None def get_wire_for_input ( self , component_name : str , input_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data consumer through its input with name `input_name`. Args: component_name (str): Name of the data consumer component. input_name (str): Name of the input variable of `component_name`. Returns: dict: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"toComponent\" ] == component_name and x [ \"toInput\" ] == input_name ] return wires [ 0 ] if wires else None def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ): \"\"\" Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Args: component (str): Name of the component which has the input given the name variable variable (str): Name of the input variable on the component which connected by the wire with_input (bool, optional): If set, the input variable will be also deleted from the component. Defaults to True. Raises: AssertionError: When the variable acts as inter signal alignment reference, it cannot be deleted, and an `AssertionError` will be raised. \"\"\" wire = self . get_wire_for_input ( component , variable ) if wire is None : raise AssertionError ( f \"There is no wiring for input '{variable}' of component '{component}'\" ) if variable in self . timeshift_reference : raise AssertionError ( \"Inter signal alignment reference variables can not be deleted.\" ) wire_hash = self . _wire_hash_string . format ( wire [ 'fromComponent' ], wire [ 'fromOutput' ], wire [ 'toComponent' ], wire [ 'toInput' ]) self . wiring . pop ( wire_hash ) if with_input : self . components [ component ] . delete_input ( variable ) def add_dependencies ( self , packages : list ): \"\"\" @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type `PythonComponent`. This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the `requirements.txt` file when the package is saved. Args: packages (list): List of the necessary python packages to execute the script defined by self.entrypoint \"\"\" python_components = [ self . components [ name ] for name in self . components if type ( self . components [ name ]) is PythonComponent ] for component in python_components : component . add_dependencies ( packages ) def set_timeshifting_periodicity ( self , periodicity : int ): \"\"\" Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, `startingPoint` property is set to `First timestamp`, which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to `Signal reference` by adding inter-signal alignment reference variables via the `add_timeshifting_reference(..)` method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Args: periodicity (int): Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). \"\"\" periodicity = int ( periodicity ) if periodicity not in range ( 10 , int ( math . pow ( 2 , 31 ))): raise AssertionError ( \"Inter signal alignment periodicity must be an integer and in range [10, 2^31)\" ) self . periodicity = periodicity _logger . info ( f \"Inter signal alignment periodicity has been set to {self.periodicity}.\" ) def add_timeshifting_reference ( self , reference : str ): \"\"\" Enables signal alignment mode `Signal reference` by declaring input variables as reference variables. Args: reference (str): Variable name to be added to `self.timeshift_reference` list. \"\"\" if reference not in [ name for _ , name in self . inputs ]: raise AssertionError ( f \"There is no input variable defined with name '{reference}'\" ) if reference in self . timeshift_reference : _logger . warning ( f \"Reference variable with name '{reference}' has been already added.\" ) return self . timeshift_reference . append ( reference ) def remove_timeshifting_reference ( self , reference : str ): \"\"\" Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the `startingPoint` will be `First timestamp`. Args: reference (str): Variable name to be removed from `self.timeshift_reference` list. \"\"\" if reference not in self . timeshift_reference : raise AssertionError ( f \"Reference variable with name {'reference'} does not exist.\" ) self . timeshift_reference . remove ( reference ) def get_pipeline_config ( self ): \"\"\" Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the `destination` folder with name `pipeline_config.yml` \"\"\" if self . save_version is None : self . save_version = self . version if self . save_package_id is None : self . save_package_id = self . package_id pipeline_inputs = [] pipeline_inputs += [{ 'name' : name , 'type' : self . components [ component_name ] . inputs [ name ][ 'type' ] } for component_name , name in self . inputs ] pipeline_outputs = [] pipeline_outputs += [{ 'name' : name , 'type' : self . components [ component_name ] . outputs [ name ][ 'type' ], 'metric' : False , } for component_name , name in self . outputs ] pipeline_outputs += [{ 'name' : name , 'type' : 'String' , 'metric' : True , 'topic' : f \"/siemens/edge/aiinference/{self.name}/{self.save_version}/metrics/{component_name}/{name}\" , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] pipeline_dag = [{ 'source' : f \"{wire['fromComponent']}.{wire['fromOutput']}\" , 'target' : f \"{wire['toComponent']}.{wire['toInput']}\" , } for wire in self . wiring . values ()] pipeline_dag += [{ 'source' : f 'Databus.{name}' , 'target' : f '{component_name}.{name}' , } for component_name , name in self . inputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , name in self . outputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] config_yml_content = { 'fileFormatVersion' : '1.2.0' , 'dataFlowPipelineInfo' : { 'author' : self . author , 'createdOn' : datetime . now (), 'dataFlowPipelineVersion' : self . save_version , 'description' : self . desc if self . desc else 'Created by AI SDK' , 'projectName' : self . name , 'packageId' : str ( self . save_package_id ) }, 'dataFlowPipeline' : { 'components' : [ component . _to_dict () for component in self . components . values ()], 'pipelineDag' : pipeline_dag , 'pipelineInputs' : pipeline_inputs , 'pipelineOutputs' : pipeline_outputs , }, 'packageType' : 'full' } if len ( self . parameters . items ()) != 0 : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] = [] for name , parameter in self . parameters . items (): if parameter [ \"topicBased\" ]: config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ], 'topicBased' : parameter [ 'topicBased' ], 'valueTopic' : parameter [ 'valueTopic' ] }) else : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ] }) return config_yml_content def save_pipeline_config ( self , destination ): \"\"\" Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the `destination` folder with name `pipeline_config.yml` Args: destination (path-like): Path of the `destination` directory. \"\"\" with open ( Path ( destination ) / PIPELINE_CONFIG , \"w\" ) as f : yaml . dump ( self . get_pipeline_config (), f ) def get_datalink_metadata ( self ): \"\"\" The method generates metadata information based on available information. Returns: dict: Dictionary with the necessary information for the AI Inference Server. \"\"\" timeshifting = { \"id\" : None , \"enabled\" : False , \"periodicity\" : self . periodicity , \"startingPoint\" : None , } if self . periodicity is not None : timeshifting [ \"enabled\" ] = True timeshifting [ \"startingPoint\" ] = 'First timestamp' if len ( self . timeshift_reference ) > 0 : timeshifting [ \"startingPoint\" ] = 'Signal reference' exported_metadata = { \"fileFormatVersion\" : \"1.0.0\" , \"id\" : None , \"version\" : None , \"createdOn\" : datetime . now (), \"updatedOn\" : datetime . now (), \"timeShifting\" : timeshifting , \"inputs\" : [ { 'name' : _name , 'mapping' : None , 'timeShiftingReference' : _name in self . timeshift_reference , 'type' : self . components [ _component ] . inputs [ _name ][ 'type' ] } for _component , _name in self . inputs ] } return exported_metadata def save_datalink_metadata ( self , destination ): \"\"\" Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the `destination` folder with the name `datalink_metadata.yml` Args: destination (path-like): Path of the destination directory. \"\"\" with open ( Path ( destination ) / DATALINK_METADATA , \"w\" ) as f : yaml . dump ( self . get_datalink_metadata (), f ) def save_telemetry_data ( self , destination : Path ): \"\"\" Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None \"\"\" telemetry_path = destination / TELEMETRY_YAML telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"simaticai package not found\" ) try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"vep-template-sdk package not found\" ) telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ] . python_version for component in self . components if isinstance ( self . components [ component ], PythonComponent ))) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = list ( set ( f . suffix for f in Path ( destination ) . rglob ( \"*\" ) if f . suffix not in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ])) yaml . dump ( telemetry_data , open ( telemetry_path , 'w' )) def save_readme_html ( self , destination ): \"\"\" Saves a `README.html` in the `destination` folder that describes the pipeline. Args: destination (path-like): Path of the destination folder. \"\"\" pipelinePage = _PipelinePage ( self ) readme_html_path = Path ( destination ) / README_HTML readme_html_path . write_text ( pipelinePage . __str__ ()) def validate ( self , destination = \".\" ): \"\"\" Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: - If the package has at least one component - If all wires create connections between existing components and their variables - If metadata is defined and valid. - If a package with the same name already exists in the `destination` folder. In this case a warning message appears and the `save(..)` method overwrites the existing package. - If the package has multiple components and if they are using the same Python version Args: destination (str, optional): Path of the expected destination folder. Defaults to \".\". \"\"\" if len ( self . components ) < 1 : raise AssertionError ( \"The package must have at least one component.\" ) for name , variable in self . outputs : if self . components [ name ] . batch . outputBatch : raise AssertionError ( f \"The component '{name}' has pipeline output defined with variable name '{variable}'. \\ None of component with pipeline output is allowed to provide batch output . \") for wire_hash in self . wiring . copy (): wire = self . wiring [ wire_hash ] self . _check_wiring ( wire , wire_hash ) pipeline_inputs = [ variable for _ , variable in self . inputs ] pipeline_outputs = [ variable for _ , variable in self . outputs ] if any ( variable in pipeline_outputs for variable in pipeline_inputs ): conflicts = set ( pipeline_inputs ) . intersection ( set ( pipeline_outputs )) raise AssertionError ( f \"Pipeline input and output variables must be unique. Conflicting variables: {conflicts}\" ) self . _check_timeshifting () package_path = Path ( destination ) / f \"{self.name}_{self.version}\" . replace ( \" \" , \"-\" ) if package_path . is_dir (): _logger . warning ( f \"Target folder ({package_path}) already exists! Unless changing the package name the package could be invalid and your files will be overwritten!\" ) python_versions = set () for component in self . components : self . components [ component ] . validate () if isinstance ( self . components [ component ], PythonComponent ): python_versions . add ( self . components [ component ] . python_version ) if ( 1 < len ( python_versions )): _logger . warning ( \"The use of multiple python version in a single pipeline is not recommended. We recommend using only one of the supported versions, which are Python 3.10 or 3.11.\" ) _logger . info ( f \"Package '{self.name}' is valid and ready to save.\" ) def _check_timeshifting ( self ): if len ( self . timeshift_reference ) > 0 and self . periodicity is None : raise AssertionError ( \"When using inter signal alignment reference variables, the periodicity must be set.\" ) def _check_wiring ( self , wire , wire_hash ): error_messages = [] if wire [ 'fromComponent' ] not in self . components : error_messages . append ( f \"From component {wire['fromComponent']} does not exist\" ) if wire [ 'toComponent' ] not in self . components : error_messages . append ( f \"To component {wire['toComponent']} does not exist\" ) if wire [ 'fromOutput' ] not in self . components [ wire [ 'fromComponent' ]] . outputs : error_messages . append ( f \"Output variable {wire['fromOutput']} does not exist on component {wire['fromComponent']}\" ) if wire [ 'toInput' ] not in self . components [ wire [ 'toComponent' ]] . inputs : error_messages . append ( f \"Input variable {wire['toInput']} does not exist on component {wire['toComponent']}\" ) if len ( error_messages ) == 0 : from_type_ = self . components [ wire [ 'fromComponent' ]] . outputs [ wire [ 'fromOutput' ]][ 'type' ] to_type_ = self . components [ wire [ 'toComponent' ]] . inputs [ wire [ 'toInput' ]][ 'type' ] if from_type_ != to_type_ : error_messages . append ( f \"The types of input and output variables does not match for wiring {wire_hash}.\" ) if len ( error_messages ) > 0 : self . wiring . pop ( wire_hash ) error_messages . append ( \"The wire has been deleted, please check the variables and re-create the connection.\" ) raise AssertionError ( error_messages . __str__ ()) def save ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as `{package_name}_{package_version}.zip`. If a file with such a name already exists in the `destination` folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name `{package_name}_{package_version}`. If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: - Package folder with name `{package_name}_{package_version}` - `datalink-metadata.yml` - `pipeline-config.yml` - Component folder with name `{component_name}` When the component is a `PythonComponent`, this folder contains: - `requirements.txt` - Entrypoint script defined by the entrypoint of the component - Extra files as added to the specified folders - Source folder with name `src` with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the `destination` folder, an error is raised. Args: destination (str, optional): Target directory for saving the package. Defaults to \".\". package_id (UUID): The optional package ID. If None, a new UUID is generated. \"\"\" self . validate ( destination ) destination = Path ( destination ) if package_id is not None and not isinstance ( package_id , uuid . UUID ): package_id = uuid . UUID ( package_id ) prev_id = None prev_id_version = None prev_version , prev_with_id = self . _find_previous ( destination , package_id ) if prev_with_id is not None : prev_id_version , prev_id = prev_with_id if package_id == prev_id and ( version or self . version ) == str ( prev_id_version ): raise RuntimeError ( f \"package with version '{version or self.version}' and id '{package_id}' already exists\" ) self . save_version = version if version is not None \\ else self . version if self . version is not None \\ else \"1\" if package_id is not None and prev_id is not None and package_id != prev_id \\ else str ( prev_id_version + 1 ) if prev_id_version is not None \\ else str ( prev_version + 1 ) if prev_version is not None and prev_id is None \\ else \"1\" self . save_package_id = package_id if package_id is not None \\ else prev_id if prev_id is not None \\ else uuid . uuid4 () name = self . name . replace ( \" \" , \"-\" ) package_name = f \"{name}_{self.save_version}\" package_file = destination / f \"{package_name}.zip\" specified_version_is_decimal = version is not None and version . isdecimal () or self . version is not None and self . version . isdecimal () if specified_version_is_decimal : if package_file . exists (): p_id = self . _extract_package_id ( package_file , False ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{package_file}'\" ) edge_package_file = destination / f \"{name}-edge_{self.save_version}.zip\" if edge_package_file . exists (): p_id = self . _extract_package_id ( edge_package_file , True ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{edge_package_file}'\" ) destination = destination / package_name destination . mkdir ( parents = True , exist_ok = True ) # Save for component in self . components : self . components [ component ] . save ( destination , False ) if isinstance ( self . components [ component ], PythonComponent ): self . report_writer . add_direct_dependencies ( self . components [ component ] . name , self . components [ component ] . python_dependencies . dependencies ) self . save_datalink_metadata ( destination ) self . save_pipeline_config ( destination ) self . save_readme_html ( destination ) self . save_telemetry_data ( destination ) zip_destination = shutil . make_archive ( base_name = str ( destination . parent / package_name ), format = 'zip' , root_dir = destination . parent , base_dir = package_name , verbose = True , logger = _logger ) pipeline_size = os . path . getsize ( zip_destination ) # zipped package size in bytes pipeline_size_GB = \"{:.2f}\" . format ( pipeline_size / 1000 / 1000 / 1000 ) pipeline_size_limit_GB = \"{:.2f}\" . format ( PIPELINE_SIZE_LIMIT / 1000 / 1000 / 1000 ) if pipeline_size > PIPELINE_SIZE_LIMIT : error_msg = f \"Pipeline size {pipeline_size} bytes ({pipeline_size_GB} GB) exceeds the limit of \" \\ f \"{PIPELINE_SIZE_LIMIT} bytes ({pipeline_size_limit_GB} GB). \" \\ \"Please remove unnecessary files and dependencies and try again.\" _logger . error ( error_msg ) raise RuntimeError ( error_msg ) return Path ( zip_destination ) # TODO: refactor the business logic in PBI 1662648 def _find_previous ( self , destination : Path , package_id : Optional [ uuid . UUID ] = None ) -> Tuple [ Optional [ int ], Optional [ Tuple [ int , uuid . UUID ]]]: latest_version = None latest_with_id = None if Path ( destination ) . is_dir () is False : return None , None for file in destination . glob ( f \"{self.name.replace(' ', '-')}*.zip\" ): zip_version , zip_package_id = self . _extract_package_info ( file ) if not zip_version . isdecimal (): continue zip_version = int ( zip_version ) if zip_package_id is None : # package id in the zip package not present if latest_version is None or zip_version > latest_version : latest_version = zip_version elif package_id is None : if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) else : if package_id != zip_package_id : continue if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) return latest_version , latest_with_id def _extract_package_info ( self , zip_path : Path ) -> Tuple : with zipfile . ZipFile ( zip_path ) as zip_file : config_path = next ( f for f in zip_file . namelist () if f . endswith ( \"pipeline_config.yml\" )) with zip_file . open ( config_path ) as config_file : config = yaml . load ( config_file , Loader = yaml . SafeLoader ) pipeline_info = config . get ( \"dataFlowPipelineInfo\" , {}) version = pipeline_info . get ( \"dataFlowPipelineVersion\" , None ) package_id = pipeline_info . get ( \"packageId\" , None ) package_id = uuid . UUID ( package_id ) if package_id is not None else None return version , package_id def _extract_package_id ( self , package_file : Path , is_edge_package : bool ) -> Optional [ uuid . UUID ]: try : with OpenZipInTemp ( package_file ) as package_dir : if not is_edge_package : package_dir = package_dir / package_file . stem with open ( package_dir / 'pipeline_config.yml' ) as config_file : return uuid . UUID ( yaml . load ( config_file , Loader = yaml . SafeLoader )[ \"dataFlowPipelineInfo\" ][ \"packageId\" ]) except Exception : _logger . debug ( f \"Could not extract Package ID from '{package_file}'\" ) return None def export ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" Export a runnable pipeline package. Args: destination (str): optional target directory for saving the package. Defaults to \".\". package_id (UUID): optional package ID. If None, a new UUID is generated. version (str): optional version. If None, an automatic version number is generated. \"\"\" config_package = None try : config_package = self . save ( destination , package_id , version ) runtime_package = convert_package ( config_package , self . report_writer ) return runtime_package finally : if config_package is not None : Path ( config_package ) . unlink ( missing_ok = True ) def add_parameter ( self , name , default_value , type_name : str = \"String\" , topic_based : bool = False , desc : str = None ): \"\"\" Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Args: name (str): Name of the parameter desc (str): Description of the parameter (optional) type_name (str, optional): Data type of the parameter. Defaults to \"String\". default_value (str): Default value of the parameter topic_based (bool, optional): If true, the parameter can be updated from a message queue. Raises: ValueError: When: - the default value of the parameter is not of the specified data type (`type_name`) or - the specified data type itself is not an allowed data type (not a part of `parameter_types` dict) or - the specified data type is not given in the right format or - the type of the given `topic_based` parameter is not `bool`. \"\"\" parameter_types = { \"String\" : 'str' , \"Integer\" : 'int' , \"Double\" : 'float' , \"Boolean\" : 'bool' } default_value_type = type ( default_value ) . __name__ if type_name not in parameter_types . keys (): raise ValueError ( f \"The given value type is not supported. Please use one of these: {parameter_types.keys()}\" ) if default_value_type != parameter_types [ type_name ]: raise ValueError ( f \"The given value type does not match the type of '{type_name}'. Please use the correct one from these: {list(parameter_types.keys())}\" ) if not isinstance ( topic_based , bool ): raise ValueError ( \"Type of the given `topic_based` parameter is not `bool`.\" ) self . parameters [ name ] = { \"name\" : name , \"type\" : type_name , \"defaultValue\" : default_value , \"topicBased\" : topic_based , \"valueTopic\" : None } if desc is not None : self . parameters [ name ][ \"desc\" ] = desc def convert_package ( zip_path : str or os . PathLike , report_writer : Optional [ PipelineReportWriter ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use Pipeline.export(...) instead. Create an Edge Configuration Package from a given Pipeline Configuration Package. If the input zip file is `{path}/{name}_{version}.zip`, the output file will be created as `{path}/{name}-edge_{version}.zip`. Please make sure that the given zip file comes from a trusted source! If a file with such a name already exists, it is overwritten. First, this method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. Currently, the supported edge devices run Linux on 64-bit x86 architecture, so the accepted Python libraries are restricted to the platform independent ones and packages built for 'x86_64' platforms. AI Inference Server also provides a Python 3.10 and runtime environment, so the supported Python libraries are restricted to Python 3.10 and 3.11 compatible packages. If for the target platform the required dependency is not available on pypi.org and not present in `PythonPackages.zip`, it will log the problem at ERROR level. Then it downloads all dependencies (either direct or transitive), and creates a new zip file, which is validated against the AI Inference Server's schema. This functionality requires pip with version of 21.3.1 or greater. This method can be used from the command line too. Example usage: ``` python -m simaticai convert_package <path_to_pipeline_configuration_package.zip> ``` Args: zip_path (path-like): path to the pipeline configuration package zip file. report_writer (ReportWriter, optional): a ReportWriter object to write the report for a pipeline. Defaults to None. Returns: os.PathLike: The path of the created zip file. Exceptions: PipelineValidationError: If the validation fails. See the logger output for details. \"\"\" zip_path = Path ( zip_path ) if zip_path . stem . find ( '_' ) < 0 : raise AssertionError ( \"The input zip file name must contain an underscore character.\" ) with tempfiles . OpenZipInTemp ( zip_path ) as zip_dir : top_level_items = list ( zip_dir . iterdir ()) if len ( top_level_items ) != 1 : raise AssertionError ( \"The Pipeline Configuration Package must contain a single top level directory.\" ) package_dir = zip_dir / top_level_items [ 0 ] runtime_dir = zip_dir / \"edge_config_package\" runtime_dir . mkdir ( parents = True , exist_ok = True ) config = yaml_helper . read_yaml ( package_dir / PIPELINE_CONFIG ) _validate_with_schema ( \"input pipeline_config.yml\" , config , \"pipeline.schema.json\" ) runtime_config = _generate_runtime_config ( config ) if report_writer is not None : # TODO: consider moving zip_path to the parameter of report_writer.write_report() report_writer . set_path ( Path ( zip_path . parent / f \"{zip_path.stem}_package_report.md\" )) report_writer . set_pipeline_config ( config ) for component in config [ 'dataFlowPipeline' ][ 'components' ]: source_dir = package_dir / component [ \"name\" ] if component [ \"runtime\" ][ \"type\" ] == \"python\" : python_version = component [ 'runtime' ][ 'version' ] try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) dependency_set = _package_component_dependencies ( source_dir , python_version ) if report_writer is not None : report_writer . add_full_dependency_set ( component_name = component [ \"name\" ], dependency_set = dependency_set ) runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"Python\" , }) if component [ \"runtime\" ][ \"type\" ] == \"gpuruntime\" : runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"gpuruntime\" , }) _package_component ( source_dir , runtime_dir / 'components' / f \"{component['name']}_{component['version']}\" ) if report_writer is not None : report_writer . write_report () _logger . info ( f \"Report on {zip_path.stem} is saved to {zip_path.parent}.\" ) shutil . copy ( str ( package_dir / PIPELINE_CONFIG ), str ( runtime_dir / PIPELINE_CONFIG )) datalink_metadata_yaml = package_dir / DATALINK_METADATA if datalink_metadata_yaml . is_file (): shutil . copy ( str ( datalink_metadata_yaml ), runtime_dir / DATALINK_METADATA ) _validate_with_schema ( f \"generated {RUNTIME_CONFIG}\" , runtime_config , \"runtime.schema.json\" ) with open ( runtime_dir / RUNTIME_CONFIG , \"w\" , encoding = \"utf8\" ) as file : yaml . dump ( runtime_config , file ) readme_html = package_dir / README_HTML if readme_html . exists (): ( runtime_dir / README_HTML ) . write_text ( readme_html . read_text ()) telemetry_yaml = package_dir / TELEMETRY_YAML if telemetry_yaml . exists (): ( runtime_dir / TELEMETRY_YAML ) . write_text ( telemetry_yaml . read_text ()) edge_package_path = Path ( shutil . make_archive ( # One Pythonic Way to replace the last occurrence of \"_\" with \"-edge\". base_name = str ( PurePath ( zip_path ) . parent / \"-edge_\" . join ( zip_path . stem . rsplit ( \"_\" , 1 ))), format = 'zip' , root_dir = runtime_dir , verbose = True , logger = _logger )) sha256_hash = calc_sha ( edge_package_path ) sha_format = f \"{sha256_hash} {edge_package_path.name}\" edge_package_path . with_suffix ( '.sha256' ) . write_text ( sha_format ) return edge_package_path def _package_component ( source_dir , target_name ): return shutil . make_archive ( base_name = target_name , format = 'zip' , root_dir = source_dir , verbose = True , logger = _logger ) def _package_component_dependencies ( component_dir : Path , python_version : str ) -> set : python_packages_folder = component_dir / 'packages' requirements_file_path = component_dir / REQUIREMENTS_TXT packages_file = component_dir / PYTHON_PACKAGES_ZIP dependency_set = set () python_packages_folder . mkdir ( exist_ok = True ) if packages_file . is_file (): with zipfile . ZipFile ( packages_file ) as zip_file : zip_file . extractall ( python_packages_folder ) packages_file . unlink () requirements_file_path . touch ( exist_ok = True ) try : dependency_set = create_wheelhouse ( requirements_file_path , python_version , python_packages_folder ) if any ( Path ( python_packages_folder ) . iterdir ()): shutil . make_archive ( base_name = str ( component_dir / PYTHON_PACKAGES ), format = 'zip' , root_dir = python_packages_folder , verbose = True , logger = _logger ) finally : shutil . rmtree ( python_packages_folder ) # This filtering needs to happen here, not in PythonDependencies, # because create_wheelhouse still needs the original requirements.txt # with the extra index urls. with open ( requirements_file_path , \"r\" ) as f : lines = f . readlines () filtered_lines = list ( filter ( lambda x : not ( x . startswith ( \"# Extra\" ) or x . startswith ( \"--extra-index-url\" ) or x . startswith ( \"# Index\" ) or x . startswith ( \"--index-url\" )), lines )) with open ( requirements_file_path , \"w\" ) as f : f . writelines ( filtered_lines ) return dependency_set def _generate_runtime_config ( pipeline_config : dict ): project_name = pipeline_config [ \"dataFlowPipelineInfo\" ][ \"projectName\" ] return { \"fileFormatVersion\" : \"1\" , \"runtimeInfo\" : { \"projectName\" : project_name , \"runtimeConfigurationVersion\" : \"1.0.0\" , \"createdOn\" : datetime . utcnow () . strftime ( \"%Y-%m- %d T%H:%M:%SZ\" ), }, \"runtimeConfiguration\" : { \"devices\" : [{ \"name\" : \"IED1\" , \"address\" : \"localhost\" , # Optional \"arch\" : \"x86_64\" , # Optional, TODO: validate target keys }], \"components\" : [], }, } def _validate_with_schema ( name : str , data : dict , schema : str ): try : jsonschema . validate ( instance = data , schema = json . load ( module_resources . open_text ( \"simaticai.data.schemas\" , schema )) # TODO: after upgrading to python 3.9 # schema=json.load(resources.files(\"simaticai\") / \"data\" / \"schemas\" / \"pipeline.schema.json\") ) except jsonschema . exceptions . ValidationError as e : raise AssertionError ( f \"\"\"Schema validation failed for {name} using '{schema}'! message: {e.message} $id: {e.schema['$id']} title: {e.schema['title']} description: {e.schema['description']} \"\"\" ) from None def _get_pipeline_info ( pipeline_config : str ): pipeline_config = yaml_helper . read_yaml ( pipeline_config ) pipeline_info = pipeline_config [ \"dataFlowPipelineInfo\" ] pipeline_info [ \"packageType\" ] = pipeline_config . get ( \"packageType\" , \"full\" ) pipeline_info [ \"originVersion\" ] = pipeline_config . get ( \"originVersion\" , None ) return pipeline_info def _validate_delta_package_inputs ( origin_package_info : dict , new_package_info : dict ): if origin_package_info [ \"packageType\" ] == \"delta\" or new_package_info [ \"packageType\" ] == \"delta\" : raise AssertionError ( \"Neither of the packages can be delta package!\" ) if origin_package_info [ \"projectName\" ] != new_package_info [ \"projectName\" ]: raise AssertionError ( \"The new edge package must have the same name as the origin edge package!\" ) if origin_package_info [ \"dataFlowPipelineVersion\" ] == new_package_info [ \"dataFlowPipelineVersion\" ]: raise AssertionError ( \"The new edge package can not have the same version as the origin edge package!\" ) def _change_pipeline_config ( config_path : str , origin_package_version : str ): data = yaml_helper . read_yaml ( config_path ) data [ \"packageType\" ] = \"delta\" data [ \"originVersion\" ] = origin_package_version with open ( config_path , \"w\" ) as f : yaml . dump ( data , f ) def _extract_edge_package ( edge_package_zip_path : str , path_to_extract : Path ): zipfile . ZipFile ( edge_package_zip_path ) . extractall ( path_to_extract ) for f in path_to_extract . rglob ( \"*.zip\" ): component_path = path_to_extract / \"components\" / f . stem packages = Path ( component_path , PYTHON_PACKAGES_ZIP ) zipfile . ZipFile ( f ) . extractall ( component_path ) if packages . is_file (): zipfile . ZipFile ( component_path / PYTHON_PACKAGES_ZIP ) . extractall ( component_path / PYTHON_PACKAGES ) os . remove ( packages ) os . remove ( f ) return path_to_extract def _copy_file ( file_path : Path , from_dir : Path , to_dir : Path ): new_path = to_dir / file_path . relative_to ( from_dir ) new_path . parent . mkdir ( parents = True , exist_ok = True ) shutil . copy ( file_path , to_dir / file_path . relative_to ( from_dir )) def create_delta_package ( origin_edge_package_zip_path : str , new_edge_package_zip_path : str ): \"\"\" Creates a Delta Edge Configuration Package from two given Edge Configuration Packages. The created Delta Configuration Package is applicable to import into AI Inference Server, if the Original Edge Configuration Package is already imported there. The Delta Configuration Package only contains the additions and modifications in the New Edge Configuration Package compared to the Original one. That also means that no file deletion is possible in a deployed pipeline via this option. Please make sure that both of the given zip files come from a trusted source! Usage: ~~~python delta_package_path = deployment.create_delta_package('Edge-Config-edge-1.0.0.zip', 'Edge-Config-edge-1.1.0.zip') ~~~ This method can be used from the command line, too. ``` python -m simaticai create_delta_package <origin_package.zip> <modified_package.zip> ``` Once the package is calculated, you will have an `Edge-Config-edge-delta-1.1.0.zip` file beside the updated package zip file. <ul>This package will contain <li><ul>the three configuration file for the package; <li>pipeline_config.yml</li> <li>runtime_config.yml</li> <li>datalink_metadata.yml</li> </li></ul> <li>the newly added files,</li> <li>and the updated files.</li> </ul> The package will not contain any information on the deleted files and they will be copied from the original pipeline. **Caution!** *If you change the version of a component in the pipeline, the delta package will contain all the files of the component because AI Inference Server identifies a component with a different version as a different component!* Args: origin_edge_package_zip_path (str): Path to the origin edge configuration package zip file. new_edge_package_zip_path (str): Path to the new edge configuration package zip file. Returns: os.PathLike: The path of the created delta edge package zip file. Raises: AssertionError: When: - either of the given edge packages is a delta package or - the names of the given edge packages differ or - the versions of the given edge packages are equal. \"\"\" workdir = Path ( tempfile . mkdtemp ( prefix = \"aisdk_deltapack-\" )) delta_dir = Path ( workdir / \"delta\" ) delta_dir . mkdir ( parents = True ) origin_dir = _extract_edge_package ( origin_edge_package_zip_path , Path ( workdir / \"orig\" )) new_dir = _extract_edge_package ( new_edge_package_zip_path , Path ( workdir / \"new\" )) origin_package_info = _get_pipeline_info ( origin_dir / PIPELINE_CONFIG ) new_package_info = _get_pipeline_info ( new_dir / PIPELINE_CONFIG ) _validate_delta_package_inputs ( origin_package_info , new_package_info ) files_in_new_package = new_dir . rglob ( \"*\" ) for f in files_in_new_package : if f . is_dir (): continue orig_file_path = origin_dir / f . relative_to ( new_dir ) if not orig_file_path . exists (): _copy_file ( f , new_dir , delta_dir ) else : checksum_original = calc_sha ( orig_file_path ) checksum_new = calc_sha ( f ) if checksum_original != checksum_new : _copy_file ( f , new_dir , delta_dir ) _change_pipeline_config ( delta_dir / PIPELINE_CONFIG , origin_package_info [ \"dataFlowPipelineVersion\" ]) new_edge_package_zip_path = Path ( new_edge_package_zip_path ) delta_path = _zip_delta_package ( delta_dir , new_edge_package_zip_path ) shutil . rmtree ( workdir , ignore_errors = True ) return Path ( delta_path ) def _zip_delta_package ( delta_dir : Path , new_package_path : Path ): target_folder = new_package_path . parent splitted_name = str ( new_package_path . stem ) . split ( \"_\" ) target_name = \"_\" . join ( splitted_name [: - 1 ]) + \"_delta_\" + \"\" . join ( splitted_name [ - 1 :]) for dir in Path ( delta_dir / \"components\" ) . glob ( \"*\" ): if Path ( dir / PYTHON_PACKAGES ) . is_dir (): shutil . make_archive ( dir / PYTHON_PACKAGES , \"zip\" , dir / PYTHON_PACKAGES ) shutil . rmtree ( dir / PYTHON_PACKAGES ) shutil . make_archive ( dir , \"zip\" , dir ) shutil . rmtree ( dir ) delta_path = shutil . make_archive ( target_folder / target_name , \"zip\" , delta_dir ) return delta_path class _PipelinePage ( markup . page ): def __init__ ( self , pipeline : Pipeline ): super () . __init__ ( 'strict_html' , 'lower' ) self . twotags . append ( \"section\" ) self . init ( title = f \"{pipeline.name} ({pipeline.version})\" , doctype = \"<!DOCTYPE html>\" , charset = \"utf-8\" , lang = \"en\" ) self . section () self . h1 ( f \"Pipeline {pipeline.name} ({pipeline.version})\" ) if pipeline . desc : self . p ( pipeline . desc ) self . html_generate_parameters ( pipeline ) self . html_generate_pipeline_inputs ( pipeline ) self . html_generate_pipeline_outputs ( pipeline ) self . html_generate_io_wiring ( pipeline ) self . html_generate_timeshifting ( pipeline ) for component in pipeline . components . values (): self . html_generate_components ( component ) self . section . close () def html_generate_components ( self , component : Component ): self . hr () self . section () self . h1 ( f \"{component.__class__.__name__} {component.name}\" ) if component . desc : self . p ( component . desc ) self . html_generate_component_inputs ( component ) self . html_generate_component_outputs ( component ) self . html_generate_metrics ( component ) if issubclass ( component . __class__ , PythonComponent ): self . html_generate_resources ( component ) self . html_generate_entrypoints ( component ) self . section . close () def html_generate_parameters ( self , pipeline : Pipeline ): if len ( pipeline . parameters ) > 0 : self . strong ( \"Parameters\" ) self . ul () for name , parameter in pipeline . parameters . items (): self . li () self . i ( f \"{name} ({parameter['type']}, default: '{parameter['defaultValue']}')\" ) self . br () if parameter . get ( 'desc' ) is not None : self . span ( parameter [ 'desc' ]) self . li . close () self . ul . close () def html_generate_pipeline_inputs ( self , pipeline : Pipeline ): if len ( pipeline . inputs ) > 0 : self . strong ( \"Inputs\" ) self . ul () for component , name in pipeline . inputs : input = pipeline . components [ component ] . inputs [ name ] self . li () self . i ( f \"{name} ({input['type']})\" ) self . br () if input . get ( 'desc' ) is not None : self . span ( input [ 'desc' ]) self . li . close () self . ul . close () def html_generate_pipeline_outputs ( self , pipeline : Pipeline ): if len ( pipeline . outputs ) > 0 : self . strong ( \"Outputs\" ) self . ul () for component , name in pipeline . outputs : output = pipeline . components [ component ] . outputs [ name ] self . li () self . i ( f \"{name} ({output['type']})\" ) self . br () if output . get ( 'desc' ) is not None : self . span ( output [ 'desc' ]) self . li . close () self . ul . close () def html_generate_component_inputs ( self , component : Component ): self . strong ( \"Inputs\" ) self . ul () for name , input in component . inputs . items (): self . li () self . i ( f \"{name} ({input['type']})\" ) self . br () if input . get ( 'desc' ) is not None : self . span ( input [ 'desc' ]) self . li . close () self . ul . close () def html_generate_component_outputs ( self , component : Component ): self . strong ( \"Outputs\" ) self . ul () for name , output in component . outputs . items (): self . li () self . i ( f \"{name} ({output['type']})\" ) self . br () if output . get ( 'desc' ) is not None : self . span ( output [ 'desc' ]) self . li . close () self . ul . close () def html_generate_io_wiring ( self , pipeline : Pipeline ): if len ( pipeline . wiring ) > 0 : self . strong ( \"I/O Wiring\" ) self . ul () for component , name in pipeline . inputs : self . li ( f \"{name} &#8594; {component}.{name}\" ) for wire_hash in pipeline . wiring : self . li ( wire_hash . replace ( \"->\" , \"&#8594;\" )) for component , name in pipeline . outputs : self . li ( f \"{component}.{name} &#8594; {name}\" ) self . ul . close () def html_generate_timeshifting ( self , pipeline : Pipeline ): if pipeline . periodicity is not None : self . strong ( \"Timeshifting\" ) self . ul () self . li ( f \"Periodicity: {pipeline.periodicity} ms\" ) self . ul . close () if len ( pipeline . timeshift_reference ) > 0 : self . strong ( \"References\" ) self . ul () for ref in pipeline . timeshift_reference : self . li ( ref ) self . ul . close () def html_generate_resources ( self , component : Component ): if isinstance ( component , PythonComponent ) and component . resources is not None and len ( component . resources ) > 0 : self . strong ( \"Resources\" ) self . ul () for path , base in component . resources . items (): self . li ( f \"{base}/{path.name}\" . replace ( './' , '' )) self . ul . close () def html_generate_entrypoints ( self , component : Component ): if isinstance ( component , PythonComponent ) and component . entrypoint is not None : self . strong ( \"Entrypoint\" ) self . ul () self . li ( component . entrypoint . name ) self . ul . close () def html_generate_metrics ( self , component : Component ): if isinstance ( component , PythonComponent ) and component . metrics is not None and len ( component . metrics ) > 0 : self . strong ( \"Metrics\" ) self . ul () for name , metric in component . metrics . items (): self . li () self . i ( name ) if metric . get ( 'desc' ) is not None : self . br () self . span ( metric [ 'desc' ]) self . li . close () self . ul . close () class GPURuntimeComponent ( Component ): \"\"\" The GPURuntimeComponent is used to define a component that runs on a GPU device. The component works only with ONNX models and can be used in an Inference Pipeline. Attributes: name (str): Component name. version (str): Component version. desc (str): Component description. Methods: use_model(self, path: Union[Path, str], max_batch_size: int, optimization: Optional[model_config.TensorRTOptimization] = None, warmup: model_config.Warmup = None) Add an ONNX model file for the component. use_config(self, path: Union[Path, str]) Use a custom config.pbtxt file instead of the autogenerated one. save(self, destination: Union[Path, str], validate = False) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. \"\"\" def __init__ ( self , name : str = \"inference\" , version : str = \"1\" , desc : str = \"\" ): \"\"\" Creates a new, empty GPU Runtime component. Args: name (str): Component name. (default: inference) version (str): Component version. (default: 1) desc (str): Component description (optional) \"\"\" super () . __init__ ( name = name , desc = desc ) self . version = version self . entrypoint : Union [ Path , None ] = None self . model_path : Union [ Path , None ] = None self . model_version : str = \"1\" self . config : Union [ Path , None ] = None self . auto_config = None def _to_dict ( self ): return { ** super () . _to_dict (), 'version' : self . version , 'entrypoint' : f \"{self.model_version}/{self.entrypoint.name}\" , 'hwType' : 'GPU' , 'runtime' : { 'type' : 'gpuruntime' , 'version' : '0.1.0' , } } def use_model ( self , path : Union [ Path , str ], max_batch_size : int , optimization : Optional [ model_config . TensorRTOptimization ] = None , warmup : model_config . Warmup = None ): \"\"\" Add the ONNX model file for the component. Args: path (Union[Path, str]): The path to the ONNX model file. max_batch_size (int): The maximum batch size for the model. optimization (model_config.TensorRTOptimization, optional): The optimization configuration for the model. Defaults to None. warmup (model_config.Warmup, optional): The warmup configuration for the model. Defaults to None. Raises: AssertionError: If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified model file not found: '{path}'\" ) if path . suffix != \".onnx\" : raise AssertionError ( f \"model file extension is not '.onnx': '{path}'\" ) if max_batch_size < 0 : raise AssertionError ( \"max_batch_size must be greater or equal to 0\" ) self . entrypoint = Path ( \"model.onnx\" ) self . model_path = path if self . config is not None : _logger . warning ( \"Previously added configuration was removed. Component will use the default configuration unless you specify your own.\" ) self . config = None # Remove old automatic variables if self . auto_config is not None : for var in self . auto_config . inputs : self . delete_input ( var [ \"name\" ]) for var in self . auto_config . outputs : self . delete_output ( var [ \"name\" ]) self . auto_config = model_config . ModelConfig ( onnx_path = path , max_batch_size = max_batch_size , warmup = warmup , optimization = optimization ) for var in self . auto_config . inputs : self . add_input ( var [ \"name\" ], var [ \"type\" ]) for var in self . auto_config . outputs : self . add_output ( var [ \"name\" ], var [ \"type\" ]) def use_config ( self , path : Union [ Path , str ]): \"\"\" Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Args: path (Union[Path, str]): The path to the configuration file. Raises: AssertionError: If the specified config file is not found or has an invalid extension. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified config file not found: '{path}'\" ) if path . suffix != \".pbtxt\" : raise AssertionError ( f \"config file extension is not '.pbtxt': '{path}'\" ) _validate_gpuruntime_config ( path ) self . config = path def save ( self , destination : Union [ Path , str ], validate = False ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: - An `.onnx` model file - A `.pbtxt` configuration file Args: destination (path-like): Target directory to which the component will be saved. \"\"\" if self . entrypoint is None : raise AssertionError ( \"An ONNX model file must be specified before the component can be saved.\" ) component_dir = Path ( destination ) / self . name component_dir . mkdir ( parents = True , exist_ok = True ) model_dir = component_dir / self . model_version model_dir . mkdir ( exist_ok = True ) shutil . copy ( self . model_path , model_dir / \"model.onnx\" ) if self . config is None : _logger . warning ( \"Configuration was not specified. Model will be saved with default configuration.\" ) ( component_dir / \"config.pbtxt\" ) . write_text ( f \"{self.auto_config}\" ) else : shutil . copy ( self . config , component_dir ) def _validate_gpuruntime_config ( path : Union [ Path , str ]): with open ( path , 'r' ) as file : text_format . Parse ( file . read (), model_config_pb2 . ModelConfig ()) Variables DATALINK_METADATA MSG_NOT_FOUND PIPELINE_CONFIG PIPELINE_SIZE_LIMIT PYTHON_PACKAGES PYTHON_PACKAGES_ZIP README_HTML REQUIREMENTS_TXT RUNTIME_CONFIG TELEMETRY_YAML supported_types Functions convert_package def convert_package ( zip_path : str , report_writer : Optional [ simaticai . helpers . reporter . PipelineReportWriter ] = None ) -> pathlib . Path @Deprecated, reason: only edge package generation will be supported in the future. Use Pipeline.export(...) instead. Create an Edge Configuration Package from a given Pipeline Configuration Package. If the input zip file is {path}/{name}_{version}.zip , the output file will be created as {path}/{name}-edge_{version}.zip . Please make sure that the given zip file comes from a trusted source! If a file with such a name already exists, it is overwritten. First, this method verifies that the requirements identified by name and version are either included in PythonPackages.zip or available on pypi.org for the target platform. Currently, the supported edge devices run Linux on 64-bit x86 architecture, so the accepted Python libraries are restricted to the platform independent ones and packages built for 'x86_64' platforms. AI Inference Server also provides a Python 3.10 and runtime environment, so the supported Python libraries are restricted to Python 3.10 and 3.11 compatible packages. If for the target platform the required dependency is not available on pypi.org and not present in PythonPackages.zip , it will log the problem at ERROR level. Then it downloads all dependencies (either direct or transitive), and creates a new zip file, which is validated against the AI Inference Server's schema. This functionality requires pip with version of 21.3.1 or greater. This method can be used from the command line too. Example usage: python - m simaticai convert_package < path_to_pipeline_configuration_package . zip > Parameters: Name Type Description Default zip_path path-like path to the pipeline configuration package zip file. None report_writer ReportWriter a ReportWriter object to write the report for a pipeline. Defaults to None. None Returns: Type Description os.PathLike The path of the created zip file. Raises: Type Description PipelineValidationError If the validation fails. See the logger output for details. View Source def convert_package ( zip_path : str or os . PathLike , report_writer : Optional [ PipelineReportWriter ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use Pipeline.export(...) instead. Create an Edge Configuration Package from a given Pipeline Configuration Package. If the input zip file is `{path}/{name}_{version}.zip`, the output file will be created as `{path}/{name}-edge_{version}.zip`. Please make sure that the given zip file comes from a trusted source! If a file with such a name already exists, it is overwritten. First, this method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. Currently, the supported edge devices run Linux on 64-bit x86 architecture, so the accepted Python libraries are restricted to the platform independent ones and packages built for 'x86_64' platforms. AI Inference Server also provides a Python 3.10 and runtime environment, so the supported Python libraries are restricted to Python 3.10 and 3.11 compatible packages. If for the target platform the required dependency is not available on pypi.org and not present in `PythonPackages.zip`, it will log the problem at ERROR level. Then it downloads all dependencies (either direct or transitive), and creates a new zip file, which is validated against the AI Inference Server's schema. This functionality requires pip with version of 21.3.1 or greater. This method can be used from the command line too. Example usage: ``` python -m simaticai convert_package <path_to_pipeline_configuration_package.zip> ``` Args: zip_path (path-like): path to the pipeline configuration package zip file. report_writer (ReportWriter, optional): a ReportWriter object to write the report for a pipeline. Defaults to None. Returns: os.PathLike: The path of the created zip file. Exceptions: PipelineValidationError: If the validation fails. See the logger output for details. \"\"\" zip_path = Path ( zip_path ) if zip_path . stem . find ( '_' ) < 0 : raise AssertionError ( \"The input zip file name must contain an underscore character.\" ) with tempfiles . OpenZipInTemp ( zip_path ) as zip_dir : top_level_items = list ( zip_dir . iterdir ()) if len ( top_level_items ) != 1 : raise AssertionError ( \"The Pipeline Configuration Package must contain a single top level directory.\" ) package_dir = zip_dir / top_level_items [ 0 ] runtime_dir = zip_dir / \"edge_config_package\" runtime_dir . mkdir ( parents = True , exist_ok = True ) config = yaml_helper . read_yaml ( package_dir / PIPELINE_CONFIG ) _validate_with_schema ( \"input pipeline_config.yml\" , config , \"pipeline.schema.json\" ) runtime_config = _generate_runtime_config ( config ) if report_writer is not None : # TODO: consider moving zip_path to the parameter of report_writer.write_report() report_writer . set_path ( Path ( zip_path . parent / f \"{zip_path.stem}_package_report.md\" )) report_writer . set_pipeline_config ( config ) for component in config [ 'dataFlowPipeline' ][ 'components' ]: source_dir = package_dir / component [ \"name\" ] if component [ \"runtime\" ][ \"type\" ] == \"python\" : python_version = component [ 'runtime' ][ 'version' ] try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) dependency_set = _package_component_dependencies ( source_dir , python_version ) if report_writer is not None : report_writer . add_full_dependency_set ( component_name = component [ \"name\" ], dependency_set = dependency_set ) runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"Python\" , }) if component [ \"runtime\" ][ \"type\" ] == \"gpuruntime\" : runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"gpuruntime\" , }) _package_component ( source_dir , runtime_dir / 'components' / f \"{component['name']}_{component['version']}\" ) if report_writer is not None : report_writer . write_report () _logger . info ( f \"Report on {zip_path.stem} is saved to {zip_path.parent}.\" ) shutil . copy ( str ( package_dir / PIPELINE_CONFIG ), str ( runtime_dir / PIPELINE_CONFIG )) datalink_metadata_yaml = package_dir / DATALINK_METADATA if datalink_metadata_yaml . is_file (): shutil . copy ( str ( datalink_metadata_yaml ), runtime_dir / DATALINK_METADATA ) _validate_with_schema ( f \"generated {RUNTIME_CONFIG}\" , runtime_config , \"runtime.schema.json\" ) with open ( runtime_dir / RUNTIME_CONFIG , \"w\" , encoding = \"utf8\" ) as file : yaml . dump ( runtime_config , file ) readme_html = package_dir / README_HTML if readme_html . exists (): ( runtime_dir / README_HTML ) . write_text ( readme_html . read_text ()) telemetry_yaml = package_dir / TELEMETRY_YAML if telemetry_yaml . exists (): ( runtime_dir / TELEMETRY_YAML ) . write_text ( telemetry_yaml . read_text ()) edge_package_path = Path ( shutil . make_archive ( # One Pythonic Way to replace the last occurrence of \"_\" with \"-edge\". base_name = str ( PurePath ( zip_path ) . parent / \"-edge_\" . join ( zip_path . stem . rsplit ( \"_\" , 1 ))), format = 'zip' , root_dir = runtime_dir , verbose = True , logger = _logger )) sha256_hash = calc_sha ( edge_package_path ) sha_format = f \"{sha256_hash} {edge_package_path.name}\" edge_package_path . with_suffix ( '.sha256' ) . write_text ( sha_format ) return edge_package_path create_delta_package def create_delta_package ( origin_edge_package_zip_path : str , new_edge_package_zip_path : str ) Creates a Delta Edge Configuration Package from two given Edge Configuration Packages. The created Delta Configuration Package is applicable to import into AI Inference Server, if the Original Edge Configuration Package is already imported there. The Delta Configuration Package only contains the additions and modifications in the New Edge Configuration Package compared to the Original one. That also means that no file deletion is possible in a deployed pipeline via this option. Please make sure that both of the given zip files come from a trusted source! Usage: delta_package_path = deployment . create_delta_package ( 'Edge-Config-edge-1.0.0.zip' , 'Edge-Config-edge-1.1.0.zip' ) This method can be used from the command line, too. python - m simaticai create_delta_package < origin_package . zip > < modified_package . zip > Once the package is calculated, you will have an Edge-Config-edge-delta-1.1.0.zip file beside the updated package zip file. This package will contain the three configuration file for the package; pipeline_config.yml runtime_config.yml datalink_metadata.yml the newly added files, and the updated files. The package will not contain any information on the deleted files and they will be copied from the original pipeline. Caution! If you change the version of a component in the pipeline, the delta package will contain all the files of the component because AI Inference Server identifies a component with a different version as a different component! Parameters: Name Type Description Default origin_edge_package_zip_path str Path to the origin edge configuration package zip file. None new_edge_package_zip_path str Path to the new edge configuration package zip file. None Returns: Type Description os.PathLike The path of the created delta edge package zip file. Raises: Type Description AssertionError When: - either of the given edge packages is a delta package or - the names of the given edge packages differ or - the versions of the given edge packages are equal. View Source def create_delta_package ( origin_edge_package_zip_path : str , new_edge_package_zip_path : str ): \"\"\" Creates a Delta Edge Configuration Package from two given Edge Configuration Packages. The created Delta Configuration Package is applicable to import into AI Inference Server, if the Original Edge Configuration Package is already imported there. The Delta Configuration Package only contains the additions and modifications in the New Edge Configuration Package compared to the Original one. That also means that no file deletion is possible in a deployed pipeline via this option. Please make sure that both of the given zip files come from a trusted source! Usage: ~~~python delta_package_path = deployment.create_delta_package('Edge-Config-edge-1.0.0.zip', 'Edge-Config-edge-1.1.0.zip') ~~~ This method can be used from the command line, too. ``` python -m simaticai create_delta_package <origin_package.zip> <modified_package.zip> ``` Once the package is calculated, you will have an `Edge-Config-edge-delta-1.1.0.zip` file beside the updated package zip file. <ul>This package will contain <li><ul>the three configuration file for the package; <li>pipeline_config.yml</li> <li>runtime_config.yml</li> <li>datalink_metadata.yml</li> </li></ul> <li>the newly added files,</li> <li>and the updated files.</li> </ul> The package will not contain any information on the deleted files and they will be copied from the original pipeline. **Caution!** *If you change the version of a component in the pipeline, the delta package will contain all the files of the component because AI Inference Server identifies a component with a different version as a different component!* Args: origin_edge_package_zip_path (str): Path to the origin edge configuration package zip file. new_edge_package_zip_path (str): Path to the new edge configuration package zip file. Returns: os.PathLike: The path of the created delta edge package zip file. Raises: AssertionError: When: - either of the given edge packages is a delta package or - the names of the given edge packages differ or - the versions of the given edge packages are equal. \"\"\" workdir = Path ( tempfile . mkdtemp ( prefix = \"aisdk_deltapack-\" )) delta_dir = Path ( workdir / \"delta\" ) delta_dir . mkdir ( parents = True ) origin_dir = _extract_edge_package ( origin_edge_package_zip_path , Path ( workdir / \"orig\" )) new_dir = _extract_edge_package ( new_edge_package_zip_path , Path ( workdir / \"new\" )) origin_package_info = _get_pipeline_info ( origin_dir / PIPELINE_CONFIG ) new_package_info = _get_pipeline_info ( new_dir / PIPELINE_CONFIG ) _validate_delta_package_inputs ( origin_package_info , new_package_info ) files_in_new_package = new_dir . rglob ( \"*\" ) for f in files_in_new_package : if f . is_dir (): continue orig_file_path = origin_dir / f . relative_to ( new_dir ) if not orig_file_path . exists (): _copy_file ( f , new_dir , delta_dir ) else : checksum_original = calc_sha ( orig_file_path ) checksum_new = calc_sha ( f ) if checksum_original != checksum_new : _copy_file ( f , new_dir , delta_dir ) _change_pipeline_config ( delta_dir / PIPELINE_CONFIG , origin_package_info [ \"dataFlowPipelineVersion\" ]) new_edge_package_zip_path = Path ( new_edge_package_zip_path ) delta_path = _zip_delta_package ( delta_dir , new_edge_package_zip_path ) shutil . rmtree ( workdir , ignore_errors = True ) return Path ( delta_path ) find_dependencies def find_dependencies ( name : str , dependencies : dict ) @Deprecated, reason: uses 'pip show' which only works for installed packages on the current platform. Collects all dependencies of the Python module given with its name in the current Python environment. All inherited dependencies will be added to the dependencies dictionary with the installed version of the module. The method executes an OS command like python -m pip show scikit-learn . Parameters: Name Type Description Default name str Name of the Python module to be searched through for its dependencies. None dependencies dict Dictionary to collect the dependencies with the module name as key, and the installed version as value. None Returns: Type Description dict The dependencies dictionary with the collected module names and versions. View Source def find_dependencies ( name : str , dependencies : dict ) : \" \"\" @Deprecated, reason: uses 'pip show' which only works for installed packages on the current platform. Collects all dependencies of the Python module given with its `name` in the current Python environment. All inherited dependencies will be added to the `dependencies` dictionary with the installed version of the module. The method executes an OS command like `python -m pip show scikit-learn`. Args: name (str): Name of the Python module to be searched through for its dependencies. dependencies (dict): Dictionary to collect the dependencies with the module name as key, and the installed version as value. Returns: dict: The `dependencies` dictionary with the collected module names and versions. \"\" \" cmd_line = [ sys . executable , '-m' , 'pip' , 'show' , name ] result = subprocess . run ( cmd_line , stdout = subprocess . PIPE , text = True ) if result . returncode != 0 : print ( f \"Dependency {name} is not found and cannot be added.\" ) return dependencies version = None for line in result . stdout . splitlines () : version_matches = _version_matcher . match ( line ) if version_matches : version = version_matches . groups () [ 0 ] . strip () transitive_matches = _transitive_matcher . match ( line ) if transitive_matches : transitives = transitive_matches . groups () [ 0 ] . split ( \", \" ) for dependency in transitives : if dependency not in dependencies : find_dependencies ( dependency , dependencies ) if name not in dependencies : spec = pep508 . Spec ( name , [] , [ ( '==' , version ) ] if version else [] , None ) dependencies [ name ] = spec print ( \"Found:\" , spec ) return dependencies python_version_validator def python_version_validator ( version : str ) Checks if Python version string is valid and describes supported version. Only version 3.10 and 3.11 is supported. A patch version is optional and accepted but logs a warning. Accepted syntaxes are: - {major}.{minor} - {major}.{minor}.{patch} Parameters: Name Type Description Default version str Python version string None Raises: Type Description ValueError if the provided version is not supported View Source def python_version_validator(version: str): \"\"\" Checks if Python version string is valid and describes supported version. Only version 3.10 and 3.11 is supported. A patch version is optional and accepted but logs a warning. Accepted syntaxes are: - {major}.{minor} - {major}.{minor}.{patch} Args: version (str): Python version string Raises: ValueError: if the provided version is not supported \"\"\" supported_versions = [\"3.10\", \"3.11\"] error_message = \"The defined python version is not supported. Currently supported Python versions are 3.10 and 3.11. Python version must be specified only with major and minor version, e.g. '3.10'.\" warning_message = \"\"\"Required Python version was specified with patch version. Please note that the patch digit of the required Python version is often not taken into account by the Python ecosystem, so there is no guarantee it has the desired effect.\"\"\" python_version_matcher = re.match(r'^(3)\\.(0|[1-9][0-9]*)\\.?(0|[1-9][0-9]*)?$', str(version)) major_minor_version = \"0.0\" has_patch_version = False if python_version_matcher is not None: major_minor_version = f\"{python_version_matcher.group(1)}.{python_version_matcher.group(2)}\" has_patch_version = python_version_matcher.group(3) is not None if major_minor_version not in supported_versions: raise ValueError(error_message) if has_patch_version: _logger.warning(warning_message) Classes Component Base class for pipeline components, with name, description, and a list of inputs and outputs. A new component is created with the given name and an empty input and output list. class Component ( name : str , desc : str = '' ) Attributes Name Type Description Default name str Name of the component None desc str Optional description of the component None inputs dict Dictionary of (name, type) pairs, which describe the input variables None outputs dict Dictionary of (name, type) pairs, which describe the output variables None View Source class Component : \"\"\" Base class for pipeline components, with name, description, and a list of inputs and outputs. A new component is created with the given name and an empty input and output list. Args: name (str): Name of the component desc (str): Optional description of the component inputs (dict): Dictionary of (name, type) pairs, which describe the input variables outputs (dict): Dictionary of (name, type) pairs, which describe the output variables \"\"\" reserved_names = [ \"timestamp\" ] @ dataclass class BatchInfo : \"\"\" Batch information for the component. This attribute specifies whether the component can handle batch input or output data. When set to True, the component will receive data in the form of a list of dictionaries instead of a single dictionary. It is important to note that the input and output variables on the component should still be defined as if they are single variables. If the input of the pipeline is configured for batch processing, it is recommended not to configure timeshifting, as the list will have the same timestamp for all elements, potentially resulting in data loss. \"\"\" inputBatch : bool = False outputBatch : bool = False def dict ( self ): return { 'inputBatch' : 'Yes' if self . inputBatch is True else 'No' , 'outputBatch' : 'Yes' if self . outputBatch is True else 'No' } def __init__ ( self , name : str , desc : str = \"\" ): \"\"\" Creates a new component with the given name and an empty input and output list. Args: name (str): Name of the component. desc (str): Optional description of the component \"\"\" self . name = name self . desc = desc self . inputs = {} self . outputs = {} self . batch = self . BatchInfo ( False , False ) def __repr__ ( self ) -> str : text = f \"[{self.__class__.__name__}] {self.name} \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Component Inputs: \\n \" for name , input in self . inputs . items (): text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Component Outputs: \\n \" for name , output in self . outputs . items (): text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" return text def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name ) def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name ) def _to_dict ( self ): inputs = [] inputs += [{ 'name' : name , 'type' : self . inputs [ name ][ 'type' ], } for name in self . inputs ] outputs = [] outputs += [{ 'name' : name , 'type' : self . outputs [ name ][ 'type' ], 'metric' : False , } for name in self . outputs ] return { 'name' : self . name , 'description' : self . desc , 'batch' : self . batch . dict (), 'inputType' : inputs , 'outputType' : outputs , } def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass def save ( self , destination , validate ): \"\"\" Empty method for child classess to implement. \"\"\" pass Descendants simaticai.deployment.PythonComponent simaticai.deployment.GPURuntimeComponent Class variables BatchInfo reserved_names Methods add_input def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector payload = { \"image\" : { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height , \"mimeType\" : [ \"image/raw\" ], \"dataType\" : \"uint8\" , \"channelsPerPixel\" : 3 , \"image\" : _swap_bytes ( image . tobytes ()) } } Between components the format is the same format as the format of Object as an output. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new input. None _type str Type of the new input. None desc str Description of the input. (optional) None View Source def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc add_output def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type Object the entrypoint must return with a dictionary containing two fields, where one field has type str and the other field has type bytes . The example below shows the required format, assuming that 'image' is a PIL Image. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new output. None _type str Type of the new output. None desc str Description of the output. (optional) None View Source def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc change_input def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the inputs of the component. Parameters: Name Type Description Default name str Name of the input to be changed. None _type str New type of the input. None desc str Description of the input. (optional) None View Source def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc change_output def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the outputs of the component. Parameters: Name Type Description Default name str Name of the output to be changed. None _type str The new type of the output. None desc str Description of the output. (optional) None View Source def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc delete_input def delete_input ( self , name : str ) Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use package.delete_input_wire(...) with default parameter with_input=True . Parameters: Name Type Description Default name str Name of the input to be deleted. None View Source def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name ) delete_output def delete_output ( self , name : str ) Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Parameters: Name Type Description Default name str Name of the output to be deleted. None View Source def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name ) save def save ( self , destination , validate ) Empty method for child classess to implement. View Source def save ( self , destination , validate ): \"\"\" Empty method for child classess to implement. \"\"\" pass validate def validate ( self ) Empty method for child classess to implement. View Source def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass GPURuntimeComponent The GPURuntimeComponent is used to define a component that runs on a GPU device. The component works only with ONNX models and can be used in an Inference Pipeline. class GPURuntimeComponent ( name : str = 'inference' , version : str = '1' , desc : str = '' ) Attributes Name Type Description Default name str Component name. None version str Component version. None desc str Component description. None View Source class GPURuntimeComponent ( Component ): \"\"\" The GPURuntimeComponent is used to define a component that runs on a GPU device. The component works only with ONNX models and can be used in an Inference Pipeline. Attributes: name (str): Component name. version (str): Component version. desc (str): Component description. Methods: use_model(self, path: Union[Path, str], max_batch_size: int, optimization: Optional[model_config.TensorRTOptimization] = None, warmup: model_config.Warmup = None) Add an ONNX model file for the component. use_config(self, path: Union[Path, str]) Use a custom config.pbtxt file instead of the autogenerated one. save(self, destination: Union[Path, str], validate = False) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. \"\"\" def __init__ ( self , name : str = \"inference\" , version : str = \"1\" , desc : str = \"\" ): \"\"\" Creates a new, empty GPU Runtime component. Args: name (str): Component name. (default: inference) version (str): Component version. (default: 1) desc (str): Component description (optional) \"\"\" super (). __init__ ( name = name , desc = desc ) self . version = version self . entrypoint : Union [ Path , None ] = None self . model_path : Union [ Path , None ] = None self . model_version : str = \"1\" self . config : Union [ Path , None ] = None self . auto_config = None def _to_dict ( self ): return { ** super (). _to_dict (), ' version ' : self . version , ' entrypoint ' : f \"{self.model_version}/{self.entrypoint.name}\" , ' hwType ' : ' GPU ' , ' runtime ' : { ' type ' : ' gpuruntime ' , ' version ' : ' 0.1.0 ' , } } def use_model ( self , path : Union [ Path , str ], max_batch_size : int , optimization : Optional [ model_config . TensorRTOptimization ] = None , warmup : model_config . Warmup = None ): \"\"\" Add the ONNX model file for the component. Args: path (Union[Path, str]): The path to the ONNX model file. max_batch_size (int): The maximum batch size for the model. optimization (model_config.TensorRTOptimization, optional): The optimization configuration for the model. Defaults to None. warmup (model_config.Warmup, optional): The warmup configuration for the model. Defaults to None. Raises: AssertionError: If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified model file not found: '{path}'\" ) if path . suffix != \".onnx\" : raise AssertionError ( f \"model file extension is not '.onnx': '{path}'\" ) if max_batch_size < 0 : raise AssertionError ( \"max_batch_size must be greater or equal to 0\" ) self . entrypoint = Path ( \"model.onnx\" ) self . model_path = path if self . config is not None : _logger . warning ( \"Previously added configuration was removed. Component will use the default configuration unless you specify your own.\" ) self . config = None # Remove old automatic variables if self . auto_config is not None : for var in self . auto_config . inputs : self . delete_input ( var [ \"name\" ]) for var in self . auto_config . outputs : self . delete_output ( var [ \"name\" ]) self . auto_config = model_config . ModelConfig ( onnx_path = path , max_batch_size = max_batch_size , warmup = warmup , optimization = optimization ) for var in self . auto_config . inputs : self . add_input ( var [ \"name\" ], var [ \"type\" ]) for var in self . auto_config . outputs : self . add_output ( var [ \"name\" ], var [ \"type\" ]) def use_config ( self , path : Union [ Path , str ]): \"\"\" Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Args: path (Union[Path, str]): The path to the configuration file. Raises: AssertionError: If the specified config file is not found or has an invalid extension. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified config file not found: '{path}'\" ) if path . suffix != \".pbtxt\" : raise AssertionError ( f \"config file extension is not '.pbtxt': '{path}'\" ) _validate_gpuruntime_config ( path ) self . config = path def save ( self , destination : Union [ Path , str ], validate = False ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: - An `.onnx` model file - A `.pbtxt` configuration file Args: destination (path-like): Target directory to which the component will be saved. \"\"\" if self . entrypoint is None : raise AssertionError ( \"An ONNX model file must be specified before the component can be saved.\" ) component_dir = Path ( destination ) / self . name component_dir . mkdir ( parents = True , exist_ok = True ) model_dir = component_dir / self . model_version model_dir . mkdir ( exist_ok = True ) shutil . copy ( self . model_path , model_dir / \"model.onnx\" ) if self . config is None : _logger . warning ( \"Configuration was not specified. Model will be saved with default configuration.\" ) ( component_dir / \"config.pbtxt\" ). write_text ( f \"{self.auto_config}\" ) else : shutil . copy ( self . config , component_dir ) Ancestors (in MRO) simaticai.deployment.Component Class variables BatchInfo reserved_names Methods add_input def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector payload = { \"image\" : { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height , \"mimeType\" : [ \"image/raw\" ], \"dataType\" : \"uint8\" , \"channelsPerPixel\" : 3 , \"image\" : _swap_bytes ( image . tobytes ()) } } Between components the format is the same format as the format of Object as an output. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new input. None _type str Type of the new input. None desc str Description of the input. (optional) None View Source def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc add_output def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type Object the entrypoint must return with a dictionary containing two fields, where one field has type str and the other field has type bytes . The example below shows the required format, assuming that 'image' is a PIL Image. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new output. None _type str Type of the new output. None desc str Description of the output. (optional) None View Source def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc change_input def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the inputs of the component. Parameters: Name Type Description Default name str Name of the input to be changed. None _type str New type of the input. None desc str Description of the input. (optional) None View Source def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc change_output def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the outputs of the component. Parameters: Name Type Description Default name str Name of the output to be changed. None _type str The new type of the output. None desc str Description of the output. (optional) None View Source def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc delete_input def delete_input ( self , name : str ) Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use package.delete_input_wire(...) with default parameter with_input=True . Parameters: Name Type Description Default name str Name of the input to be deleted. None View Source def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name ) delete_output def delete_output ( self , name : str ) Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Parameters: Name Type Description Default name str Name of the output to be deleted. None View Source def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name ) save def save ( self , destination : Union [ pathlib . Path , str ], validate = False ) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: An .onnx model file A .pbtxt configuration file Parameters: Name Type Description Default destination path-like Target directory to which the component will be saved. None View Source def save ( self , destination : Union [ Path , str ], validate = False ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: - An `.onnx` model file - A `.pbtxt` configuration file Args: destination (path-like): Target directory to which the component will be saved. \"\"\" if self . entrypoint is None : raise AssertionError ( \"An ONNX model file must be specified before the component can be saved.\" ) component_dir = Path ( destination ) / self . name component_dir . mkdir ( parents = True , exist_ok = True ) model_dir = component_dir / self . model_version model_dir . mkdir ( exist_ok = True ) shutil . copy ( self . model_path , model_dir / \"model.onnx\" ) if self . config is None : _logger . warning ( \"Configuration was not specified. Model will be saved with default configuration.\" ) ( component_dir / \"config.pbtxt\" ). write_text ( f \"{self.auto_config}\" ) else : shutil . copy ( self . config , component_dir ) use_config def use_config ( self , path : Union [ pathlib . Path , str ] ) Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Parameters: Name Type Description Default path Union[Path, str] The path to the configuration file. None Raises: Type Description AssertionError If the specified config file is not found or has an invalid extension. View Source def use_config(self, path: Union[Path, str]): \"\"\" Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Args: path (Union[Path, str]): The path to the configuration file. Raises: AssertionError: If the specified config file is not found or has an invalid extension. \"\"\" path = Path(path) if not path.is_file(): raise AssertionError(f\"specified config file not found: '{path}'\") if path.suffix != \".pbtxt\": raise AssertionError(f\"config file extension is not '.pbtxt': '{path}'\") _validate_gpuruntime_config(path) self.config = path use_model def use_model ( self , path : Union [ pathlib . Path , str ], max_batch_size : int , optimization : Optional [ simaticai . helpers . model_config . TensorRTOptimization ] = None , warmup : simaticai . helpers . model_config . Warmup = None ) Add the ONNX model file for the component. Parameters: Name Type Description Default path Union[Path, str] The path to the ONNX model file. None max_batch_size int The maximum batch size for the model. None optimization model_config.TensorRTOptimization The optimization configuration for the model. Defaults to None. None warmup model_config.Warmup The warmup configuration for the model. Defaults to None. None Raises: Type Description AssertionError If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. View Source def use_model ( self , path : Union [ Path , str ], max_batch_size : int , optimization : Optional [ model_config . TensorRTOptimization ] = None , warmup : model_config . Warmup = None ): \"\"\" Add the ONNX model file for the component. Args: path (Union[Path, str]): The path to the ONNX model file. max_batch_size (int): The maximum batch size for the model. optimization (model_config.TensorRTOptimization, optional): The optimization configuration for the model. Defaults to None. warmup (model_config.Warmup, optional): The warmup configuration for the model. Defaults to None. Raises: AssertionError: If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified model file not found: '{path}'\" ) if path . suffix != \".onnx\" : raise AssertionError ( f \"model file extension is not '.onnx': '{path}'\" ) if max_batch_size < 0 : raise AssertionError ( \"max_batch_size must be greater or equal to 0\" ) self . entrypoint = Path ( \"model.onnx\" ) self . model_path = path if self . config is not None : _logger . warning ( \"Previously added configuration was removed. Component will use the default configuration unless you specify your own.\" ) self . config = None # Remove old automatic variables if self . auto_config is not None : for var in self . auto_config . inputs : self . delete_input ( var [ \"name\" ]) for var in self . auto_config . outputs : self . delete_output ( var [ \"name\" ]) self . auto_config = model_config . ModelConfig ( onnx_path = path , max_batch_size = max_batch_size , warmup = warmup , optimization = optimization ) for var in self . auto_config . inputs : self . add_input ( var [ \"name\" ], var [ \"type\" ]) for var in self . auto_config . outputs : self . add_output ( var [ \"name\" ], var [ \"type\" ]) validate def validate ( self ) Empty method for child classess to implement. View Source def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass Pipeline Pipeline represents a pipeline configuration package with Components and wires to provide a data flow on the AI Inference Server. The Components have inputs and outputs to transfer data to each other and the wires describe this data flow between them. The package also contains configuration files required to deploy a pipeline on an Industrial Edge device. A newly initialized Pipeline does not contain any Component or wire, only its name and version will be set. The name and version together will define the name of the zip file when the package is saved. class Pipeline ( name : str , version : Optional [ str ] = None , desc : str = '' ) Attributes Name Type Description Default name str Name of the package None version str Version of the package None View Source class Pipeline : \"\"\" `Pipeline` represents a pipeline configuration package with `Components` and wires to provide a data flow on the AI Inference Server. The `Components` have inputs and outputs to transfer data to each other and the wires describe this data flow between them. The package also contains configuration files required to deploy a pipeline on an Industrial Edge device. A newly initialized `Pipeline` does not contain any `Component` or wire, only its name and version will be set. The name and version together will define the name of the zip file when the package is saved. Args: name (str): Name of the package version (str): Version of the package \"\"\" _wire_hash_string = \"{}.{} -> {}.{}\" def __init__ ( self , name : str , version : Optional [ str ] = None , desc : str = \"\" ): \"\"\" A newly initialized `Pipeline` will contain no `Component` or wire, just its name and version will be set. The name and version will define together the name of the zip file when the package is saved. Args: name (str): Name of the package desc (str): Package description (optional) version (str): Version of the package \"\"\" self . name = name self . desc = desc self . version = version self . package_id : Optional [ uuid . UUID ] = None self . save_version = None self . save_package_id : Optional [ uuid . UUID ] = None self . author = 'AI SDK' self . components = {} self . wiring = {} self . parameters = {} self . periodicity = None self . timeshift_reference = [] self . inputs = [] self . outputs = [] self . log_level = logging . INFO self . report_writer = PipelineReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) _python_dependencies_logger . addHandler ( report_writer_handler ) _wheelhouse_logger . addHandler ( report_writer_handler ) def _set_log_level ( self , log_level : int ): self . log_level = log_level _logger . setLevel ( self . log_level ) @ staticmethod def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = \"\" ) -> \"Pipeline\" : \"\"\" Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Args: components (list): List of PythonComponents name (str): Name of the pipeline version (str): Version information of the pipeline. (Optional) Returns: Pipeline: Pipeline object with the auto-wired components \"\"\" pipeline = Pipeline ( name , version , desc = desc ) first_component = components [ 0 ] pipeline . add_component ( first_component ) pipeline . inputs = [( first_component . name , component_input ) for component_input in first_component . inputs ] pipeline . outputs = [( first_component . name , output ) for output in first_component . outputs ] for component in components [ 1 :]: pipeline . add_component ( component ) for ( wire_component , wire_name ) in pipeline . outputs : try : pipeline . add_wiring ( wire_component , wire_name , component . name , wire_name ) except Exception as e : _logger . warning ( f \"Output variable {wire_component}.{wire_name} couldn't be auto-wired. \\n Cause: {e}\" ) unwired_variables = [ f '{component.name}.{x}' for x in component . inputs if not any ( s . endswith ( f '{component.name}.{x}' ) for s in pipeline . wiring )] if len ( unwired_variables ) > 0 : for variable in unwired_variables : _logger . warning ( f \"Input variable {variable} couldn't be auto-wired. \\n \" ) pipeline . outputs = [( component . name , output ) for output in component . outputs ] return pipeline def __repr__ ( self ) -> str : \"\"\" Textual representation of the configured package. The method shows the `Components` with their inputs, outputs and parameters as well as the wiring between these `Components`. Returns: [str]: Textual representation of the package \"\"\" text = f \"[{self.__class__.__name__}] {self.name} ({self.version}) \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . parameters ) > 0 : text += \" \\n Pipeline Parameters: \\n \" for name , parameter in self . parameters . items (): text += f \"- {name} ({parameter['type']}, default: '{parameter['defaultValue']}'){(': ' + parameter['desc']) if parameter.get('desc') is not None else ''} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Pipeline Inputs: \\n \" for component , name in self . inputs : input = self . components [ component ] . inputs [ name ] text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Pipeline Outputs: \\n \" for component , name in self . outputs : output = self . components [ component ] . outputs [ name ] text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" metrics = [( name , metric , component_name ) for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name , metric in component . metrics . items ()] if len ( metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric , _ in metrics : text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . wiring ) > 0 : text += \" \\n I/O Wiring: \\n \" for component , name in self . inputs : text += f \" {name} -> {component}.{name} \\n \" for wire_hash in self . wiring : text += f \" {wire_hash} \\n \" for component , name in self . outputs : text += f \" {component}.{name} -> {name} \\n \" for name , metric , component_name in metrics : text += f \" {component_name}.{name} -> {name} \\n \" if self . periodicity is not None : text += \" \\n Timeshifting: \\n \" text += f \" Periodicity: {self.periodicity} ms \\n \" if len ( self . timeshift_reference ) > 0 : text += \" References: \\n \" for ref in self . timeshift_reference : text += f \" - {ref} \\n \" for component in self . components . values (): text += \" \\n \" + component . __repr__ () return text def add_input ( self , component , variable ): \"\"\" Defines an input variable on the given component as a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" try : _ = self . components [ component ] . inputs [ variable ] except KeyError : raise AssertionError ( \"The component with input variable must exist in the pipeline.\" ) if self . inputs is None : self . inputs = [] if ( component , variable ) in self . inputs : raise AssertionError ( \"The pipeline input already exists.\" ) self . inputs . append (( component , variable )) def delete_input ( self , component : str , variable : str ): \"\"\" Deletes a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . inputs : raise AssertionError ( \"The pipeline input does not exist.\" ) self . inputs . remove (( component , variable )) def add_output ( self , component , variable ): \"\"\" Defines an output variable on the given component as a pipeline output. Args: component (str): Name of the component variable (str): Name of the output variable \"\"\" try : _ = self . components [ component ] . outputs [ variable ] except KeyError : raise AssertionError ( \"The component with output variable must exist in the pipeline.\" ) if self . outputs is None : self . outputs = [] if ( component , variable ) in self . outputs : raise AssertionError ( \"The pipeline output already exists.\" ) self . outputs . append (( component , variable )) def delete_output ( self , component : str , variable : str ): \"\"\" Deletes a pipeline output. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . outputs : raise AssertionError ( \"The pipeline output does not exist.\" ) self . outputs . remove (( component , variable )) def add_component ( self , component : Component ): \"\"\" Adds a `Component` to the pipeline configuration without any connection. The `Component` can be marked as an input or output component of the pipeline. When these parameters are True, the `Component` is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Args: component (Component): `Component` to be added \"\"\" if component . name in self . components : raise AssertionError ( f \"Component with name {component.name} already exists. Please rename the component.\" ) self . components [ component . name ] = component def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ): \"\"\" Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: - The components exist with the given inputs/outputs - The given inputs and outputs are not connected to any wire - The types of the connected input and output are compatible Args: from_component (str): Name of the component which provides data to the `to_component` from_output (str): Name of the output variable of the `from_component` to_component (str): Name of the component which consumes data from the `from_component` to_input (str): Name of the input variable of the `to_component` \"\"\" if from_component not in self . components : raise AssertionError ( f \"No component named '{from_component}'\" ) if to_component not in self . components : raise AssertionError ( f \"No component named '{to_component}'\" ) if from_output not in self . components [ from_component ] . outputs : raise AssertionError ( f \"Component '{from_component}' has no output named '{from_output}'\" ) if to_input not in self . components [ to_component ] . inputs : raise AssertionError ( f \"Component '{to_component}' has no input named '{to_input}'\" ) if self . get_wire_for_input ( to_component , to_input ) is not None : raise AssertionError ( f \"Input '{to_input}' of component '{to_component}' is already wired\" ) _output_type = self . components [ from_component ] . outputs [ from_output ][ \"type\" ] _input_type = self . components [ to_component ] . inputs [ to_input ][ \"type\" ] if _output_type != _input_type : raise AssertionError ( \"Output and input types do not match\" ) wire_hash = self . _wire_hash_string . format ( from_component , from_output , to_component , to_input ) self . wiring [ wire_hash ] = { \"fromComponent\" : from_component , \"fromOutput\" : from_output , \"toComponent\" : to_component , \"toInput\" : to_input , } def get_wire_for_output ( self , component_name : str , output_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data provider through its output with name output_name. Args: component_name (str): Name of the data provider component. output_name (str): Name of the output variable of `component_name`. Returns: [dict]: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"fromComponent\" ] == component_name and x [ \"fromOutput\" ] == output_name ] return wires [ 0 ] if wires else None def get_wire_for_input ( self , component_name : str , input_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data consumer through its input with name `input_name`. Args: component_name (str): Name of the data consumer component. input_name (str): Name of the input variable of `component_name`. Returns: dict: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"toComponent\" ] == component_name and x [ \"toInput\" ] == input_name ] return wires [ 0 ] if wires else None def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ): \"\"\" Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Args: component (str): Name of the component which has the input given the name variable variable (str): Name of the input variable on the component which connected by the wire with_input (bool, optional): If set, the input variable will be also deleted from the component. Defaults to True. Raises: AssertionError: When the variable acts as inter signal alignment reference, it cannot be deleted, and an `AssertionError` will be raised. \"\"\" wire = self . get_wire_for_input ( component , variable ) if wire is None : raise AssertionError ( f \"There is no wiring for input '{variable}' of component '{component}'\" ) if variable in self . timeshift_reference : raise AssertionError ( \"Inter signal alignment reference variables can not be deleted.\" ) wire_hash = self . _wire_hash_string . format ( wire [ 'fromComponent' ], wire [ 'fromOutput' ], wire [ 'toComponent' ], wire [ 'toInput' ]) self . wiring . pop ( wire_hash ) if with_input : self . components [ component ] . delete_input ( variable ) def add_dependencies ( self , packages : list ): \"\"\" @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type `PythonComponent`. This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the `requirements.txt` file when the package is saved. Args: packages (list): List of the necessary python packages to execute the script defined by self.entrypoint \"\"\" python_components = [ self . components [ name ] for name in self . components if type ( self . components [ name ]) is PythonComponent ] for component in python_components : component . add_dependencies ( packages ) def set_timeshifting_periodicity ( self , periodicity : int ): \"\"\" Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, `startingPoint` property is set to `First timestamp`, which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to `Signal reference` by adding inter-signal alignment reference variables via the `add_timeshifting_reference(..)` method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Args: periodicity (int): Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). \"\"\" periodicity = int ( periodicity ) if periodicity not in range ( 10 , int ( math . pow ( 2 , 31 ))): raise AssertionError ( \"Inter signal alignment periodicity must be an integer and in range [10, 2^31)\" ) self . periodicity = periodicity _logger . info ( f \"Inter signal alignment periodicity has been set to {self.periodicity}.\" ) def add_timeshifting_reference ( self , reference : str ): \"\"\" Enables signal alignment mode `Signal reference` by declaring input variables as reference variables. Args: reference (str): Variable name to be added to `self.timeshift_reference` list. \"\"\" if reference not in [ name for _ , name in self . inputs ]: raise AssertionError ( f \"There is no input variable defined with name '{reference}'\" ) if reference in self . timeshift_reference : _logger . warning ( f \"Reference variable with name '{reference}' has been already added.\" ) return self . timeshift_reference . append ( reference ) def remove_timeshifting_reference ( self , reference : str ): \"\"\" Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the `startingPoint` will be `First timestamp`. Args: reference (str): Variable name to be removed from `self.timeshift_reference` list. \"\"\" if reference not in self . timeshift_reference : raise AssertionError ( f \"Reference variable with name {'reference'} does not exist.\" ) self . timeshift_reference . remove ( reference ) def get_pipeline_config ( self ): \"\"\" Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the `destination` folder with name `pipeline_config.yml` \"\"\" if self . save_version is None : self . save_version = self . version if self . save_package_id is None : self . save_package_id = self . package_id pipeline_inputs = [] pipeline_inputs += [{ 'name' : name , 'type' : self . components [ component_name ] . inputs [ name ][ 'type' ] } for component_name , name in self . inputs ] pipeline_outputs = [] pipeline_outputs += [{ 'name' : name , 'type' : self . components [ component_name ] . outputs [ name ][ 'type' ], 'metric' : False , } for component_name , name in self . outputs ] pipeline_outputs += [{ 'name' : name , 'type' : 'String' , 'metric' : True , 'topic' : f \"/siemens/edge/aiinference/{self.name}/{self.save_version}/metrics/{component_name}/{name}\" , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] pipeline_dag = [{ 'source' : f \"{wire['fromComponent']}.{wire['fromOutput']}\" , 'target' : f \"{wire['toComponent']}.{wire['toInput']}\" , } for wire in self . wiring . values ()] pipeline_dag += [{ 'source' : f 'Databus.{name}' , 'target' : f '{component_name}.{name}' , } for component_name , name in self . inputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , name in self . outputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] config_yml_content = { 'fileFormatVersion' : '1.2.0' , 'dataFlowPipelineInfo' : { 'author' : self . author , 'createdOn' : datetime . now (), 'dataFlowPipelineVersion' : self . save_version , 'description' : self . desc if self . desc else 'Created by AI SDK' , 'projectName' : self . name , 'packageId' : str ( self . save_package_id ) }, 'dataFlowPipeline' : { 'components' : [ component . _to_dict () for component in self . components . values ()], 'pipelineDag' : pipeline_dag , 'pipelineInputs' : pipeline_inputs , 'pipelineOutputs' : pipeline_outputs , }, 'packageType' : 'full' } if len ( self . parameters . items ()) != 0 : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] = [] for name , parameter in self . parameters . items (): if parameter [ \"topicBased\" ]: config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ], 'topicBased' : parameter [ 'topicBased' ], 'valueTopic' : parameter [ 'valueTopic' ] }) else : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ] }) return config_yml_content def save_pipeline_config ( self , destination ): \"\"\" Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the `destination` folder with name `pipeline_config.yml` Args: destination (path-like): Path of the `destination` directory. \"\"\" with open ( Path ( destination ) / PIPELINE_CONFIG , \"w\" ) as f : yaml . dump ( self . get_pipeline_config (), f ) def get_datalink_metadata ( self ): \"\"\" The method generates metadata information based on available information. Returns: dict: Dictionary with the necessary information for the AI Inference Server. \"\"\" timeshifting = { \"id\" : None , \"enabled\" : False , \"periodicity\" : self . periodicity , \"startingPoint\" : None , } if self . periodicity is not None : timeshifting [ \"enabled\" ] = True timeshifting [ \"startingPoint\" ] = 'First timestamp' if len ( self . timeshift_reference ) > 0 : timeshifting [ \"startingPoint\" ] = 'Signal reference' exported_metadata = { \"fileFormatVersion\" : \"1.0.0\" , \"id\" : None , \"version\" : None , \"createdOn\" : datetime . now (), \"updatedOn\" : datetime . now (), \"timeShifting\" : timeshifting , \"inputs\" : [ { 'name' : _name , 'mapping' : None , 'timeShiftingReference' : _name in self . timeshift_reference , 'type' : self . components [ _component ] . inputs [ _name ][ 'type' ] } for _component , _name in self . inputs ] } return exported_metadata def save_datalink_metadata ( self , destination ): \"\"\" Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the `destination` folder with the name `datalink_metadata.yml` Args: destination (path-like): Path of the destination directory. \"\"\" with open ( Path ( destination ) / DATALINK_METADATA , \"w\" ) as f : yaml . dump ( self . get_datalink_metadata (), f ) def save_telemetry_data ( self , destination : Path ): \"\"\" Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None \"\"\" telemetry_path = destination / TELEMETRY_YAML telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"simaticai package not found\" ) try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"vep-template-sdk package not found\" ) telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ] . python_version for component in self . components if isinstance ( self . components [ component ], PythonComponent ))) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = list ( set ( f . suffix for f in Path ( destination ) . rglob ( \"*\" ) if f . suffix not in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ])) yaml . dump ( telemetry_data , open ( telemetry_path , 'w' )) def save_readme_html ( self , destination ): \"\"\" Saves a `README.html` in the `destination` folder that describes the pipeline. Args: destination (path-like): Path of the destination folder. \"\"\" pipelinePage = _PipelinePage ( self ) readme_html_path = Path ( destination ) / README_HTML readme_html_path . write_text ( pipelinePage . __str__ ()) def validate ( self , destination = \".\" ): \"\"\" Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: - If the package has at least one component - If all wires create connections between existing components and their variables - If metadata is defined and valid. - If a package with the same name already exists in the `destination` folder. In this case a warning message appears and the `save(..)` method overwrites the existing package. - If the package has multiple components and if they are using the same Python version Args: destination (str, optional): Path of the expected destination folder. Defaults to \".\". \"\"\" if len ( self . components ) < 1 : raise AssertionError ( \"The package must have at least one component.\" ) for name , variable in self . outputs : if self . components [ name ] . batch . outputBatch : raise AssertionError ( f \"The component '{name}' has pipeline output defined with variable name '{variable}'. \\ None of component with pipeline output is allowed to provide batch output . \") for wire_hash in self . wiring . copy (): wire = self . wiring [ wire_hash ] self . _check_wiring ( wire , wire_hash ) pipeline_inputs = [ variable for _ , variable in self . inputs ] pipeline_outputs = [ variable for _ , variable in self . outputs ] if any ( variable in pipeline_outputs for variable in pipeline_inputs ): conflicts = set ( pipeline_inputs ) . intersection ( set ( pipeline_outputs )) raise AssertionError ( f \"Pipeline input and output variables must be unique. Conflicting variables: {conflicts}\" ) self . _check_timeshifting () package_path = Path ( destination ) / f \"{self.name}_{self.version}\" . replace ( \" \" , \"-\" ) if package_path . is_dir (): _logger . warning ( f \"Target folder ({package_path}) already exists! Unless changing the package name the package could be invalid and your files will be overwritten!\" ) python_versions = set () for component in self . components : self . components [ component ] . validate () if isinstance ( self . components [ component ], PythonComponent ): python_versions . add ( self . components [ component ] . python_version ) if ( 1 < len ( python_versions )): _logger . warning ( \"The use of multiple python version in a single pipeline is not recommended. We recommend using only one of the supported versions, which are Python 3.10 or 3.11.\" ) _logger . info ( f \"Package '{self.name}' is valid and ready to save.\" ) def _check_timeshifting ( self ): if len ( self . timeshift_reference ) > 0 and self . periodicity is None : raise AssertionError ( \"When using inter signal alignment reference variables, the periodicity must be set.\" ) def _check_wiring ( self , wire , wire_hash ): error_messages = [] if wire [ 'fromComponent' ] not in self . components : error_messages . append ( f \"From component {wire['fromComponent']} does not exist\" ) if wire [ 'toComponent' ] not in self . components : error_messages . append ( f \"To component {wire['toComponent']} does not exist\" ) if wire [ 'fromOutput' ] not in self . components [ wire [ 'fromComponent' ]] . outputs : error_messages . append ( f \"Output variable {wire['fromOutput']} does not exist on component {wire['fromComponent']}\" ) if wire [ 'toInput' ] not in self . components [ wire [ 'toComponent' ]] . inputs : error_messages . append ( f \"Input variable {wire['toInput']} does not exist on component {wire['toComponent']}\" ) if len ( error_messages ) == 0 : from_type_ = self . components [ wire [ 'fromComponent' ]] . outputs [ wire [ 'fromOutput' ]][ 'type' ] to_type_ = self . components [ wire [ 'toComponent' ]] . inputs [ wire [ 'toInput' ]][ 'type' ] if from_type_ != to_type_ : error_messages . append ( f \"The types of input and output variables does not match for wiring {wire_hash}.\" ) if len ( error_messages ) > 0 : self . wiring . pop ( wire_hash ) error_messages . append ( \"The wire has been deleted, please check the variables and re-create the connection.\" ) raise AssertionError ( error_messages . __str__ ()) def save ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as `{package_name}_{package_version}.zip`. If a file with such a name already exists in the `destination` folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name `{package_name}_{package_version}`. If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: - Package folder with name `{package_name}_{package_version}` - `datalink-metadata.yml` - `pipeline-config.yml` - Component folder with name `{component_name}` When the component is a `PythonComponent`, this folder contains: - `requirements.txt` - Entrypoint script defined by the entrypoint of the component - Extra files as added to the specified folders - Source folder with name `src` with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the `destination` folder, an error is raised. Args: destination (str, optional): Target directory for saving the package. Defaults to \".\". package_id (UUID): The optional package ID. If None, a new UUID is generated. \"\"\" self . validate ( destination ) destination = Path ( destination ) if package_id is not None and not isinstance ( package_id , uuid . UUID ): package_id = uuid . UUID ( package_id ) prev_id = None prev_id_version = None prev_version , prev_with_id = self . _find_previous ( destination , package_id ) if prev_with_id is not None : prev_id_version , prev_id = prev_with_id if package_id == prev_id and ( version or self . version ) == str ( prev_id_version ): raise RuntimeError ( f \"package with version '{version or self.version}' and id '{package_id}' already exists\" ) self . save_version = version if version is not None \\ else self . version if self . version is not None \\ else \"1\" if package_id is not None and prev_id is not None and package_id != prev_id \\ else str ( prev_id_version + 1 ) if prev_id_version is not None \\ else str ( prev_version + 1 ) if prev_version is not None and prev_id is None \\ else \"1\" self . save_package_id = package_id if package_id is not None \\ else prev_id if prev_id is not None \\ else uuid . uuid4 () name = self . name . replace ( \" \" , \"-\" ) package_name = f \"{name}_{self.save_version}\" package_file = destination / f \"{package_name}.zip\" specified_version_is_decimal = version is not None and version . isdecimal () or self . version is not None and self . version . isdecimal () if specified_version_is_decimal : if package_file . exists (): p_id = self . _extract_package_id ( package_file , False ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{package_file}'\" ) edge_package_file = destination / f \"{name}-edge_{self.save_version}.zip\" if edge_package_file . exists (): p_id = self . _extract_package_id ( edge_package_file , True ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{edge_package_file}'\" ) destination = destination / package_name destination . mkdir ( parents = True , exist_ok = True ) # Save for component in self . components : self . components [ component ] . save ( destination , False ) if isinstance ( self . components [ component ], PythonComponent ): self . report_writer . add_direct_dependencies ( self . components [ component ] . name , self . components [ component ] . python_dependencies . dependencies ) self . save_datalink_metadata ( destination ) self . save_pipeline_config ( destination ) self . save_readme_html ( destination ) self . save_telemetry_data ( destination ) zip_destination = shutil . make_archive ( base_name = str ( destination . parent / package_name ), format = 'zip' , root_dir = destination . parent , base_dir = package_name , verbose = True , logger = _logger ) pipeline_size = os . path . getsize ( zip_destination ) # zipped package size in bytes pipeline_size_GB = \"{:.2f}\" . format ( pipeline_size / 1000 / 1000 / 1000 ) pipeline_size_limit_GB = \"{:.2f}\" . format ( PIPELINE_SIZE_LIMIT / 1000 / 1000 / 1000 ) if pipeline_size > PIPELINE_SIZE_LIMIT : error_msg = f \"Pipeline size {pipeline_size} bytes ({pipeline_size_GB} GB) exceeds the limit of \" \\ f \"{PIPELINE_SIZE_LIMIT} bytes ({pipeline_size_limit_GB} GB). \" \\ \"Please remove unnecessary files and dependencies and try again.\" _logger . error ( error_msg ) raise RuntimeError ( error_msg ) return Path ( zip_destination ) # TODO: refactor the business logic in PBI 1662648 def _find_previous ( self , destination : Path , package_id : Optional [ uuid . UUID ] = None ) -> Tuple [ Optional [ int ], Optional [ Tuple [ int , uuid . UUID ]]]: latest_version = None latest_with_id = None if Path ( destination ) . is_dir () is False : return None , None for file in destination . glob ( f \"{self.name.replace(' ', '-')}*.zip\" ): zip_version , zip_package_id = self . _extract_package_info ( file ) if not zip_version . isdecimal (): continue zip_version = int ( zip_version ) if zip_package_id is None : # package id in the zip package not present if latest_version is None or zip_version > latest_version : latest_version = zip_version elif package_id is None : if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) else : if package_id != zip_package_id : continue if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) return latest_version , latest_with_id def _extract_package_info ( self , zip_path : Path ) -> Tuple : with zipfile . ZipFile ( zip_path ) as zip_file : config_path = next ( f for f in zip_file . namelist () if f . endswith ( \"pipeline_config.yml\" )) with zip_file . open ( config_path ) as config_file : config = yaml . load ( config_file , Loader = yaml . SafeLoader ) pipeline_info = config . get ( \"dataFlowPipelineInfo\" , {}) version = pipeline_info . get ( \"dataFlowPipelineVersion\" , None ) package_id = pipeline_info . get ( \"packageId\" , None ) package_id = uuid . UUID ( package_id ) if package_id is not None else None return version , package_id def _extract_package_id ( self , package_file : Path , is_edge_package : bool ) -> Optional [ uuid . UUID ]: try : with OpenZipInTemp ( package_file ) as package_dir : if not is_edge_package : package_dir = package_dir / package_file . stem with open ( package_dir / 'pipeline_config.yml' ) as config_file : return uuid . UUID ( yaml . load ( config_file , Loader = yaml . SafeLoader )[ \"dataFlowPipelineInfo\" ][ \"packageId\" ]) except Exception : _logger . debug ( f \"Could not extract Package ID from '{package_file}'\" ) return None def export ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" Export a runnable pipeline package. Args: destination (str): optional target directory for saving the package. Defaults to \".\". package_id (UUID): optional package ID. If None, a new UUID is generated. version (str): optional version. If None, an automatic version number is generated. \"\"\" config_package = None try : config_package = self . save ( destination , package_id , version ) runtime_package = convert_package ( config_package , self . report_writer ) return runtime_package finally : if config_package is not None : Path ( config_package ) . unlink ( missing_ok = True ) def add_parameter ( self , name , default_value , type_name : str = \"String\" , topic_based : bool = False , desc : str = None ): \"\"\" Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Args: name (str): Name of the parameter desc (str): Description of the parameter (optional) type_name (str, optional): Data type of the parameter. Defaults to \"String\". default_value (str): Default value of the parameter topic_based (bool, optional): If true, the parameter can be updated from a message queue. Raises: ValueError: When: - the default value of the parameter is not of the specified data type (`type_name`) or - the specified data type itself is not an allowed data type (not a part of `parameter_types` dict) or - the specified data type is not given in the right format or - the type of the given `topic_based` parameter is not `bool`. \"\"\" parameter_types = { \"String\" : 'str' , \"Integer\" : 'int' , \"Double\" : 'float' , \"Boolean\" : 'bool' } default_value_type = type ( default_value ) . __name__ if type_name not in parameter_types . keys (): raise ValueError ( f \"The given value type is not supported. Please use one of these: {parameter_types.keys()}\" ) if default_value_type != parameter_types [ type_name ]: raise ValueError ( f \"The given value type does not match the type of '{type_name}'. Please use the correct one from these: {list(parameter_types.keys())}\" ) if not isinstance ( topic_based , bool ): raise ValueError ( \"Type of the given `topic_based` parameter is not `bool`.\" ) self . parameters [ name ] = { \"name\" : name , \"type\" : type_name , \"defaultValue\" : default_value , \"topicBased\" : topic_based , \"valueTopic\" : None } if desc is not None : self . parameters [ name ][ \"desc\" ] = desc Static methods from_components def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = '' ) -> 'Pipeline' Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Parameters: Name Type Description Default components list List of PythonComponents None name str Name of the pipeline None version str Version information of the pipeline. (Optional) None Returns: Type Description Pipeline Pipeline object with the auto-wired components View Source @staticmethod def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = \"\" ) -> \"Pipeline\" : \"\"\" Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Args: components (list): List of PythonComponents name (str): Name of the pipeline version (str): Version information of the pipeline. (Optional) Returns: Pipeline: Pipeline object with the auto-wired components \"\"\" pipeline = Pipeline ( name , version , desc = desc ) first_component = components [ 0 ] pipeline . add_component ( first_component ) pipeline . inputs = [ (first_component.name, component_input) for component_input in first_component.inputs ] pipeline . outputs = [ (first_component.name, output) for output in first_component.outputs ] for component in components [ 1: ] : pipeline . add_component ( component ) for ( wire_component , wire_name ) in pipeline . outputs : try : pipeline . add_wiring ( wire_component , wire_name , component . name , wire_name ) except Exception as e : _logger . warning ( f \"Output variable {wire_component}.{wire_name} couldn't be auto-wired.\\nCause: {e}\" ) unwired_variables = [ f'{component.name}.{x}' for x in component.inputs if not any(s.endswith(f'{component.name}.{x}') for s in pipeline.wiring) ] if len ( unwired_variables ) > 0 : for variable in unwired_variables : _logger . warning ( f \"Input variable {variable} couldn't be auto-wired.\\n\" ) pipeline . outputs = [ (component.name, output) for output in component.outputs ] return pipeline Methods add_component def add_component ( self , component : simaticai . deployment . Component ) Adds a Component to the pipeline configuration without any connection. The Component can be marked as an input or output component of the pipeline. When these parameters are True, the Component is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Parameters: Name Type Description Default component Component Component to be added None View Source def add_component ( self , component : Component ) : \" \"\" Adds a `Component` to the pipeline configuration without any connection. The `Component` can be marked as an input or output component of the pipeline. When these parameters are True, the `Component` is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Args: component (Component): `Component` to be added \"\" \" if component . name in self . components : raise AssertionError ( f \"Component with name {component.name} already exists. Please rename the component.\" ) self . components [ component . name ] = component add_dependencies def add_dependencies ( self , packages : list ) @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type PythonComponent . This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the requirements.txt file when the package is saved. Parameters: Name Type Description Default packages list List of the necessary python packages to execute the script defined by self.entrypoint None View Source def add_dependencies ( self , packages : list ) : \"\"\" @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type `PythonComponent`. This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the `requirements.txt` file when the package is saved. Args: packages (list): List of the necessary python packages to execute the script defined by self.entrypoint \"\"\" python_components = [ self.components[name ] for name in self . components if type ( self . components [ name ] ) is PythonComponent ] for component in python_components : component . add_dependencies ( packages ) add_input def add_input ( self , component , variable ) Defines an input variable on the given component as a pipeline input. Parameters: Name Type Description Default component str Name of the component None variable str Name of the input variable None View Source def add_input ( self , component , variable ) : \"\"\" Defines an input variable on the given component as a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" try : _ = self . components [ component ] . inputs [ variable ] except KeyError : raise AssertionError ( \"The component with input variable must exist in the pipeline.\" ) if self . inputs is None : self . inputs = [] if ( component , variable ) in self . inputs : raise AssertionError ( \"The pipeline input already exists.\" ) self . inputs . append (( component , variable )) add_output def add_output ( self , component , variable ) Defines an output variable on the given component as a pipeline output. Parameters: Name Type Description Default component str Name of the component None variable str Name of the output variable None View Source def add_output ( self , component , variable ) : \"\"\" Defines an output variable on the given component as a pipeline output. Args: component (str): Name of the component variable (str): Name of the output variable \"\"\" try : _ = self . components [ component ] . outputs [ variable ] except KeyError : raise AssertionError ( \"The component with output variable must exist in the pipeline.\" ) if self . outputs is None : self . outputs = [] if ( component , variable ) in self . outputs : raise AssertionError ( \"The pipeline output already exists.\" ) self . outputs . append (( component , variable )) add_parameter def add_parameter ( self , name , default_value , type_name : str = 'String' , topic_based : bool = False , desc : str = None ) Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Parameters: Name Type Description Default name str Name of the parameter None desc str Description of the parameter (optional) None type_name str Data type of the parameter. Defaults to \"String\". \"String\" default_value str Default value of the parameter None topic_based bool If true, the parameter can be updated from a message queue. None Raises: Type Description ValueError When: - the default value of the parameter is not of the specified data type ( type_name ) or - the specified data type itself is not an allowed data type (not a part of parameter_types dict) or - the specified data type is not given in the right format or - the type of the given topic_based parameter is not bool . View Source def add_parameter ( self , name , default_value , type_name : str = \"String\" , topic_based : bool = False , desc : str = None ) : \" \"\" Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Args: name (str): Name of the parameter desc (str): Description of the parameter (optional) type_name (str, optional): Data type of the parameter. Defaults to \" String \". default_value (str): Default value of the parameter topic_based (bool, optional): If true, the parameter can be updated from a message queue. Raises: ValueError: When: - the default value of the parameter is not of the specified data type (`type_name`) or - the specified data type itself is not an allowed data type (not a part of `parameter_types` dict) or - the specified data type is not given in the right format or - the type of the given `topic_based` parameter is not `bool`. \"\" \" parameter_types = { \"String\" : 'str' , \"Integer\" : 'int' , \"Double\" : 'float' , \"Boolean\" : 'bool' } default_value_type = type ( default_value ). __name__ if type_name not in parameter_types . keys () : raise ValueError ( f \"The given value type is not supported. Please use one of these: {parameter_types.keys()}\" ) if default_value_type != parameter_types [ type_name ] : raise ValueError ( f \"The given value type does not match the type of '{type_name}'. Please use the correct one from these: {list(parameter_types.keys())}\" ) if not isinstance ( topic_based , bool ) : raise ValueError ( \"Type of the given `topic_based` parameter is not `bool`.\" ) self . parameters [ name ] = { \"name\" : name , \"type\" : type_name , \"defaultValue\" : default_value , \"topicBased\" : topic_based , \"valueTopic\" : None } if desc is not None : self . parameters [ name ][ \"desc\" ] = desc add_timeshifting_reference def add_timeshifting_reference ( self , reference : str ) Enables signal alignment mode Signal reference by declaring input variables as reference variables. Parameters: Name Type Description Default reference str Variable name to be added to self.timeshift_reference list. None View Source def add_timeshifting_reference ( self , reference : str ): \"\"\" Enables signal alignment mode `Signal reference` by declaring input variables as reference variables. Args: reference (str): Variable name to be added to `self.timeshift_reference` list. \"\"\" if reference not in [ name for _ , name in self . inputs ]: raise AssertionError ( f \"There is no input variable defined with name '{reference}'\" ) if reference in self . timeshift_reference : _logger . warning ( f \"Reference variable with name '{reference}' has been already added.\" ) return self . timeshift_reference . append ( reference ) add_wiring def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ) Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: The components exist with the given inputs/outputs The given inputs and outputs are not connected to any wire The types of the connected input and output are compatible Parameters: Name Type Description Default from_component str Name of the component which provides data to the to_component None from_output str Name of the output variable of the from_component None to_component str Name of the component which consumes data from the from_component None to_input str Name of the input variable of the to_component None View Source def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ): \"\"\" Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: - The components exist with the given inputs/outputs - The given inputs and outputs are not connected to any wire - The types of the connected input and output are compatible Args: from_component (str): Name of the component which provides data to the `to_component` from_output (str): Name of the output variable of the `from_component` to_component (str): Name of the component which consumes data from the `from_component` to_input (str): Name of the input variable of the `to_component` \"\"\" if from_component not in self . components : raise AssertionError ( f \"No component named '{from_component}'\" ) if to_component not in self . components : raise AssertionError ( f \"No component named '{to_component}'\" ) if from_output not in self . components [ from_component ] . outputs : raise AssertionError ( f \"Component '{from_component}' has no output named '{from_output}'\" ) if to_input not in self . components [ to_component ] . inputs : raise AssertionError ( f \"Component '{to_component}' has no input named '{to_input}'\" ) if self . get_wire_for_input ( to_component , to_input ) is not None : raise AssertionError ( f \"Input '{to_input}' of component '{to_component}' is already wired\" ) _output_type = self . components [ from_component ] . outputs [ from_output ][ \"type\" ] _input_type = self . components [ to_component ] . inputs [ to_input ][ \"type\" ] if _output_type != _input_type : raise AssertionError ( \"Output and input types do not match\" ) wire_hash = self . _wire_hash_string . format ( from_component , from_output , to_component , to_input ) self . wiring [ wire_hash ] = { \"fromComponent\" : from_component , \"fromOutput\" : from_output , \"toComponent\" : to_component , \"toInput\" : to_input , } delete_input def delete_input ( self , component : str , variable : str ) Deletes a pipeline input. Parameters: Name Type Description Default component str Name of the component None variable str Name of the input variable None View Source def delete_input ( self , component : str , variable : str ): \"\"\" Deletes a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . inputs : raise AssertionError ( \"The pipeline input does not exist.\" ) self . inputs . remove (( component , variable )) delete_input_wire def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ) Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Parameters: Name Type Description Default component str Name of the component which has the input given the name variable None variable str Name of the input variable on the component which connected by the wire None with_input bool If set, the input variable will be also deleted from the component. Defaults to True. True Raises: Type Description AssertionError When the variable acts as inter signal alignment reference, it cannot be deleted, and an AssertionError will be raised. View Source def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ): \"\"\" Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Args: component (str): Name of the component which has the input given the name variable variable (str): Name of the input variable on the component which connected by the wire with_input (bool, optional): If set, the input variable will be also deleted from the component. Defaults to True. Raises: AssertionError: When the variable acts as inter signal alignment reference, it cannot be deleted, and an `AssertionError` will be raised. \"\"\" wire = self . get_wire_for_input ( component , variable ) if wire is None : raise AssertionError ( f \"There is no wiring for input '{variable}' of component '{component}'\" ) if variable in self . timeshift_reference : raise AssertionError ( \"Inter signal alignment reference variables can not be deleted.\" ) wire_hash = self . _wire_hash_string . format ( wire [ 'fromComponent' ], wire [ 'fromOutput' ], wire [ 'toComponent' ], wire [ 'toInput' ]) self . wiring . pop ( wire_hash ) if with_input : self . components [ component ] . delete_input ( variable ) delete_output def delete_output ( self , component : str , variable : str ) Deletes a pipeline output. Parameters: Name Type Description Default component str Name of the component None variable str Name of the input variable None View Source def delete_output ( self , component : str , variable : str ): \"\"\" Deletes a pipeline output. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . outputs : raise AssertionError ( \"The pipeline output does not exist.\" ) self . outputs . remove (( component , variable )) export def export ( self , destination = '.' , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> pathlib . Path Export a runnable pipeline package. Parameters: Name Type Description Default destination str optional target directory for saving the package. Defaults to \".\". \".\" package_id UUID optional package ID. If None, a new UUID is generated. None version str optional version. If None, an automatic version number is generated. None View Source def export ( self , destination = \".\" , package_id : Optional [ uuid.UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" Export a runnable pipeline package. Args: destination (str): optional target directory for saving the package. Defaults to \" . \". package_id (UUID): optional package ID. If None, a new UUID is generated. version (str): optional version. If None, an automatic version number is generated. \"\"\" config_package = None try : config_package = self . save ( destination , package_id , version ) runtime_package = convert_package ( config_package , self . report_writer ) return runtime_package finally : if config_package is not None : Path ( config_package ). unlink ( missing_ok = True ) get_datalink_metadata def get_datalink_metadata ( self ) The method generates metadata information based on available information. Returns: Type Description dict Dictionary with the necessary information for the AI Inference Server. View Source def get_datalink_metadata ( self ) : \"\"\" The method generates metadata information based on available information. Returns: dict: Dictionary with the necessary information for the AI Inference Server. \"\"\" timeshifting = { \"id\" : None , \"enabled\" : False , \"periodicity\" : self . periodicity , \"startingPoint\" : None , } if self . periodicity is not None : timeshifting [ \"enabled\" ] = True timeshifting [ \"startingPoint\" ] = 'First timestamp' if len ( self . timeshift_reference ) > 0 : timeshifting [ \"startingPoint\" ] = 'Signal reference' exported_metadata = { \"fileFormatVersion\" : \"1.0.0\" , \"id\" : None , \"version\" : None , \"createdOn\" : datetime . now (), \"updatedOn\" : datetime . now (), \"timeShifting\" : timeshifting , \"inputs\" : [ { 'name': _name, 'mapping': None, 'timeShiftingReference': _name in self.timeshift_reference, 'type': self.components[_component ] . inputs [ _name ][ 'type' ] } for _component , _name in self . inputs ] } return exported_metadata get_pipeline_config def get_pipeline_config ( self ) Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the destination folder with name pipeline_config.yml View Source def get_pipeline_config ( self ) : \"\"\" Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the `destination` folder with name `pipeline_config.yml` \"\"\" if self . save_version is None : self . save_version = self . version if self . save_package_id is None : self . save_package_id = self . package_id pipeline_inputs = [] pipeline_inputs += [ { 'name': name, 'type': self.components[component_name ] . inputs [ name ][ 'type' ] } for component_name , name in self . inputs ] pipeline_outputs = [] pipeline_outputs += [ { 'name': name, 'type': self.components[component_name ] . outputs [ name ][ 'type' ] , 'metric' : False , } for component_name , name in self . outputs ] pipeline_outputs += [ { 'name': name, 'type': 'String', 'metric': True, 'topic': f\"/siemens/edge/aiinference/{self.name}/{self.save_version}/metrics/{component_name}/{name}\", } for component_name, component in self.components.items() if isinstance(component, PythonComponent) for name in component.metrics.keys() ] pipeline_dag = [ { 'source': f\"{wire['fromComponent' ] } . { wire [ 'fromOutput' ] } \", 'target': f\" { wire [ 'toComponent' ] } . { wire [ 'toInput' ] } \", } for wire in self.wiring.values()] pipeline_dag += [{ 'source': f'Databus.{name}', 'target': f'{component_name}.{name}', } for component_name, name in self.inputs] pipeline_dag += [{ 'source': f'{component_name}.{name}', 'target': f'Databus.{name}', } for component_name, name in self.outputs] pipeline_dag += [{ 'source': f'{component_name}.{name}', 'target': f'Databus.{name}', } for component_name, component in self.components.items() if isinstance(component, PythonComponent) for name in component.metrics.keys()] config_yml_content = { 'fileFormatVersion': '1.2.0', 'dataFlowPipelineInfo': { 'author': self.author, 'createdOn': datetime.now(), 'dataFlowPipelineVersion': self.save_version, 'description': self.desc if self.desc else 'Created by AI SDK', 'projectName': self.name, 'packageId': str(self.save_package_id) }, 'dataFlowPipeline': { 'components': [component._to_dict() for component in self.components.values()], 'pipelineDag': pipeline_dag, 'pipelineInputs': pipeline_inputs, 'pipelineOutputs': pipeline_outputs, }, 'packageType': 'full' } if len(self.parameters.items()) != 0: config_yml_content[\" dataFlowPipeline \"][\" pipelineParameters \"] = [] for name, parameter in self.parameters.items(): if parameter[\" topicBased \"]: config_yml_content[\" dataFlowPipeline \"][\" pipelineParameters \"].append({ 'name': name, 'type': parameter['type'], 'defaultValue': parameter['defaultValue'], 'topicBased': parameter['topicBased'], 'valueTopic': parameter['valueTopic'] }) else: config_yml_content[\" dataFlowPipeline \"][\" pipelineParameters \"] . append ( { 'name' : name , 'type' : parameter [ 'type' ] , 'defaultValue' : parameter [ 'defaultValue' ] } ) return config_yml_content get_wire_for_input def get_wire_for_input ( self , component_name : str , input_name : str ) Searches for the wire which connects a component with component_name as data consumer through its input with name input_name . Parameters: Name Type Description Default component_name str Name of the data consumer component. None input_name str Name of the input variable of component_name . None Returns: Type Description dict Wire which contains the data provider and receiver with their names and the names of their variables. View Source def get_wire_for_input ( self , component_name : str , input_name : str ) : \" \"\" Searches for the wire which connects a component with `component_name` as data consumer through its input with name `input_name`. Args: component_name (str): Name of the data consumer component. input_name (str): Name of the input variable of `component_name`. Returns: dict: Wire which contains the data provider and receiver with their names and the names of their variables. \"\" \" wires = [ x for x in self . wiring . values () if x [ \"toComponent\" ] == component_name and x [ \"toInput\" ] == input_name ] return wires [ 0 ] if wires else None get_wire_for_output def get_wire_for_output ( self , component_name : str , output_name : str ) Searches for the wire which connects a component with component_name as data provider through its output with name output_name. Parameters: Name Type Description Default component_name str Name of the data provider component. None output_name str Name of the output variable of component_name . None Returns: Type Description [dict] Wire which contains the data provider and receiver with their names and the names of their variables. View Source def get_wire_for_output ( self , component_name : str , output_name : str ) : \" \"\" Searches for the wire which connects a component with `component_name` as data provider through its output with name output_name. Args: component_name (str): Name of the data provider component. output_name (str): Name of the output variable of `component_name`. Returns: [dict]: Wire which contains the data provider and receiver with their names and the names of their variables. \"\" \" wires = [ x for x in self . wiring . values () if x [ \"fromComponent\" ] == component_name and x [ \"fromOutput\" ] == output_name ] return wires [ 0 ] if wires else None remove_timeshifting_reference def remove_timeshifting_reference ( self , reference : str ) Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the startingPoint will be First timestamp . Parameters: Name Type Description Default reference str Variable name to be removed from self.timeshift_reference list. None View Source def remove_timeshifting_reference ( self , reference : str ) : \" \"\" Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the `startingPoint` will be `First timestamp`. Args: reference (str): Variable name to be removed from `self.timeshift_reference` list. \"\" \" if reference not in self . timeshift_reference : raise AssertionError ( f \"Reference variable with name {'reference'} does not exist.\" ) self . timeshift_reference . remove ( reference ) save def save ( self , destination = '.' , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> pathlib . Path @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as {package_name}_{package_version}.zip . If a file with such a name already exists in the destination folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name {package_name}_{package_version} . If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: Package folder with name {package_name}_{package_version} datalink-metadata.yml pipeline-config.yml Component folder with name {component_name} When the component is a PythonComponent , this folder contains: requirements.txt Entrypoint script defined by the entrypoint of the component Extra files as added to the specified folders Source folder with name src with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the destination folder, an error is raised. Parameters: Name Type Description Default destination str Target directory for saving the package. Defaults to \".\". \".\" package_id UUID The optional package ID. If None, a new UUID is generated. None View Source def save ( self , destination = \".\" , package_id : Optional [ uuid.UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as `{package_name}_{package_version}.zip`. If a file with such a name already exists in the `destination` folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name `{package_name}_{package_version}`. If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: - Package folder with name `{package_name}_{package_version}` - `datalink-metadata.yml` - `pipeline-config.yml` - Component folder with name `{component_name}` When the component is a `PythonComponent`, this folder contains: - `requirements.txt` - Entrypoint script defined by the entrypoint of the component - Extra files as added to the specified folders - Source folder with name `src` with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the `destination` folder, an error is raised. Args: destination (str, optional): Target directory for saving the package. Defaults to \" . \". package_id (UUID): The optional package ID. If None, a new UUID is generated. \"\"\" self . validate ( destination ) destination = Path ( destination ) if package_id is not None and not isinstance ( package_id , uuid . UUID ) : package_id = uuid . UUID ( package_id ) prev_id = None prev_id_version = None prev_version , prev_with_id = self . _find_previous ( destination , package_id ) if prev_with_id is not None : prev_id_version , prev_id = prev_with_id if package_id == prev_id and ( version or self . version ) == str ( prev_id_version ) : raise RuntimeError ( f \"package with version '{version or self.version}' and id '{package_id}' already exists\" ) self . save_version = version if version is not None \\ else self . version if self . version is not None \\ else \"1\" if package_id is not None and prev_id is not None and package_id != prev_id \\ else str ( prev_id_version + 1 ) if prev_id_version is not None \\ else str ( prev_version + 1 ) if prev_version is not None and prev_id is None \\ else \"1\" self . save_package_id = package_id if package_id is not None \\ else prev_id if prev_id is not None \\ else uuid . uuid4 () name = self . name . replace ( \" \" , \"-\" ) package_name = f \"{name}_{self.save_version}\" package_file = destination / f \"{package_name}.zip\" specified_version_is_decimal = version is not None and version . isdecimal () or self . version is not None and self . version . isdecimal () if specified_version_is_decimal : if package_file . exists () : p_id = self . _extract_package_id ( package_file , False ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{package_file}'\" ) edge_package_file = destination / f \"{name}-edge_{self.save_version}.zip\" if edge_package_file . exists () : p_id = self . _extract_package_id ( edge_package_file , True ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{edge_package_file}'\" ) destination = destination / package_name destination . mkdir ( parents = True , exist_ok = True ) # Save for component in self . components : self . components [ component ] . save ( destination , False ) if isinstance ( self . components [ component ] , PythonComponent ) : self . report_writer . add_direct_dependencies ( self . components [ component ] . name , self . components [ component ] . python_dependencies . dependencies ) self . save_datalink_metadata ( destination ) self . save_pipeline_config ( destination ) self . save_readme_html ( destination ) self . save_telemetry_data ( destination ) zip_destination = shutil . make_archive ( base_name = str ( destination . parent / package_name ), format = 'zip' , root_dir = destination . parent , base_dir = package_name , verbose = True , logger = _logger ) pipeline_size = os . path . getsize ( zip_destination ) # zipped package size in bytes pipeline_size_GB = \"{:.2f}\" . format ( pipeline_size / 1000 / 1000 / 1000 ) pipeline_size_limit_GB = \"{:.2f}\" . format ( PIPELINE_SIZE_LIMIT / 1000 / 1000 / 1000 ) if pipeline_size > PIPELINE_SIZE_LIMIT : error_msg = f \"Pipeline size {pipeline_size} bytes ({pipeline_size_GB} GB) exceeds the limit of \" \\ f \"{PIPELINE_SIZE_LIMIT} bytes ({pipeline_size_limit_GB} GB). \" \\ \"Please remove unnecessary files and dependencies and try again.\" _logger . error ( error_msg ) raise RuntimeError ( error_msg ) return Path ( zip_destination ) save_datalink_metadata def save_datalink_metadata ( self , destination ) Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the destination folder with the name datalink_metadata.yml Parameters: Name Type Description Default destination path-like Path of the destination directory. None View Source def save_datalink_metadata ( self , destination ) : \" \"\" Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the `destination` folder with the name `datalink_metadata.yml` Args: destination (path-like): Path of the destination directory. \"\" \" with open ( Path ( destination ) / DATALINK_METADATA , \"w\" ) as f : yaml . dump ( self . get_datalink_metadata (), f ) save_pipeline_config def save_pipeline_config ( self , destination ) Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the destination folder with name pipeline_config.yml Parameters: Name Type Description Default destination path-like Path of the destination directory. None View Source def save_pipeline_config ( self , destination ) : \" \"\" Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the `destination` folder with name `pipeline_config.yml` Args: destination (path-like): Path of the `destination` directory. \"\" \" with open ( Path ( destination ) / PIPELINE_CONFIG , \"w\" ) as f : yaml . dump ( self . get_pipeline_config (), f ) save_readme_html def save_readme_html ( self , destination ) Saves a README.html in the destination folder that describes the pipeline. Parameters: Name Type Description Default destination path-like Path of the destination folder. None View Source def save_readme_html ( self , destination ) : \" \"\" Saves a `README.html` in the `destination` folder that describes the pipeline. Args: destination (path-like): Path of the destination folder. \"\" \" pipelinePage = _PipelinePage ( self ) readme_html_path = Path ( destination ) / README_HTML readme_html_path . write_text ( pipelinePage . __str__ ()) save_telemetry_data def save_telemetry_data ( self , destination : pathlib . Path ) Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None View Source def save_telemetry_data ( self , destination : Path ) : \"\"\" Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None \"\"\" telemetry_path = destination / TELEMETRY_YAML telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\", \"get_ipython\" ] ) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ). version except pkg_resources . DistributionNotFound : _logger . debug ( \"simaticai package not found\" ) try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ). version except pkg_resources . DistributionNotFound : _logger . debug ( \"vep-template-sdk package not found\" ) telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ] . python_version for component in self . components if isinstance ( self . components [ component ] , PythonComponent ))) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = list ( set ( f . suffix for f in Path ( destination ). rglob ( \"*\" ) if f . suffix not in [ \"\", \".zip\", \".yml\", \".yaml\", \".html\" ] )) yaml . dump ( telemetry_data , open ( telemetry_path , 'w' )) set_timeshifting_periodicity def set_timeshifting_periodicity ( self , periodicity : int ) Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, startingPoint property is set to First timestamp , which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to Signal reference by adding inter-signal alignment reference variables via the add_timeshifting_reference(..) method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Parameters: Name Type Description Default periodicity int Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). None View Source def set _timeshifting_periodicity ( self , periodicity : int ) : \" \"\" Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, `startingPoint` property is set to `First timestamp`, which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to `Signal reference` by adding inter-signal alignment reference variables via the `add_timeshifting_reference(..)` method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Args: periodicity (int): Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). \"\" \" periodicity = int ( periodicity ) if periodicity not in range ( 10 , int ( math . pow ( 2 , 31 ))) : raise AssertionError ( \"Inter signal alignment periodicity must be an integer and in range [10, 2^31)\" ) self . periodicity = periodicity _logger . info ( f \"Inter signal alignment periodicity has been set to {self.periodicity}.\" ) validate def validate ( self , destination = '.' ) Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: If the package has at least one component If all wires create connections between existing components and their variables If metadata is defined and valid. If a package with the same name already exists in the destination folder. In this case a warning message appears and the save(..) method overwrites the existing package. If the package has multiple components and if they are using the same Python version Parameters: Name Type Description Default destination str Path of the expected destination folder. Defaults to \".\". \".\" View Source def validate ( self , destination = \".\" ) : \"\"\" Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: - If the package has at least one component - If all wires create connections between existing components and their variables - If metadata is defined and valid. - If a package with the same name already exists in the `destination` folder. In this case a warning message appears and the `save(..)` method overwrites the existing package. - If the package has multiple components and if they are using the same Python version Args: destination (str, optional): Path of the expected destination folder. Defaults to \" . \". \"\"\" if len ( self . components ) < 1 : raise AssertionError ( \"The package must have at least one component.\" ) for name , variable in self . outputs : if self . components [ name ] . batch . outputBatch : raise AssertionError ( f \"The component '{name}' has pipeline output defined with variable name '{variable}'. \\ None of component with pipeline output is allowed to provide batch output.\" ) for wire_hash in self . wiring . copy () : wire = self . wiring [ wire_hash ] self . _check_wiring ( wire , wire_hash ) pipeline_inputs = [ variable for _, variable in self.inputs ] pipeline_outputs = [ variable for _, variable in self.outputs ] if any ( variable in pipeline_outputs for variable in pipeline_inputs ) : conflicts = set ( pipeline_inputs ). intersection ( set ( pipeline_outputs )) raise AssertionError ( f \"Pipeline input and output variables must be unique. Conflicting variables: {conflicts}\" ) self . _check_timeshifting () package_path = Path ( destination ) / f \"{self.name}_{self.version}\" . replace ( \" \" , \"-\" ) if package_path . is_dir () : _logger . warning ( f \"Target folder ({package_path}) already exists! Unless changing the package name the package could be invalid and your files will be overwritten!\" ) python_versions = set () for component in self . components : self . components [ component ] . validate () if isinstance ( self . components [ component ] , PythonComponent ) : python_versions . add ( self . components [ component ] . python_version ) if ( 1 < len ( python_versions )) : _logger . warning ( \"The use of multiple python version in a single pipeline is not recommended. We recommend using only one of the supported versions, which are Python 3.10 or 3.11.\" ) _logger . info ( f \"Package '{self.name}' is valid and ready to save.\" ) PythonComponent A pipeline component implemented using Python scripts and libraries. A PythonComponent wraps Python code resource files such as saved models into a structured folder, which can be added to a pipeline configuration package. For a comprehensive overview on how to wrap ML models into Python components, we recommend you refer to the AI SDK User Manual, especially the guideline for writing pipeline components. We also recommend you study the example Python components in the E2E Tutorials for the AI SDK. A new PythonComponent is empty. class PythonComponent ( name = 'inference' , version = '0.0.1' , python_version = '3.10' , desc : str = '' ) Attributes Name Type Description Default name str Component name. (default: inference) None desc str Component description (optional) None version str Component version. (default: 0.0.1) None python_version str Python version on the target AI Inference Server. At the moment of writing, the current version supports Python 3.10 and 3.11. None View Source class PythonComponent ( Component ): \"\"\" A pipeline component implemented using Python scripts and libraries. A `PythonComponent` wraps Python code resource files such as saved models into a structured folder, which can be added to a pipeline configuration package. For a comprehensive overview on how to wrap ML models into Python components, we recommend you refer to the AI SDK User Manual, especially the guideline for writing pipeline components. We also recommend you study the example Python components in the E2E Tutorials for the AI SDK. A new `PythonComponent` is empty. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, the current version supports Python 3.10 and 3.11. \"\"\" def __init__ ( self , name = \"inference\" , version = \"0.0.1\" , python_version = '3.10' , desc : str = \"\" ): \"\"\" Creates a new, empty Python component. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, AI Inference Server supports Python 3.10 and 3.11. \"\"\" super () . __init__ ( name = name , desc = desc ) try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) self . python_version = python_version self . version = version self . metrics = {} self . entrypoint : Optional [ Path ] = None self . resources = {} self . python_dependencies = PythonDependencies ( python_version ) self . _replicas = 1 self . is_valid = False def __repr__ ( self ) -> str : text = super () . __repr__ () if len ( self . metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric in self . metrics . items (): text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . resources ): text += \" \\n Resources: \\n \" for path , base in self . resources . items (): text += f \" {base}/{path.name} \\n \" . replace ( './' , '' ) if self . entrypoint is not None : text += f \"Entrypoint: {self.entrypoint} \\n \" return text def set_entrypoint ( self , entrypoint : str ): \"\"\" Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. ```python import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys.path.insert(0, str(Path('./src').resolve())) from my_module import processor # then the processor module can be imported def run(data: str): input_data = json.loads(data) # incoming JSON string is loaded as a dictionary result = processor.process_data(input_data) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None: answer = {\"ready\": False, \"output\": None} else: answer = {\"ready\": True, \"output\": json.dumps(result)} return answer ``` Args: entrypoint (str): Name of the new entrypoint script to be copied \"\"\" self . is_valid = False if not any ( key . name for key , value in self . resources . items () if key . name == entrypoint and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) self . entrypoint = Path ( entrypoint ) def add_resources ( self , base_dir : os . PathLike , resources : Union [ os . PathLike , list ]): \"\"\" Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in '__pycache__' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Args: base_dir (path-like): Root folder of your code from which the resources are referred resources (os.PathLike or List): A single path or list of relative paths to resource files \"\"\" self . is_valid = False base_dir = Path ( base_dir ) . resolve () . absolute () if not base_dir . is_dir (): raise AssertionError ( f \"Parameter 'base_dir' must be a directory and available in path {base_dir}.\" ) resources = resources if type ( resources ) is list else [ resources ] for resource in resources : self . _add_resource ( base_dir , resource ) def _add_resource ( self , base_dir : Path , resource : os . PathLike ): self . is_valid = False if Path ( resource ) . is_absolute () or '..' in resource : raise AssertionError ( \"The resource path must be relative and cannot contain '/../' elements.\" ) resource_path = base_dir / resource if resource_path . is_file (): self . _add_resource_file ( base_dir , resource_path ) return if resource_path . is_dir (): for glob_path in resource_path . rglob ( \"*\" ): if glob_path . is_file (): self . _add_resource_file ( base_dir , glob_path ) return raise AssertionError ( f \"Specified resource is not a file or directory: '{resource}'\" ) def _add_resource_file ( self , base_dir : Path , resource_path : Path ): self . is_valid = False for parent in resource_path . parents : if parent . name == '__pycache__' : return if resource_path in self . resources . keys (): _logger . warning ( f \"Resource '{resource_path}' is already added to target directory '{self.resources[resource_path]}'\" ) return self . resources [ resource_path ] = f \"{resource_path.parent.relative_to(base_dir)}\" def add_dependencies ( self , packages : list ): \"\"\" Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Args: packages (list): Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution \"\"\" self . is_valid = False self . python_dependencies . add_dependencies ( packages ) def set_requirements ( self , requirements_path : os . PathLike ): \"\"\" Reads the defined dependencies from the given `requirements.txt` file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of `--extra-index-url=my.repo.example.com`. Args: requirements_path (str): Path of the given `requirements.txt` file \"\"\" self . is_valid = False self . python_dependencies . set_requirements ( requirements_path ) def add_python_packages ( self , path : str ) -> None : \"\"\" Adds Python package(s) to the `PythonPackages.zip` file of the component. The `path` parameter can refer to either a `whl`, a `zip` or a `tar.gz` file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the `tempfile.tempdir` folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Args: path (str): Path of the distribution file Examples: `component.add_python_packages('../resources/my_package-0.0.1-py3-none-any.wheel')` adds the wheel file to `PythonPackages.zip` and adds dictionary item `component.dependencies['my_package'] = '0.0.1'` `component.add_python_packages('../resources/inference-wheels.zip')` adds all the wheel files in the zip to `PythonPackages.zip` and `component.dependencies` \"\"\" self . is_valid = False self . python_dependencies . add_python_packages ( path ) def set_parallel_steps ( self , replicas ): \"\"\" Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Args: replicas (int): Number of parallel executors. Default is 1. Raises: ValueError: if the given argument is not a positive integer. \"\"\" self . is_valid = False if ( not isinstance ( replicas , int )) or replicas < 1 : raise ValueError ( \"Replica count must be a positive integer.\" ) if 8 < replicas : _logger . warning ( \"The current maximum of parallel executors is 8.\" ) self . _replicas = replicas def add_metric ( self , name : str , desc : Optional [ str ] = None ): \"\"\" Adds a metric that will be automatically used as a pipeline output. Args: name (str): Name of the metric. desc (str): Description of the metric. (optional) \"\"\" if \"_\" not in name : raise AssertionError ( \"The metric name must contain at least one underscore\" ) if self . metrics is None : self . metrics = {} if name in self . metrics : raise AssertionError ( f \"Metric '{name}' already exists\" ) self . metrics [ name ] = {} if desc is not None : self . metrics [ name ][ 'desc' ] = desc def delete_metric ( self , name : str ): \"\"\" Remove a previously added metric. Args: name (str): Name of the metric to be deleted. \"\"\" if name not in self . metrics : raise AssertionError ( f \"Component '{self.name}' has no metric '{name}'\" ) self . metrics . pop ( name ) def _to_dict ( self ): component_dict = { ** super () . _to_dict (), 'version' : self . version , 'entrypoint' : f \"./{self.entrypoint.name}\" , 'hwType' : 'CPU' , 'runtime' : { 'type' : 'python' , 'version' : self . python_version }, 'replicas' : self . _replicas } component_dict [ \"outputType\" ] += [{ 'name' : name , 'type' : 'String' , 'metric' : True , } for name in self . metrics . keys ()] return component_dict def enable_dependency_optimization ( self ): \"\"\" Allows changing repository URLs to optimize the package size Allows the replacement of the `--index-url` argument during `pip download` to download CPU runtime optimized dependencies only. Enabling this optimization, the present `--index-url` will be prepended to the `--extra-index-url` list, and the Pytorch CPU only repository will be set as the `--index-url`. A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . enable_dependency_optimization () def disable_dependency_optimization ( self ): \"\"\" Disables any modification to repository URLs Disables the replacement of the `--index-url` argument during `pip download`. This way all `--index-url` or `--extra-index-url` arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . disable_dependency_optimization () def validate ( self ): \"\"\" Validates that the component is ready to be serialized and packaged as part of a pipeline. \"\"\" if not self . is_valid : if self . entrypoint is None : raise AssertionError ( \"Entrypoint must be defined\" ) if not any ( key . name for key , value in self . resources . items () if key . name == self . entrypoint . name and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) if len ( self . python_dependencies . dependencies ) < 1 : _logger . warning ( f \"WARNING! There are no dependencies defined for component '{self.name}'. Please make sure that all necessary dependencies have been added.\" ) self . python_dependencies . validate () self . is_valid = True _logger . info ( f \"Component '{self.name}' is valid and ready to use.\" ) def save ( self , destination , validate = True ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter `validate` to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: - `requirements.txt` with a list of Python dependencies - Entry point script defined by the `entrypoint` attribute of the component - Extra files as added to the specified folders - `PythonPackages.zip` with the wheel binaries for the environment to be installed Args: destination (path-like): Target directory to which the component will be saved. validate (bool): With value True, triggers component validation. Defaults to True. \"\"\" if validate : self . validate () folder_path = Path ( destination ) / self . name folder_path . mkdir ( parents = True , exist_ok = True ) for file_path in self . resources : dir_path = folder_path / self . resources [ file_path ] os . makedirs ( dir_path , exist_ok = True ) shutil . copy ( file_path , dir_path / file_path . name ) self . python_dependencies . save ( folder_path ) Ancestors (in MRO) simaticai.deployment.Component Class variables BatchInfo reserved_names Methods add_dependencies def add_dependencies ( self , packages : list ) Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Parameters: Name Type Description Default packages list Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution None View Source def add_dependencies ( self , packages : list ): \"\"\" Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Args: packages (list): Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution \"\"\" self . is_valid = False self . python_dependencies . add_dependencies ( packages ) add_input def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector payload = { \"image\" : { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height , \"mimeType\" : [ \"image/raw\" ], \"dataType\" : \"uint8\" , \"channelsPerPixel\" : 3 , \"image\" : _swap_bytes ( image . tobytes ()) } } Between components the format is the same format as the format of Object as an output. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new input. None _type str Type of the new input. None desc str Description of the input. (optional) None View Source def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc add_metric def add_metric ( self , name : str , desc : Optional [ str ] = None ) Adds a metric that will be automatically used as a pipeline output. Parameters: Name Type Description Default name str Name of the metric. None desc str Description of the metric. (optional) None View Source def add_metric ( self , name : str , desc : Optional [ str ] = None ) : \"\"\" Adds a metric that will be automatically used as a pipeline output. Args: name (str): Name of the metric. desc (str): Description of the metric. (optional) \"\"\" if \"_\" not in name : raise AssertionError ( \"The metric name must contain at least one underscore\" ) if self . metrics is None : self . metrics = {} if name in self . metrics : raise AssertionError ( f \"Metric '{name}' already exists\" ) self . metrics [ name ] = {} if desc is not None : self . metrics [ name ][ 'desc' ] = desc add_output def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type Object the entrypoint must return with a dictionary containing two fields, where one field has type str and the other field has type bytes . The example below shows the required format, assuming that 'image' is a PIL Image. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new output. None _type str Type of the new output. None desc str Description of the output. (optional) None View Source def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc add_python_packages def add_python_packages ( self , path : str ) -> None Adds Python package(s) to the PythonPackages.zip file of the component. The path parameter can refer to either a whl , a zip or a tar.gz file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the tempfile.tempdir folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Parameters: Name Type Description Default path str Path of the distribution file None View Source def add_python_packages ( self , path : str ) -> None : \" \"\" Adds Python package(s) to the `PythonPackages.zip` file of the component. The `path` parameter can refer to either a `whl`, a `zip` or a `tar.gz` file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the `tempfile.tempdir` folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Args: path (str): Path of the distribution file Examples: `component.add_python_packages('../resources/my_package-0.0.1-py3-none-any.wheel')` adds the wheel file to `PythonPackages.zip` and adds dictionary item `component.dependencies['my_package'] = '0.0.1'` `component.add_python_packages('../resources/inference-wheels.zip')` adds all the wheel files in the zip to `PythonPackages.zip` and `component.dependencies` \"\" \" self . is_valid = False self . python_dependencies . add_python_packages ( path ) add_resources def add_resources ( self , base_dir : os . PathLike , resources : Union [ os . PathLike , list ] ) Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in ' pycache ' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Parameters: Name Type Description Default base_dir path-like Root folder of your code from which the resources are referred None resources os.PathLike or List A single path or list of relative paths to resource files None View Source def add_resources ( self , base_dir : os . PathLike , resources : Union [ os.PathLike, list ] ) : \"\"\" Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in '__pycache__' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Args: base_dir (path-like): Root folder of your code from which the resources are referred resources (os.PathLike or List): A single path or list of relative paths to resource files \"\"\" self . is_valid = False base_dir = Path ( base_dir ). resolve (). absolute () if not base_dir . is_dir () : raise AssertionError ( f \"Parameter 'base_dir' must be a directory and available in path {base_dir}.\" ) resources = resources if type ( resources ) is list else [ resources ] for resource in resources : self . _add_resource ( base_dir , resource ) change_input def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the inputs of the component. Parameters: Name Type Description Default name str Name of the input to be changed. None _type str New type of the input. None desc str Description of the input. (optional) None View Source def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc change_output def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the outputs of the component. Parameters: Name Type Description Default name str Name of the output to be changed. None _type str The new type of the output. None desc str Description of the output. (optional) None View Source def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc delete_input def delete_input ( self , name : str ) Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use package.delete_input_wire(...) with default parameter with_input=True . Parameters: Name Type Description Default name str Name of the input to be deleted. None View Source def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name ) delete_metric def delete_metric ( self , name : str ) Remove a previously added metric. Parameters: Name Type Description Default name str Name of the metric to be deleted. None View Source def delete_metric(self, name: str): \"\"\" Remove a previously added metric. Args: name (str): Name of the metric to be deleted. \"\"\" if name not in self.metrics: raise AssertionError(f\"Component '{self.name}' has no metric '{name}'\") self.metrics.pop(name) delete_output def delete_output ( self , name : str ) Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Parameters: Name Type Description Default name str Name of the output to be deleted. None View Source def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name ) disable_dependency_optimization def disable_dependency_optimization ( self ) Disables any modification to repository URLs Disables the replacement of the --index-url argument during pip download . This way all --index-url or --extra-index-url arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a PythonComponent can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an ONNX model and use it within a GPURuntimeComponent . View Source def disable_dependency_optimization ( self ) : \" \"\" Disables any modification to repository URLs Disables the replacement of the `--index-url` argument during `pip download`. This way all `--index-url` or `--extra-index-url` arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\" \" self . python_dependencies . disable_dependency_optimization () enable_dependency_optimization def enable_dependency_optimization ( self ) Allows changing repository URLs to optimize the package size Allows the replacement of the --index-url argument during pip download to download CPU runtime optimized dependencies only. Enabling this optimization, the present --index-url will be prepended to the --extra-index-url list, and the Pytorch CPU only repository will be set as the --index-url . A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a PythonComponent can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an ONNX model and use it within a GPURuntimeComponent . View Source def enable_dependency_optimization ( self ) : \" \"\" Allows changing repository URLs to optimize the package size Allows the replacement of the `--index-url` argument during `pip download` to download CPU runtime optimized dependencies only. Enabling this optimization, the present `--index-url` will be prepended to the `--extra-index-url` list, and the Pytorch CPU only repository will be set as the `--index-url`. A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\" \" self . python_dependencies . enable_dependency_optimization () save def save ( self , destination , validate = True ) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter validate to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: requirements.txt with a list of Python dependencies Entry point script defined by the entrypoint attribute of the component Extra files as added to the specified folders PythonPackages.zip with the wheel binaries for the environment to be installed Parameters: Name Type Description Default destination path-like Target directory to which the component will be saved. None validate bool With value True, triggers component validation. Defaults to True. True View Source def save ( self , destination , validate = True ) : \" \"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter `validate` to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: - `requirements.txt` with a list of Python dependencies - Entry point script defined by the `entrypoint` attribute of the component - Extra files as added to the specified folders - `PythonPackages.zip` with the wheel binaries for the environment to be installed Args: destination (path-like): Target directory to which the component will be saved. validate (bool): With value True, triggers component validation. Defaults to True. \"\" \" if validate : self . validate () folder_path = Path ( destination ) / self . name folder_path . mkdir ( parents = True , exist_ok = True ) for file_path in self . resources : dir_path = folder_path / self . resources [ file_path ] os . makedirs ( dir_path , exist_ok = True ) shutil . copy ( file_path , dir_path / file_path . name ) self . python_dependencies . save ( folder_path ) set_entrypoint def set_entrypoint ( self , entrypoint : str ) Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys . path . insert ( 0 , str ( Path ( './src' ) . resolve ())) from my_module import processor # then the processor module can be imported def run ( data : str ): input_data = json . loads ( data ) # incoming JSON string is loaded as a dictionary result = processor . process_data ( input_data ) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None : answer = { \"ready\" : False , \"output\" : None } else : answer = { \"ready\" : True , \"output\" : json . dumps ( result )} return answer Parameters: Name Type Description Default entrypoint str Name of the new entrypoint script to be copied None View Source def set_entrypoint ( self , entrypoint : str ): \"\"\" Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. ```python import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys.path.insert(0, str(Path('./src').resolve())) from my_module import processor # then the processor module can be imported def run(data: str): input_data = json.loads(data) # incoming JSON string is loaded as a dictionary result = processor.process_data(input_data) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None: answer = {\"ready\": False, \"output\": None} else: answer = {\"ready\": True, \"output\": json.dumps(result)} return answer ``` Args: entrypoint (str): Name of the new entrypoint script to be copied \"\"\" self . is_valid = False if not any ( key . name for key , value in self . resources . items () if key . name == entrypoint and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) self . entrypoint = Path ( entrypoint ) set_parallel_steps def set_parallel_steps ( self , replicas ) Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Parameters: Name Type Description Default replicas int Number of parallel executors. Default is 1. None Raises: Type Description ValueError if the given argument is not a positive integer. View Source def set_parallel_steps ( self , replicas ): \"\"\" Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Args: replicas (int): Number of parallel executors. Default is 1. Raises: ValueError: if the given argument is not a positive integer. \"\"\" self . is_valid = False if ( not isinstance ( replicas , int )) or replicas < 1 : raise ValueError ( \"Replica count must be a positive integer.\" ) if 8 < replicas : _logger . warning ( \"The current maximum of parallel executors is 8.\" ) self . _replicas = replicas set_requirements def set_requirements ( self , requirements_path : os . PathLike ) Reads the defined dependencies from the given requirements.txt file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of --extra-index-url=my.repo.example.com . Parameters: Name Type Description Default requirements_path str Path of the given requirements.txt file None View Source def set_requirements(self, requirements_path: os.PathLike): \"\"\" Reads the defined dependencies from the given `requirements.txt` file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of `--extra-index-url=my.repo.example.com` . Args: requirements_path (str): Path of the given `requirements.txt` file \"\"\" self.is_valid = False self.python_dependencies.set_requirements(requirements_path) validate def validate ( self ) Validates that the component is ready to be serialized and packaged as part of a pipeline. View Source def validate ( self ): \"\"\" Validates that the component is ready to be serialized and packaged as part of a pipeline. \"\"\" if not self . is_valid : if self . entrypoint is None : raise AssertionError ( \"Entrypoint must be defined\" ) if not any ( key . name for key , value in self . resources . items () if key . name == self . entrypoint . name and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) if len ( self . python_dependencies . dependencies ) < 1 : _logger . warning ( f \"WARNING! There are no dependencies defined for component '{self.name}'. Please make sure that all necessary dependencies have been added.\" ) self . python_dependencies . validate () self . is_valid = True _logger . info ( f \"Component '{self.name}' is valid and ready to use.\" )","title":"Deployment"},{"location":"reference/simaticai/deployment.html#module-simaticaideployment","text":"Packaging ML models for deployment on the AI Inference Server. The AI SDK offers the functionality to create a pipeline configuration package and wrap trained models, which can be converted to an edge configuration package and then uploaded and run on an AI Inference Server on an Industrial Edge device. From a deployment perspective, the inference pipeline can consist of one or more components. This is independent of the logical structure of the inference pipeline. For example, a typical time series pipeline that consists of multiple Scikit Learn pipeline elements can be packaged into a single pipeline component, which includes both a feature extractor and a classifier. Alternatively, you can deploy the same pipeline split into two components, one for the feature extractor and another for the classifier. To keep things simple and less error-prone, a pipeline should have as few components as possible. In many cases, a single component will be sufficient. However, there might be reasons why you might consider using separate components, such as: You need a different Python environment for different parts of your processing, e.g., you have components requiring conflicting package versions. You want to exploit parallelism between components without implementing multithreading. You want to modularize and build your pipeline from a pool of component variants, which you can combine flexibly. The AI SDK allows you to create pipeline components implemented in Python and compose linear pipelines of one or multiple of such components. The API is designed to anticipate future possible types of components that might be based on a different technology than Python, e.g. ONNX or native TensorFlow Serving. Currently, only Python is supported. For a comprehensive overview on how to package ML models in the context of a machine learning workflow, we recommend you refer to the AI SDK User Manual, especially the chapter concerning packaging models into an inference pipeline. We also recommend you follow the project templates for the AI SDK, which provide packaging notebooks as examples, and where source code and saved trained models are organized into a given folder structure. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Packaging ML models for deployment on the AI Inference Server. The AI SDK offers the functionality to create a pipeline configuration package and wrap trained models, which can be converted to an edge configuration package and then uploaded and run on an AI Inference Server on an Industrial Edge device. From a deployment perspective, the inference pipeline can consist of one or more components. This is independent of the logical structure of the inference pipeline. For example, a typical time series pipeline that consists of multiple Scikit Learn pipeline elements can be packaged into a single pipeline component, which includes both a feature extractor and a classifier. Alternatively, you can deploy the same pipeline split into two components, one for the feature extractor and another for the classifier. To keep things simple and less error-prone, a pipeline should have as few components as possible. In many cases, a single component will be sufficient. However, there might be reasons why you might consider using separate components, such as: - You need a different Python environment for different parts of your processing, e.g., you have components requiring conflicting package versions. - You want to exploit parallelism between components without implementing multithreading. - You want to modularize and build your pipeline from a pool of component variants, which you can combine flexibly. The AI SDK allows you to create pipeline components implemented in Python and compose linear pipelines of one or multiple of such components. The API is designed to anticipate future possible types of components that might be based on a different technology than Python, e.g. ONNX or native TensorFlow Serving. Currently, only Python is supported. For a comprehensive overview on how to package ML models in the context of a machine learning workflow, we recommend you refer to the AI SDK User Manual, especially the chapter concerning packaging models into an inference pipeline. We also recommend you follow the project templates for the AI SDK, which provide packaging notebooks as examples, and where source code and saved trained models are organized into a given folder structure. \"\"\" from dataclasses import dataclass import json import logging import math import os import uuid import platform import re import shutil import subprocess import sys import tempfile import zipfile from datetime import datetime from importlib import resources as module_resources from pathlib import Path , PurePath from typing import Optional , Tuple , Union import jsonschema import jsonschema . exceptions import pkg_resources import yaml from MarkupPy import markup from google . protobuf import text_format from simaticai import model_config_pb2 from simaticai . helpers import pep508 , tempfiles , yaml_helper , model_config , calc_sha from simaticai . helpers . tempfiles import OpenZipInTemp from simaticai . packaging . constants import ( PIPELINE_CONFIG , RUNTIME_CONFIG , DATALINK_METADATA , # pipeline configuration files TELEMETRY_YAML , README_HTML , # additional pipeline information files REQUIREMENTS_TXT , PYTHON_PACKAGES_ZIP , # component dependency configuration PYTHON_PACKAGES , supported_types , MSG_NOT_FOUND , # additional constants PIPELINE_SIZE_LIMIT ) from simaticai . packaging . python_dependencies import PythonDependencies from simaticai . packaging . wheelhouse import create_wheelhouse from simaticai . helpers . reporter import PipelineReportWriter , ReportWriterHandler from simaticai . packaging . python_dependencies import _logger as _python_dependencies_logger from simaticai . packaging . wheelhouse import _logger as _wheelhouse_logger logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _version_matcher = re . compile ( 'Version: ([^ ]+).*' ) _transitive_matcher = re . compile ( 'Requires: (.+)' ) def find_dependencies ( name : str , dependencies : dict ): \"\"\" @Deprecated, reason: uses 'pip show' which only works for installed packages on the current platform. Collects all dependencies of the Python module given with its `name` in the current Python environment. All inherited dependencies will be added to the `dependencies` dictionary with the installed version of the module. The method executes an OS command like `python -m pip show scikit-learn`. Args: name (str): Name of the Python module to be searched through for its dependencies. dependencies (dict): Dictionary to collect the dependencies with the module name as key, and the installed version as value. Returns: dict: The `dependencies` dictionary with the collected module names and versions. \"\"\" cmd_line = [ sys . executable , '-m' , 'pip' , 'show' , name ] result = subprocess . run ( cmd_line , stdout = subprocess . PIPE , text = True ) if result . returncode != 0 : print ( f \"Dependency {name} is not found and cannot be added.\" ) return dependencies version = None for line in result . stdout . splitlines (): version_matches = _version_matcher . match ( line ) if version_matches : version = version_matches . groups ()[ 0 ] . strip () transitive_matches = _transitive_matcher . match ( line ) if transitive_matches : transitives = transitive_matches . groups ()[ 0 ] . split ( \", \" ) for dependency in transitives : if dependency not in dependencies : find_dependencies ( dependency , dependencies ) if name not in dependencies : spec = pep508 . Spec ( name , [], [( '==' , version )] if version else [], None ) dependencies [ name ] = spec print ( \"Found:\" , spec ) return dependencies def python_version_validator ( version : str ): \"\"\" Checks if Python version string is valid and describes supported version. Only version 3.10 and 3.11 is supported. A patch version is optional and accepted but logs a warning. Accepted syntaxes are: - {major}.{minor} - {major}.{minor}.{patch} Args: version (str): Python version string Raises: ValueError: if the provided version is not supported \"\"\" supported_versions = [ \"3.10\" , \"3.11\" ] error_message = \"The defined python version is not supported. Currently supported Python versions are 3.10 and 3.11. Python version must be specified only with major and minor version, e.g. '3.10'.\" warning_message = \"\"\"Required Python version was specified with patch version. Please note that the patch digit of the required Python version is often not taken into account by the Python ecosystem, so there is no guarantee it has the desired effect.\"\"\" python_version_matcher = re . match ( r '^(3)\\.(0|[1-9][0-9]*)\\.?(0|[1-9][0-9]*)?$' , str ( version )) major_minor_version = \"0.0\" has_patch_version = False if python_version_matcher is not None : major_minor_version = f \"{python_version_matcher.group(1)}.{python_version_matcher.group(2)}\" has_patch_version = python_version_matcher . group ( 3 ) is not None if major_minor_version not in supported_versions : raise ValueError ( error_message ) if has_patch_version : _logger . warning ( warning_message ) class Component : \"\"\" Base class for pipeline components, with name, description, and a list of inputs and outputs. A new component is created with the given name and an empty input and output list. Args: name (str): Name of the component desc (str): Optional description of the component inputs (dict): Dictionary of (name, type) pairs, which describe the input variables outputs (dict): Dictionary of (name, type) pairs, which describe the output variables \"\"\" reserved_names = [ \"timestamp\" ] @ dataclass class BatchInfo : \"\"\" Batch information for the component. This attribute specifies whether the component can handle batch input or output data. When set to True, the component will receive data in the form of a list of dictionaries instead of a single dictionary. It is important to note that the input and output variables on the component should still be defined as if they are single variables. If the input of the pipeline is configured for batch processing, it is recommended not to configure timeshifting, as the list will have the same timestamp for all elements, potentially resulting in data loss. \"\"\" inputBatch : bool = False outputBatch : bool = False def dict ( self ): return { 'inputBatch' : 'Yes' if self . inputBatch is True else 'No' , 'outputBatch' : 'Yes' if self . outputBatch is True else 'No' } def __init__ ( self , name : str , desc : str = \"\" ): \"\"\" Creates a new component with the given name and an empty input and output list. Args: name (str): Name of the component. desc (str): Optional description of the component \"\"\" self . name = name self . desc = desc self . inputs = {} self . outputs = {} self . batch = self . BatchInfo ( False , False ) def __repr__ ( self ) -> str : text = f \"[{self.__class__.__name__}] {self.name} \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Component Inputs: \\n \" for name , input in self . inputs . items (): text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Component Outputs: \\n \" for name , output in self . outputs . items (): text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" return text def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name ) def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name ) def _to_dict ( self ): inputs = [] inputs += [{ 'name' : name , 'type' : self . inputs [ name ][ 'type' ], } for name in self . inputs ] outputs = [] outputs += [{ 'name' : name , 'type' : self . outputs [ name ][ 'type' ], 'metric' : False , } for name in self . outputs ] return { 'name' : self . name , 'description' : self . desc , 'batch' : self . batch . dict (), 'inputType' : inputs , 'outputType' : outputs , } def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass def save ( self , destination , validate ): \"\"\" Empty method for child classess to implement. \"\"\" pass class PythonComponent ( Component ): \"\"\" A pipeline component implemented using Python scripts and libraries. A `PythonComponent` wraps Python code resource files such as saved models into a structured folder, which can be added to a pipeline configuration package. For a comprehensive overview on how to wrap ML models into Python components, we recommend you refer to the AI SDK User Manual, especially the guideline for writing pipeline components. We also recommend you study the example Python components in the E2E Tutorials for the AI SDK. A new `PythonComponent` is empty. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, the current version supports Python 3.10 and 3.11. \"\"\" def __init__ ( self , name = \"inference\" , version = \"0.0.1\" , python_version = '3.10' , desc : str = \"\" ): \"\"\" Creates a new, empty Python component. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, AI Inference Server supports Python 3.10 and 3.11. \"\"\" super () . __init__ ( name = name , desc = desc ) try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) self . python_version = python_version self . version = version self . metrics = {} self . entrypoint : Optional [ Path ] = None self . resources = {} self . python_dependencies = PythonDependencies ( python_version ) self . _replicas = 1 self . is_valid = False def __repr__ ( self ) -> str : text = super () . __repr__ () if len ( self . metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric in self . metrics . items (): text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . resources ): text += \" \\n Resources: \\n \" for path , base in self . resources . items (): text += f \" {base}/{path.name} \\n \" . replace ( './' , '' ) if self . entrypoint is not None : text += f \"Entrypoint: {self.entrypoint} \\n \" return text def set_entrypoint ( self , entrypoint : str ): \"\"\" Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. ```python import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys.path.insert(0, str(Path('./src').resolve())) from my_module import processor # then the processor module can be imported def run(data: str): input_data = json.loads(data) # incoming JSON string is loaded as a dictionary result = processor.process_data(input_data) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None: answer = {\"ready\": False, \"output\": None} else: answer = {\"ready\": True, \"output\": json.dumps(result)} return answer ``` Args: entrypoint (str): Name of the new entrypoint script to be copied \"\"\" self . is_valid = False if not any ( key . name for key , value in self . resources . items () if key . name == entrypoint and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) self . entrypoint = Path ( entrypoint ) def add_resources ( self , base_dir : os . PathLike , resources : Union [ os . PathLike , list ]): \"\"\" Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in '__pycache__' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Args: base_dir (path-like): Root folder of your code from which the resources are referred resources (os.PathLike or List): A single path or list of relative paths to resource files \"\"\" self . is_valid = False base_dir = Path ( base_dir ) . resolve () . absolute () if not base_dir . is_dir (): raise AssertionError ( f \"Parameter 'base_dir' must be a directory and available in path {base_dir}.\" ) resources = resources if type ( resources ) is list else [ resources ] for resource in resources : self . _add_resource ( base_dir , resource ) def _add_resource ( self , base_dir : Path , resource : os . PathLike ): self . is_valid = False if Path ( resource ) . is_absolute () or '..' in resource : raise AssertionError ( \"The resource path must be relative and cannot contain '/../' elements.\" ) resource_path = base_dir / resource if resource_path . is_file (): self . _add_resource_file ( base_dir , resource_path ) return if resource_path . is_dir (): for glob_path in resource_path . rglob ( \"*\" ): if glob_path . is_file (): self . _add_resource_file ( base_dir , glob_path ) return raise AssertionError ( f \"Specified resource is not a file or directory: '{resource}'\" ) def _add_resource_file ( self , base_dir : Path , resource_path : Path ): self . is_valid = False for parent in resource_path . parents : if parent . name == '__pycache__' : return if resource_path in self . resources . keys (): _logger . warning ( f \"Resource '{resource_path}' is already added to target directory '{self.resources[resource_path]}'\" ) return self . resources [ resource_path ] = f \"{resource_path.parent.relative_to(base_dir)}\" def add_dependencies ( self , packages : list ): \"\"\" Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Args: packages (list): Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution \"\"\" self . is_valid = False self . python_dependencies . add_dependencies ( packages ) def set_requirements ( self , requirements_path : os . PathLike ): \"\"\" Reads the defined dependencies from the given `requirements.txt` file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of `--extra-index-url=my.repo.example.com`. Args: requirements_path (str): Path of the given `requirements.txt` file \"\"\" self . is_valid = False self . python_dependencies . set_requirements ( requirements_path ) def add_python_packages ( self , path : str ) -> None : \"\"\" Adds Python package(s) to the `PythonPackages.zip` file of the component. The `path` parameter can refer to either a `whl`, a `zip` or a `tar.gz` file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the `tempfile.tempdir` folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Args: path (str): Path of the distribution file Examples: `component.add_python_packages('../resources/my_package-0.0.1-py3-none-any.wheel')` adds the wheel file to `PythonPackages.zip` and adds dictionary item `component.dependencies['my_package'] = '0.0.1'` `component.add_python_packages('../resources/inference-wheels.zip')` adds all the wheel files in the zip to `PythonPackages.zip` and `component.dependencies` \"\"\" self . is_valid = False self . python_dependencies . add_python_packages ( path ) def set_parallel_steps ( self , replicas ): \"\"\" Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Args: replicas (int): Number of parallel executors. Default is 1. Raises: ValueError: if the given argument is not a positive integer. \"\"\" self . is_valid = False if ( not isinstance ( replicas , int )) or replicas < 1 : raise ValueError ( \"Replica count must be a positive integer.\" ) if 8 < replicas : _logger . warning ( \"The current maximum of parallel executors is 8.\" ) self . _replicas = replicas def add_metric ( self , name : str , desc : Optional [ str ] = None ): \"\"\" Adds a metric that will be automatically used as a pipeline output. Args: name (str): Name of the metric. desc (str): Description of the metric. (optional) \"\"\" if \"_\" not in name : raise AssertionError ( \"The metric name must contain at least one underscore\" ) if self . metrics is None : self . metrics = {} if name in self . metrics : raise AssertionError ( f \"Metric '{name}' already exists\" ) self . metrics [ name ] = {} if desc is not None : self . metrics [ name ][ 'desc' ] = desc def delete_metric ( self , name : str ): \"\"\" Remove a previously added metric. Args: name (str): Name of the metric to be deleted. \"\"\" if name not in self . metrics : raise AssertionError ( f \"Component '{self.name}' has no metric '{name}'\" ) self . metrics . pop ( name ) def _to_dict ( self ): component_dict = { ** super () . _to_dict (), 'version' : self . version , 'entrypoint' : f \"./{self.entrypoint.name}\" , 'hwType' : 'CPU' , 'runtime' : { 'type' : 'python' , 'version' : self . python_version }, 'replicas' : self . _replicas } component_dict [ \"outputType\" ] += [{ 'name' : name , 'type' : 'String' , 'metric' : True , } for name in self . metrics . keys ()] return component_dict def enable_dependency_optimization ( self ): \"\"\" Allows changing repository URLs to optimize the package size Allows the replacement of the `--index-url` argument during `pip download` to download CPU runtime optimized dependencies only. Enabling this optimization, the present `--index-url` will be prepended to the `--extra-index-url` list, and the Pytorch CPU only repository will be set as the `--index-url`. A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . enable_dependency_optimization () def disable_dependency_optimization ( self ): \"\"\" Disables any modification to repository URLs Disables the replacement of the `--index-url` argument during `pip download`. This way all `--index-url` or `--extra-index-url` arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . disable_dependency_optimization () def validate ( self ): \"\"\" Validates that the component is ready to be serialized and packaged as part of a pipeline. \"\"\" if not self . is_valid : if self . entrypoint is None : raise AssertionError ( \"Entrypoint must be defined\" ) if not any ( key . name for key , value in self . resources . items () if key . name == self . entrypoint . name and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) if len ( self . python_dependencies . dependencies ) < 1 : _logger . warning ( f \"WARNING! There are no dependencies defined for component '{self.name}'. Please make sure that all necessary dependencies have been added.\" ) self . python_dependencies . validate () self . is_valid = True _logger . info ( f \"Component '{self.name}' is valid and ready to use.\" ) def save ( self , destination , validate = True ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter `validate` to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: - `requirements.txt` with a list of Python dependencies - Entry point script defined by the `entrypoint` attribute of the component - Extra files as added to the specified folders - `PythonPackages.zip` with the wheel binaries for the environment to be installed Args: destination (path-like): Target directory to which the component will be saved. validate (bool): With value True, triggers component validation. Defaults to True. \"\"\" if validate : self . validate () folder_path = Path ( destination ) / self . name folder_path . mkdir ( parents = True , exist_ok = True ) for file_path in self . resources : dir_path = folder_path / self . resources [ file_path ] os . makedirs ( dir_path , exist_ok = True ) shutil . copy ( file_path , dir_path / file_path . name ) self . python_dependencies . save ( folder_path ) class Pipeline : \"\"\" `Pipeline` represents a pipeline configuration package with `Components` and wires to provide a data flow on the AI Inference Server. The `Components` have inputs and outputs to transfer data to each other and the wires describe this data flow between them. The package also contains configuration files required to deploy a pipeline on an Industrial Edge device. A newly initialized `Pipeline` does not contain any `Component` or wire, only its name and version will be set. The name and version together will define the name of the zip file when the package is saved. Args: name (str): Name of the package version (str): Version of the package \"\"\" _wire_hash_string = \"{}.{} -> {}.{}\" def __init__ ( self , name : str , version : Optional [ str ] = None , desc : str = \"\" ): \"\"\" A newly initialized `Pipeline` will contain no `Component` or wire, just its name and version will be set. The name and version will define together the name of the zip file when the package is saved. Args: name (str): Name of the package desc (str): Package description (optional) version (str): Version of the package \"\"\" self . name = name self . desc = desc self . version = version self . package_id : Optional [ uuid . UUID ] = None self . save_version = None self . save_package_id : Optional [ uuid . UUID ] = None self . author = 'AI SDK' self . components = {} self . wiring = {} self . parameters = {} self . periodicity = None self . timeshift_reference = [] self . inputs = [] self . outputs = [] self . log_level = logging . INFO self . report_writer = PipelineReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) _python_dependencies_logger . addHandler ( report_writer_handler ) _wheelhouse_logger . addHandler ( report_writer_handler ) def _set_log_level ( self , log_level : int ): self . log_level = log_level _logger . setLevel ( self . log_level ) @ staticmethod def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = \"\" ) -> \"Pipeline\" : \"\"\" Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Args: components (list): List of PythonComponents name (str): Name of the pipeline version (str): Version information of the pipeline. (Optional) Returns: Pipeline: Pipeline object with the auto-wired components \"\"\" pipeline = Pipeline ( name , version , desc = desc ) first_component = components [ 0 ] pipeline . add_component ( first_component ) pipeline . inputs = [( first_component . name , component_input ) for component_input in first_component . inputs ] pipeline . outputs = [( first_component . name , output ) for output in first_component . outputs ] for component in components [ 1 :]: pipeline . add_component ( component ) for ( wire_component , wire_name ) in pipeline . outputs : try : pipeline . add_wiring ( wire_component , wire_name , component . name , wire_name ) except Exception as e : _logger . warning ( f \"Output variable {wire_component}.{wire_name} couldn't be auto-wired. \\n Cause: {e}\" ) unwired_variables = [ f '{component.name}.{x}' for x in component . inputs if not any ( s . endswith ( f '{component.name}.{x}' ) for s in pipeline . wiring )] if len ( unwired_variables ) > 0 : for variable in unwired_variables : _logger . warning ( f \"Input variable {variable} couldn't be auto-wired. \\n \" ) pipeline . outputs = [( component . name , output ) for output in component . outputs ] return pipeline def __repr__ ( self ) -> str : \"\"\" Textual representation of the configured package. The method shows the `Components` with their inputs, outputs and parameters as well as the wiring between these `Components`. Returns: [str]: Textual representation of the package \"\"\" text = f \"[{self.__class__.__name__}] {self.name} ({self.version}) \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . parameters ) > 0 : text += \" \\n Pipeline Parameters: \\n \" for name , parameter in self . parameters . items (): text += f \"- {name} ({parameter['type']}, default: '{parameter['defaultValue']}'){(': ' + parameter['desc']) if parameter.get('desc') is not None else ''} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Pipeline Inputs: \\n \" for component , name in self . inputs : input = self . components [ component ] . inputs [ name ] text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Pipeline Outputs: \\n \" for component , name in self . outputs : output = self . components [ component ] . outputs [ name ] text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" metrics = [( name , metric , component_name ) for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name , metric in component . metrics . items ()] if len ( metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric , _ in metrics : text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . wiring ) > 0 : text += \" \\n I/O Wiring: \\n \" for component , name in self . inputs : text += f \" {name} -> {component}.{name} \\n \" for wire_hash in self . wiring : text += f \" {wire_hash} \\n \" for component , name in self . outputs : text += f \" {component}.{name} -> {name} \\n \" for name , metric , component_name in metrics : text += f \" {component_name}.{name} -> {name} \\n \" if self . periodicity is not None : text += \" \\n Timeshifting: \\n \" text += f \" Periodicity: {self.periodicity} ms \\n \" if len ( self . timeshift_reference ) > 0 : text += \" References: \\n \" for ref in self . timeshift_reference : text += f \" - {ref} \\n \" for component in self . components . values (): text += \" \\n \" + component . __repr__ () return text def add_input ( self , component , variable ): \"\"\" Defines an input variable on the given component as a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" try : _ = self . components [ component ] . inputs [ variable ] except KeyError : raise AssertionError ( \"The component with input variable must exist in the pipeline.\" ) if self . inputs is None : self . inputs = [] if ( component , variable ) in self . inputs : raise AssertionError ( \"The pipeline input already exists.\" ) self . inputs . append (( component , variable )) def delete_input ( self , component : str , variable : str ): \"\"\" Deletes a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . inputs : raise AssertionError ( \"The pipeline input does not exist.\" ) self . inputs . remove (( component , variable )) def add_output ( self , component , variable ): \"\"\" Defines an output variable on the given component as a pipeline output. Args: component (str): Name of the component variable (str): Name of the output variable \"\"\" try : _ = self . components [ component ] . outputs [ variable ] except KeyError : raise AssertionError ( \"The component with output variable must exist in the pipeline.\" ) if self . outputs is None : self . outputs = [] if ( component , variable ) in self . outputs : raise AssertionError ( \"The pipeline output already exists.\" ) self . outputs . append (( component , variable )) def delete_output ( self , component : str , variable : str ): \"\"\" Deletes a pipeline output. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . outputs : raise AssertionError ( \"The pipeline output does not exist.\" ) self . outputs . remove (( component , variable )) def add_component ( self , component : Component ): \"\"\" Adds a `Component` to the pipeline configuration without any connection. The `Component` can be marked as an input or output component of the pipeline. When these parameters are True, the `Component` is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Args: component (Component): `Component` to be added \"\"\" if component . name in self . components : raise AssertionError ( f \"Component with name {component.name} already exists. Please rename the component.\" ) self . components [ component . name ] = component def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ): \"\"\" Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: - The components exist with the given inputs/outputs - The given inputs and outputs are not connected to any wire - The types of the connected input and output are compatible Args: from_component (str): Name of the component which provides data to the `to_component` from_output (str): Name of the output variable of the `from_component` to_component (str): Name of the component which consumes data from the `from_component` to_input (str): Name of the input variable of the `to_component` \"\"\" if from_component not in self . components : raise AssertionError ( f \"No component named '{from_component}'\" ) if to_component not in self . components : raise AssertionError ( f \"No component named '{to_component}'\" ) if from_output not in self . components [ from_component ] . outputs : raise AssertionError ( f \"Component '{from_component}' has no output named '{from_output}'\" ) if to_input not in self . components [ to_component ] . inputs : raise AssertionError ( f \"Component '{to_component}' has no input named '{to_input}'\" ) if self . get_wire_for_input ( to_component , to_input ) is not None : raise AssertionError ( f \"Input '{to_input}' of component '{to_component}' is already wired\" ) _output_type = self . components [ from_component ] . outputs [ from_output ][ \"type\" ] _input_type = self . components [ to_component ] . inputs [ to_input ][ \"type\" ] if _output_type != _input_type : raise AssertionError ( \"Output and input types do not match\" ) wire_hash = self . _wire_hash_string . format ( from_component , from_output , to_component , to_input ) self . wiring [ wire_hash ] = { \"fromComponent\" : from_component , \"fromOutput\" : from_output , \"toComponent\" : to_component , \"toInput\" : to_input , } def get_wire_for_output ( self , component_name : str , output_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data provider through its output with name output_name. Args: component_name (str): Name of the data provider component. output_name (str): Name of the output variable of `component_name`. Returns: [dict]: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"fromComponent\" ] == component_name and x [ \"fromOutput\" ] == output_name ] return wires [ 0 ] if wires else None def get_wire_for_input ( self , component_name : str , input_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data consumer through its input with name `input_name`. Args: component_name (str): Name of the data consumer component. input_name (str): Name of the input variable of `component_name`. Returns: dict: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"toComponent\" ] == component_name and x [ \"toInput\" ] == input_name ] return wires [ 0 ] if wires else None def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ): \"\"\" Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Args: component (str): Name of the component which has the input given the name variable variable (str): Name of the input variable on the component which connected by the wire with_input (bool, optional): If set, the input variable will be also deleted from the component. Defaults to True. Raises: AssertionError: When the variable acts as inter signal alignment reference, it cannot be deleted, and an `AssertionError` will be raised. \"\"\" wire = self . get_wire_for_input ( component , variable ) if wire is None : raise AssertionError ( f \"There is no wiring for input '{variable}' of component '{component}'\" ) if variable in self . timeshift_reference : raise AssertionError ( \"Inter signal alignment reference variables can not be deleted.\" ) wire_hash = self . _wire_hash_string . format ( wire [ 'fromComponent' ], wire [ 'fromOutput' ], wire [ 'toComponent' ], wire [ 'toInput' ]) self . wiring . pop ( wire_hash ) if with_input : self . components [ component ] . delete_input ( variable ) def add_dependencies ( self , packages : list ): \"\"\" @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type `PythonComponent`. This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the `requirements.txt` file when the package is saved. Args: packages (list): List of the necessary python packages to execute the script defined by self.entrypoint \"\"\" python_components = [ self . components [ name ] for name in self . components if type ( self . components [ name ]) is PythonComponent ] for component in python_components : component . add_dependencies ( packages ) def set_timeshifting_periodicity ( self , periodicity : int ): \"\"\" Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, `startingPoint` property is set to `First timestamp`, which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to `Signal reference` by adding inter-signal alignment reference variables via the `add_timeshifting_reference(..)` method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Args: periodicity (int): Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). \"\"\" periodicity = int ( periodicity ) if periodicity not in range ( 10 , int ( math . pow ( 2 , 31 ))): raise AssertionError ( \"Inter signal alignment periodicity must be an integer and in range [10, 2^31)\" ) self . periodicity = periodicity _logger . info ( f \"Inter signal alignment periodicity has been set to {self.periodicity}.\" ) def add_timeshifting_reference ( self , reference : str ): \"\"\" Enables signal alignment mode `Signal reference` by declaring input variables as reference variables. Args: reference (str): Variable name to be added to `self.timeshift_reference` list. \"\"\" if reference not in [ name for _ , name in self . inputs ]: raise AssertionError ( f \"There is no input variable defined with name '{reference}'\" ) if reference in self . timeshift_reference : _logger . warning ( f \"Reference variable with name '{reference}' has been already added.\" ) return self . timeshift_reference . append ( reference ) def remove_timeshifting_reference ( self , reference : str ): \"\"\" Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the `startingPoint` will be `First timestamp`. Args: reference (str): Variable name to be removed from `self.timeshift_reference` list. \"\"\" if reference not in self . timeshift_reference : raise AssertionError ( f \"Reference variable with name {'reference'} does not exist.\" ) self . timeshift_reference . remove ( reference ) def get_pipeline_config ( self ): \"\"\" Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the `destination` folder with name `pipeline_config.yml` \"\"\" if self . save_version is None : self . save_version = self . version if self . save_package_id is None : self . save_package_id = self . package_id pipeline_inputs = [] pipeline_inputs += [{ 'name' : name , 'type' : self . components [ component_name ] . inputs [ name ][ 'type' ] } for component_name , name in self . inputs ] pipeline_outputs = [] pipeline_outputs += [{ 'name' : name , 'type' : self . components [ component_name ] . outputs [ name ][ 'type' ], 'metric' : False , } for component_name , name in self . outputs ] pipeline_outputs += [{ 'name' : name , 'type' : 'String' , 'metric' : True , 'topic' : f \"/siemens/edge/aiinference/{self.name}/{self.save_version}/metrics/{component_name}/{name}\" , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] pipeline_dag = [{ 'source' : f \"{wire['fromComponent']}.{wire['fromOutput']}\" , 'target' : f \"{wire['toComponent']}.{wire['toInput']}\" , } for wire in self . wiring . values ()] pipeline_dag += [{ 'source' : f 'Databus.{name}' , 'target' : f '{component_name}.{name}' , } for component_name , name in self . inputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , name in self . outputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] config_yml_content = { 'fileFormatVersion' : '1.2.0' , 'dataFlowPipelineInfo' : { 'author' : self . author , 'createdOn' : datetime . now (), 'dataFlowPipelineVersion' : self . save_version , 'description' : self . desc if self . desc else 'Created by AI SDK' , 'projectName' : self . name , 'packageId' : str ( self . save_package_id ) }, 'dataFlowPipeline' : { 'components' : [ component . _to_dict () for component in self . components . values ()], 'pipelineDag' : pipeline_dag , 'pipelineInputs' : pipeline_inputs , 'pipelineOutputs' : pipeline_outputs , }, 'packageType' : 'full' } if len ( self . parameters . items ()) != 0 : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] = [] for name , parameter in self . parameters . items (): if parameter [ \"topicBased\" ]: config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ], 'topicBased' : parameter [ 'topicBased' ], 'valueTopic' : parameter [ 'valueTopic' ] }) else : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ] }) return config_yml_content def save_pipeline_config ( self , destination ): \"\"\" Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the `destination` folder with name `pipeline_config.yml` Args: destination (path-like): Path of the `destination` directory. \"\"\" with open ( Path ( destination ) / PIPELINE_CONFIG , \"w\" ) as f : yaml . dump ( self . get_pipeline_config (), f ) def get_datalink_metadata ( self ): \"\"\" The method generates metadata information based on available information. Returns: dict: Dictionary with the necessary information for the AI Inference Server. \"\"\" timeshifting = { \"id\" : None , \"enabled\" : False , \"periodicity\" : self . periodicity , \"startingPoint\" : None , } if self . periodicity is not None : timeshifting [ \"enabled\" ] = True timeshifting [ \"startingPoint\" ] = 'First timestamp' if len ( self . timeshift_reference ) > 0 : timeshifting [ \"startingPoint\" ] = 'Signal reference' exported_metadata = { \"fileFormatVersion\" : \"1.0.0\" , \"id\" : None , \"version\" : None , \"createdOn\" : datetime . now (), \"updatedOn\" : datetime . now (), \"timeShifting\" : timeshifting , \"inputs\" : [ { 'name' : _name , 'mapping' : None , 'timeShiftingReference' : _name in self . timeshift_reference , 'type' : self . components [ _component ] . inputs [ _name ][ 'type' ] } for _component , _name in self . inputs ] } return exported_metadata def save_datalink_metadata ( self , destination ): \"\"\" Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the `destination` folder with the name `datalink_metadata.yml` Args: destination (path-like): Path of the destination directory. \"\"\" with open ( Path ( destination ) / DATALINK_METADATA , \"w\" ) as f : yaml . dump ( self . get_datalink_metadata (), f ) def save_telemetry_data ( self , destination : Path ): \"\"\" Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None \"\"\" telemetry_path = destination / TELEMETRY_YAML telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"simaticai package not found\" ) try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"vep-template-sdk package not found\" ) telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ] . python_version for component in self . components if isinstance ( self . components [ component ], PythonComponent ))) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = list ( set ( f . suffix for f in Path ( destination ) . rglob ( \"*\" ) if f . suffix not in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ])) yaml . dump ( telemetry_data , open ( telemetry_path , 'w' )) def save_readme_html ( self , destination ): \"\"\" Saves a `README.html` in the `destination` folder that describes the pipeline. Args: destination (path-like): Path of the destination folder. \"\"\" pipelinePage = _PipelinePage ( self ) readme_html_path = Path ( destination ) / README_HTML readme_html_path . write_text ( pipelinePage . __str__ ()) def validate ( self , destination = \".\" ): \"\"\" Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: - If the package has at least one component - If all wires create connections between existing components and their variables - If metadata is defined and valid. - If a package with the same name already exists in the `destination` folder. In this case a warning message appears and the `save(..)` method overwrites the existing package. - If the package has multiple components and if they are using the same Python version Args: destination (str, optional): Path of the expected destination folder. Defaults to \".\". \"\"\" if len ( self . components ) < 1 : raise AssertionError ( \"The package must have at least one component.\" ) for name , variable in self . outputs : if self . components [ name ] . batch . outputBatch : raise AssertionError ( f \"The component '{name}' has pipeline output defined with variable name '{variable}'. \\ None of component with pipeline output is allowed to provide batch output . \") for wire_hash in self . wiring . copy (): wire = self . wiring [ wire_hash ] self . _check_wiring ( wire , wire_hash ) pipeline_inputs = [ variable for _ , variable in self . inputs ] pipeline_outputs = [ variable for _ , variable in self . outputs ] if any ( variable in pipeline_outputs for variable in pipeline_inputs ): conflicts = set ( pipeline_inputs ) . intersection ( set ( pipeline_outputs )) raise AssertionError ( f \"Pipeline input and output variables must be unique. Conflicting variables: {conflicts}\" ) self . _check_timeshifting () package_path = Path ( destination ) / f \"{self.name}_{self.version}\" . replace ( \" \" , \"-\" ) if package_path . is_dir (): _logger . warning ( f \"Target folder ({package_path}) already exists! Unless changing the package name the package could be invalid and your files will be overwritten!\" ) python_versions = set () for component in self . components : self . components [ component ] . validate () if isinstance ( self . components [ component ], PythonComponent ): python_versions . add ( self . components [ component ] . python_version ) if ( 1 < len ( python_versions )): _logger . warning ( \"The use of multiple python version in a single pipeline is not recommended. We recommend using only one of the supported versions, which are Python 3.10 or 3.11.\" ) _logger . info ( f \"Package '{self.name}' is valid and ready to save.\" ) def _check_timeshifting ( self ): if len ( self . timeshift_reference ) > 0 and self . periodicity is None : raise AssertionError ( \"When using inter signal alignment reference variables, the periodicity must be set.\" ) def _check_wiring ( self , wire , wire_hash ): error_messages = [] if wire [ 'fromComponent' ] not in self . components : error_messages . append ( f \"From component {wire['fromComponent']} does not exist\" ) if wire [ 'toComponent' ] not in self . components : error_messages . append ( f \"To component {wire['toComponent']} does not exist\" ) if wire [ 'fromOutput' ] not in self . components [ wire [ 'fromComponent' ]] . outputs : error_messages . append ( f \"Output variable {wire['fromOutput']} does not exist on component {wire['fromComponent']}\" ) if wire [ 'toInput' ] not in self . components [ wire [ 'toComponent' ]] . inputs : error_messages . append ( f \"Input variable {wire['toInput']} does not exist on component {wire['toComponent']}\" ) if len ( error_messages ) == 0 : from_type_ = self . components [ wire [ 'fromComponent' ]] . outputs [ wire [ 'fromOutput' ]][ 'type' ] to_type_ = self . components [ wire [ 'toComponent' ]] . inputs [ wire [ 'toInput' ]][ 'type' ] if from_type_ != to_type_ : error_messages . append ( f \"The types of input and output variables does not match for wiring {wire_hash}.\" ) if len ( error_messages ) > 0 : self . wiring . pop ( wire_hash ) error_messages . append ( \"The wire has been deleted, please check the variables and re-create the connection.\" ) raise AssertionError ( error_messages . __str__ ()) def save ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as `{package_name}_{package_version}.zip`. If a file with such a name already exists in the `destination` folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name `{package_name}_{package_version}`. If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: - Package folder with name `{package_name}_{package_version}` - `datalink-metadata.yml` - `pipeline-config.yml` - Component folder with name `{component_name}` When the component is a `PythonComponent`, this folder contains: - `requirements.txt` - Entrypoint script defined by the entrypoint of the component - Extra files as added to the specified folders - Source folder with name `src` with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the `destination` folder, an error is raised. Args: destination (str, optional): Target directory for saving the package. Defaults to \".\". package_id (UUID): The optional package ID. If None, a new UUID is generated. \"\"\" self . validate ( destination ) destination = Path ( destination ) if package_id is not None and not isinstance ( package_id , uuid . UUID ): package_id = uuid . UUID ( package_id ) prev_id = None prev_id_version = None prev_version , prev_with_id = self . _find_previous ( destination , package_id ) if prev_with_id is not None : prev_id_version , prev_id = prev_with_id if package_id == prev_id and ( version or self . version ) == str ( prev_id_version ): raise RuntimeError ( f \"package with version '{version or self.version}' and id '{package_id}' already exists\" ) self . save_version = version if version is not None \\ else self . version if self . version is not None \\ else \"1\" if package_id is not None and prev_id is not None and package_id != prev_id \\ else str ( prev_id_version + 1 ) if prev_id_version is not None \\ else str ( prev_version + 1 ) if prev_version is not None and prev_id is None \\ else \"1\" self . save_package_id = package_id if package_id is not None \\ else prev_id if prev_id is not None \\ else uuid . uuid4 () name = self . name . replace ( \" \" , \"-\" ) package_name = f \"{name}_{self.save_version}\" package_file = destination / f \"{package_name}.zip\" specified_version_is_decimal = version is not None and version . isdecimal () or self . version is not None and self . version . isdecimal () if specified_version_is_decimal : if package_file . exists (): p_id = self . _extract_package_id ( package_file , False ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{package_file}'\" ) edge_package_file = destination / f \"{name}-edge_{self.save_version}.zip\" if edge_package_file . exists (): p_id = self . _extract_package_id ( edge_package_file , True ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{edge_package_file}'\" ) destination = destination / package_name destination . mkdir ( parents = True , exist_ok = True ) # Save for component in self . components : self . components [ component ] . save ( destination , False ) if isinstance ( self . components [ component ], PythonComponent ): self . report_writer . add_direct_dependencies ( self . components [ component ] . name , self . components [ component ] . python_dependencies . dependencies ) self . save_datalink_metadata ( destination ) self . save_pipeline_config ( destination ) self . save_readme_html ( destination ) self . save_telemetry_data ( destination ) zip_destination = shutil . make_archive ( base_name = str ( destination . parent / package_name ), format = 'zip' , root_dir = destination . parent , base_dir = package_name , verbose = True , logger = _logger ) pipeline_size = os . path . getsize ( zip_destination ) # zipped package size in bytes pipeline_size_GB = \"{:.2f}\" . format ( pipeline_size / 1000 / 1000 / 1000 ) pipeline_size_limit_GB = \"{:.2f}\" . format ( PIPELINE_SIZE_LIMIT / 1000 / 1000 / 1000 ) if pipeline_size > PIPELINE_SIZE_LIMIT : error_msg = f \"Pipeline size {pipeline_size} bytes ({pipeline_size_GB} GB) exceeds the limit of \" \\ f \"{PIPELINE_SIZE_LIMIT} bytes ({pipeline_size_limit_GB} GB). \" \\ \"Please remove unnecessary files and dependencies and try again.\" _logger . error ( error_msg ) raise RuntimeError ( error_msg ) return Path ( zip_destination ) # TODO: refactor the business logic in PBI 1662648 def _find_previous ( self , destination : Path , package_id : Optional [ uuid . UUID ] = None ) -> Tuple [ Optional [ int ], Optional [ Tuple [ int , uuid . UUID ]]]: latest_version = None latest_with_id = None if Path ( destination ) . is_dir () is False : return None , None for file in destination . glob ( f \"{self.name.replace(' ', '-')}*.zip\" ): zip_version , zip_package_id = self . _extract_package_info ( file ) if not zip_version . isdecimal (): continue zip_version = int ( zip_version ) if zip_package_id is None : # package id in the zip package not present if latest_version is None or zip_version > latest_version : latest_version = zip_version elif package_id is None : if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) else : if package_id != zip_package_id : continue if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) return latest_version , latest_with_id def _extract_package_info ( self , zip_path : Path ) -> Tuple : with zipfile . ZipFile ( zip_path ) as zip_file : config_path = next ( f for f in zip_file . namelist () if f . endswith ( \"pipeline_config.yml\" )) with zip_file . open ( config_path ) as config_file : config = yaml . load ( config_file , Loader = yaml . SafeLoader ) pipeline_info = config . get ( \"dataFlowPipelineInfo\" , {}) version = pipeline_info . get ( \"dataFlowPipelineVersion\" , None ) package_id = pipeline_info . get ( \"packageId\" , None ) package_id = uuid . UUID ( package_id ) if package_id is not None else None return version , package_id def _extract_package_id ( self , package_file : Path , is_edge_package : bool ) -> Optional [ uuid . UUID ]: try : with OpenZipInTemp ( package_file ) as package_dir : if not is_edge_package : package_dir = package_dir / package_file . stem with open ( package_dir / 'pipeline_config.yml' ) as config_file : return uuid . UUID ( yaml . load ( config_file , Loader = yaml . SafeLoader )[ \"dataFlowPipelineInfo\" ][ \"packageId\" ]) except Exception : _logger . debug ( f \"Could not extract Package ID from '{package_file}'\" ) return None def export ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" Export a runnable pipeline package. Args: destination (str): optional target directory for saving the package. Defaults to \".\". package_id (UUID): optional package ID. If None, a new UUID is generated. version (str): optional version. If None, an automatic version number is generated. \"\"\" config_package = None try : config_package = self . save ( destination , package_id , version ) runtime_package = convert_package ( config_package , self . report_writer ) return runtime_package finally : if config_package is not None : Path ( config_package ) . unlink ( missing_ok = True ) def add_parameter ( self , name , default_value , type_name : str = \"String\" , topic_based : bool = False , desc : str = None ): \"\"\" Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Args: name (str): Name of the parameter desc (str): Description of the parameter (optional) type_name (str, optional): Data type of the parameter. Defaults to \"String\". default_value (str): Default value of the parameter topic_based (bool, optional): If true, the parameter can be updated from a message queue. Raises: ValueError: When: - the default value of the parameter is not of the specified data type (`type_name`) or - the specified data type itself is not an allowed data type (not a part of `parameter_types` dict) or - the specified data type is not given in the right format or - the type of the given `topic_based` parameter is not `bool`. \"\"\" parameter_types = { \"String\" : 'str' , \"Integer\" : 'int' , \"Double\" : 'float' , \"Boolean\" : 'bool' } default_value_type = type ( default_value ) . __name__ if type_name not in parameter_types . keys (): raise ValueError ( f \"The given value type is not supported. Please use one of these: {parameter_types.keys()}\" ) if default_value_type != parameter_types [ type_name ]: raise ValueError ( f \"The given value type does not match the type of '{type_name}'. Please use the correct one from these: {list(parameter_types.keys())}\" ) if not isinstance ( topic_based , bool ): raise ValueError ( \"Type of the given `topic_based` parameter is not `bool`.\" ) self . parameters [ name ] = { \"name\" : name , \"type\" : type_name , \"defaultValue\" : default_value , \"topicBased\" : topic_based , \"valueTopic\" : None } if desc is not None : self . parameters [ name ][ \"desc\" ] = desc def convert_package ( zip_path : str or os . PathLike , report_writer : Optional [ PipelineReportWriter ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use Pipeline.export(...) instead. Create an Edge Configuration Package from a given Pipeline Configuration Package. If the input zip file is `{path}/{name}_{version}.zip`, the output file will be created as `{path}/{name}-edge_{version}.zip`. Please make sure that the given zip file comes from a trusted source! If a file with such a name already exists, it is overwritten. First, this method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. Currently, the supported edge devices run Linux on 64-bit x86 architecture, so the accepted Python libraries are restricted to the platform independent ones and packages built for 'x86_64' platforms. AI Inference Server also provides a Python 3.10 and runtime environment, so the supported Python libraries are restricted to Python 3.10 and 3.11 compatible packages. If for the target platform the required dependency is not available on pypi.org and not present in `PythonPackages.zip`, it will log the problem at ERROR level. Then it downloads all dependencies (either direct or transitive), and creates a new zip file, which is validated against the AI Inference Server's schema. This functionality requires pip with version of 21.3.1 or greater. This method can be used from the command line too. Example usage: ``` python -m simaticai convert_package <path_to_pipeline_configuration_package.zip> ``` Args: zip_path (path-like): path to the pipeline configuration package zip file. report_writer (ReportWriter, optional): a ReportWriter object to write the report for a pipeline. Defaults to None. Returns: os.PathLike: The path of the created zip file. Exceptions: PipelineValidationError: If the validation fails. See the logger output for details. \"\"\" zip_path = Path ( zip_path ) if zip_path . stem . find ( '_' ) < 0 : raise AssertionError ( \"The input zip file name must contain an underscore character.\" ) with tempfiles . OpenZipInTemp ( zip_path ) as zip_dir : top_level_items = list ( zip_dir . iterdir ()) if len ( top_level_items ) != 1 : raise AssertionError ( \"The Pipeline Configuration Package must contain a single top level directory.\" ) package_dir = zip_dir / top_level_items [ 0 ] runtime_dir = zip_dir / \"edge_config_package\" runtime_dir . mkdir ( parents = True , exist_ok = True ) config = yaml_helper . read_yaml ( package_dir / PIPELINE_CONFIG ) _validate_with_schema ( \"input pipeline_config.yml\" , config , \"pipeline.schema.json\" ) runtime_config = _generate_runtime_config ( config ) if report_writer is not None : # TODO: consider moving zip_path to the parameter of report_writer.write_report() report_writer . set_path ( Path ( zip_path . parent / f \"{zip_path.stem}_package_report.md\" )) report_writer . set_pipeline_config ( config ) for component in config [ 'dataFlowPipeline' ][ 'components' ]: source_dir = package_dir / component [ \"name\" ] if component [ \"runtime\" ][ \"type\" ] == \"python\" : python_version = component [ 'runtime' ][ 'version' ] try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) dependency_set = _package_component_dependencies ( source_dir , python_version ) if report_writer is not None : report_writer . add_full_dependency_set ( component_name = component [ \"name\" ], dependency_set = dependency_set ) runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"Python\" , }) if component [ \"runtime\" ][ \"type\" ] == \"gpuruntime\" : runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"gpuruntime\" , }) _package_component ( source_dir , runtime_dir / 'components' / f \"{component['name']}_{component['version']}\" ) if report_writer is not None : report_writer . write_report () _logger . info ( f \"Report on {zip_path.stem} is saved to {zip_path.parent}.\" ) shutil . copy ( str ( package_dir / PIPELINE_CONFIG ), str ( runtime_dir / PIPELINE_CONFIG )) datalink_metadata_yaml = package_dir / DATALINK_METADATA if datalink_metadata_yaml . is_file (): shutil . copy ( str ( datalink_metadata_yaml ), runtime_dir / DATALINK_METADATA ) _validate_with_schema ( f \"generated {RUNTIME_CONFIG}\" , runtime_config , \"runtime.schema.json\" ) with open ( runtime_dir / RUNTIME_CONFIG , \"w\" , encoding = \"utf8\" ) as file : yaml . dump ( runtime_config , file ) readme_html = package_dir / README_HTML if readme_html . exists (): ( runtime_dir / README_HTML ) . write_text ( readme_html . read_text ()) telemetry_yaml = package_dir / TELEMETRY_YAML if telemetry_yaml . exists (): ( runtime_dir / TELEMETRY_YAML ) . write_text ( telemetry_yaml . read_text ()) edge_package_path = Path ( shutil . make_archive ( # One Pythonic Way to replace the last occurrence of \"_\" with \"-edge\". base_name = str ( PurePath ( zip_path ) . parent / \"-edge_\" . join ( zip_path . stem . rsplit ( \"_\" , 1 ))), format = 'zip' , root_dir = runtime_dir , verbose = True , logger = _logger )) sha256_hash = calc_sha ( edge_package_path ) sha_format = f \"{sha256_hash} {edge_package_path.name}\" edge_package_path . with_suffix ( '.sha256' ) . write_text ( sha_format ) return edge_package_path def _package_component ( source_dir , target_name ): return shutil . make_archive ( base_name = target_name , format = 'zip' , root_dir = source_dir , verbose = True , logger = _logger ) def _package_component_dependencies ( component_dir : Path , python_version : str ) -> set : python_packages_folder = component_dir / 'packages' requirements_file_path = component_dir / REQUIREMENTS_TXT packages_file = component_dir / PYTHON_PACKAGES_ZIP dependency_set = set () python_packages_folder . mkdir ( exist_ok = True ) if packages_file . is_file (): with zipfile . ZipFile ( packages_file ) as zip_file : zip_file . extractall ( python_packages_folder ) packages_file . unlink () requirements_file_path . touch ( exist_ok = True ) try : dependency_set = create_wheelhouse ( requirements_file_path , python_version , python_packages_folder ) if any ( Path ( python_packages_folder ) . iterdir ()): shutil . make_archive ( base_name = str ( component_dir / PYTHON_PACKAGES ), format = 'zip' , root_dir = python_packages_folder , verbose = True , logger = _logger ) finally : shutil . rmtree ( python_packages_folder ) # This filtering needs to happen here, not in PythonDependencies, # because create_wheelhouse still needs the original requirements.txt # with the extra index urls. with open ( requirements_file_path , \"r\" ) as f : lines = f . readlines () filtered_lines = list ( filter ( lambda x : not ( x . startswith ( \"# Extra\" ) or x . startswith ( \"--extra-index-url\" ) or x . startswith ( \"# Index\" ) or x . startswith ( \"--index-url\" )), lines )) with open ( requirements_file_path , \"w\" ) as f : f . writelines ( filtered_lines ) return dependency_set def _generate_runtime_config ( pipeline_config : dict ): project_name = pipeline_config [ \"dataFlowPipelineInfo\" ][ \"projectName\" ] return { \"fileFormatVersion\" : \"1\" , \"runtimeInfo\" : { \"projectName\" : project_name , \"runtimeConfigurationVersion\" : \"1.0.0\" , \"createdOn\" : datetime . utcnow () . strftime ( \"%Y-%m- %d T%H:%M:%SZ\" ), }, \"runtimeConfiguration\" : { \"devices\" : [{ \"name\" : \"IED1\" , \"address\" : \"localhost\" , # Optional \"arch\" : \"x86_64\" , # Optional, TODO: validate target keys }], \"components\" : [], }, } def _validate_with_schema ( name : str , data : dict , schema : str ): try : jsonschema . validate ( instance = data , schema = json . load ( module_resources . open_text ( \"simaticai.data.schemas\" , schema )) # TODO: after upgrading to python 3.9 # schema=json.load(resources.files(\"simaticai\") / \"data\" / \"schemas\" / \"pipeline.schema.json\") ) except jsonschema . exceptions . ValidationError as e : raise AssertionError ( f \"\"\"Schema validation failed for {name} using '{schema}'! message: {e.message} $id: {e.schema['$id']} title: {e.schema['title']} description: {e.schema['description']} \"\"\" ) from None def _get_pipeline_info ( pipeline_config : str ): pipeline_config = yaml_helper . read_yaml ( pipeline_config ) pipeline_info = pipeline_config [ \"dataFlowPipelineInfo\" ] pipeline_info [ \"packageType\" ] = pipeline_config . get ( \"packageType\" , \"full\" ) pipeline_info [ \"originVersion\" ] = pipeline_config . get ( \"originVersion\" , None ) return pipeline_info def _validate_delta_package_inputs ( origin_package_info : dict , new_package_info : dict ): if origin_package_info [ \"packageType\" ] == \"delta\" or new_package_info [ \"packageType\" ] == \"delta\" : raise AssertionError ( \"Neither of the packages can be delta package!\" ) if origin_package_info [ \"projectName\" ] != new_package_info [ \"projectName\" ]: raise AssertionError ( \"The new edge package must have the same name as the origin edge package!\" ) if origin_package_info [ \"dataFlowPipelineVersion\" ] == new_package_info [ \"dataFlowPipelineVersion\" ]: raise AssertionError ( \"The new edge package can not have the same version as the origin edge package!\" ) def _change_pipeline_config ( config_path : str , origin_package_version : str ): data = yaml_helper . read_yaml ( config_path ) data [ \"packageType\" ] = \"delta\" data [ \"originVersion\" ] = origin_package_version with open ( config_path , \"w\" ) as f : yaml . dump ( data , f ) def _extract_edge_package ( edge_package_zip_path : str , path_to_extract : Path ): zipfile . ZipFile ( edge_package_zip_path ) . extractall ( path_to_extract ) for f in path_to_extract . rglob ( \"*.zip\" ): component_path = path_to_extract / \"components\" / f . stem packages = Path ( component_path , PYTHON_PACKAGES_ZIP ) zipfile . ZipFile ( f ) . extractall ( component_path ) if packages . is_file (): zipfile . ZipFile ( component_path / PYTHON_PACKAGES_ZIP ) . extractall ( component_path / PYTHON_PACKAGES ) os . remove ( packages ) os . remove ( f ) return path_to_extract def _copy_file ( file_path : Path , from_dir : Path , to_dir : Path ): new_path = to_dir / file_path . relative_to ( from_dir ) new_path . parent . mkdir ( parents = True , exist_ok = True ) shutil . copy ( file_path , to_dir / file_path . relative_to ( from_dir )) def create_delta_package ( origin_edge_package_zip_path : str , new_edge_package_zip_path : str ): \"\"\" Creates a Delta Edge Configuration Package from two given Edge Configuration Packages. The created Delta Configuration Package is applicable to import into AI Inference Server, if the Original Edge Configuration Package is already imported there. The Delta Configuration Package only contains the additions and modifications in the New Edge Configuration Package compared to the Original one. That also means that no file deletion is possible in a deployed pipeline via this option. Please make sure that both of the given zip files come from a trusted source! Usage: ~~~python delta_package_path = deployment.create_delta_package('Edge-Config-edge-1.0.0.zip', 'Edge-Config-edge-1.1.0.zip') ~~~ This method can be used from the command line, too. ``` python -m simaticai create_delta_package <origin_package.zip> <modified_package.zip> ``` Once the package is calculated, you will have an `Edge-Config-edge-delta-1.1.0.zip` file beside the updated package zip file. <ul>This package will contain <li><ul>the three configuration file for the package; <li>pipeline_config.yml</li> <li>runtime_config.yml</li> <li>datalink_metadata.yml</li> </li></ul> <li>the newly added files,</li> <li>and the updated files.</li> </ul> The package will not contain any information on the deleted files and they will be copied from the original pipeline. **Caution!** *If you change the version of a component in the pipeline, the delta package will contain all the files of the component because AI Inference Server identifies a component with a different version as a different component!* Args: origin_edge_package_zip_path (str): Path to the origin edge configuration package zip file. new_edge_package_zip_path (str): Path to the new edge configuration package zip file. Returns: os.PathLike: The path of the created delta edge package zip file. Raises: AssertionError: When: - either of the given edge packages is a delta package or - the names of the given edge packages differ or - the versions of the given edge packages are equal. \"\"\" workdir = Path ( tempfile . mkdtemp ( prefix = \"aisdk_deltapack-\" )) delta_dir = Path ( workdir / \"delta\" ) delta_dir . mkdir ( parents = True ) origin_dir = _extract_edge_package ( origin_edge_package_zip_path , Path ( workdir / \"orig\" )) new_dir = _extract_edge_package ( new_edge_package_zip_path , Path ( workdir / \"new\" )) origin_package_info = _get_pipeline_info ( origin_dir / PIPELINE_CONFIG ) new_package_info = _get_pipeline_info ( new_dir / PIPELINE_CONFIG ) _validate_delta_package_inputs ( origin_package_info , new_package_info ) files_in_new_package = new_dir . rglob ( \"*\" ) for f in files_in_new_package : if f . is_dir (): continue orig_file_path = origin_dir / f . relative_to ( new_dir ) if not orig_file_path . exists (): _copy_file ( f , new_dir , delta_dir ) else : checksum_original = calc_sha ( orig_file_path ) checksum_new = calc_sha ( f ) if checksum_original != checksum_new : _copy_file ( f , new_dir , delta_dir ) _change_pipeline_config ( delta_dir / PIPELINE_CONFIG , origin_package_info [ \"dataFlowPipelineVersion\" ]) new_edge_package_zip_path = Path ( new_edge_package_zip_path ) delta_path = _zip_delta_package ( delta_dir , new_edge_package_zip_path ) shutil . rmtree ( workdir , ignore_errors = True ) return Path ( delta_path ) def _zip_delta_package ( delta_dir : Path , new_package_path : Path ): target_folder = new_package_path . parent splitted_name = str ( new_package_path . stem ) . split ( \"_\" ) target_name = \"_\" . join ( splitted_name [: - 1 ]) + \"_delta_\" + \"\" . join ( splitted_name [ - 1 :]) for dir in Path ( delta_dir / \"components\" ) . glob ( \"*\" ): if Path ( dir / PYTHON_PACKAGES ) . is_dir (): shutil . make_archive ( dir / PYTHON_PACKAGES , \"zip\" , dir / PYTHON_PACKAGES ) shutil . rmtree ( dir / PYTHON_PACKAGES ) shutil . make_archive ( dir , \"zip\" , dir ) shutil . rmtree ( dir ) delta_path = shutil . make_archive ( target_folder / target_name , \"zip\" , delta_dir ) return delta_path class _PipelinePage ( markup . page ): def __init__ ( self , pipeline : Pipeline ): super () . __init__ ( 'strict_html' , 'lower' ) self . twotags . append ( \"section\" ) self . init ( title = f \"{pipeline.name} ({pipeline.version})\" , doctype = \"<!DOCTYPE html>\" , charset = \"utf-8\" , lang = \"en\" ) self . section () self . h1 ( f \"Pipeline {pipeline.name} ({pipeline.version})\" ) if pipeline . desc : self . p ( pipeline . desc ) self . html_generate_parameters ( pipeline ) self . html_generate_pipeline_inputs ( pipeline ) self . html_generate_pipeline_outputs ( pipeline ) self . html_generate_io_wiring ( pipeline ) self . html_generate_timeshifting ( pipeline ) for component in pipeline . components . values (): self . html_generate_components ( component ) self . section . close () def html_generate_components ( self , component : Component ): self . hr () self . section () self . h1 ( f \"{component.__class__.__name__} {component.name}\" ) if component . desc : self . p ( component . desc ) self . html_generate_component_inputs ( component ) self . html_generate_component_outputs ( component ) self . html_generate_metrics ( component ) if issubclass ( component . __class__ , PythonComponent ): self . html_generate_resources ( component ) self . html_generate_entrypoints ( component ) self . section . close () def html_generate_parameters ( self , pipeline : Pipeline ): if len ( pipeline . parameters ) > 0 : self . strong ( \"Parameters\" ) self . ul () for name , parameter in pipeline . parameters . items (): self . li () self . i ( f \"{name} ({parameter['type']}, default: '{parameter['defaultValue']}')\" ) self . br () if parameter . get ( 'desc' ) is not None : self . span ( parameter [ 'desc' ]) self . li . close () self . ul . close () def html_generate_pipeline_inputs ( self , pipeline : Pipeline ): if len ( pipeline . inputs ) > 0 : self . strong ( \"Inputs\" ) self . ul () for component , name in pipeline . inputs : input = pipeline . components [ component ] . inputs [ name ] self . li () self . i ( f \"{name} ({input['type']})\" ) self . br () if input . get ( 'desc' ) is not None : self . span ( input [ 'desc' ]) self . li . close () self . ul . close () def html_generate_pipeline_outputs ( self , pipeline : Pipeline ): if len ( pipeline . outputs ) > 0 : self . strong ( \"Outputs\" ) self . ul () for component , name in pipeline . outputs : output = pipeline . components [ component ] . outputs [ name ] self . li () self . i ( f \"{name} ({output['type']})\" ) self . br () if output . get ( 'desc' ) is not None : self . span ( output [ 'desc' ]) self . li . close () self . ul . close () def html_generate_component_inputs ( self , component : Component ): self . strong ( \"Inputs\" ) self . ul () for name , input in component . inputs . items (): self . li () self . i ( f \"{name} ({input['type']})\" ) self . br () if input . get ( 'desc' ) is not None : self . span ( input [ 'desc' ]) self . li . close () self . ul . close () def html_generate_component_outputs ( self , component : Component ): self . strong ( \"Outputs\" ) self . ul () for name , output in component . outputs . items (): self . li () self . i ( f \"{name} ({output['type']})\" ) self . br () if output . get ( 'desc' ) is not None : self . span ( output [ 'desc' ]) self . li . close () self . ul . close () def html_generate_io_wiring ( self , pipeline : Pipeline ): if len ( pipeline . wiring ) > 0 : self . strong ( \"I/O Wiring\" ) self . ul () for component , name in pipeline . inputs : self . li ( f \"{name} &#8594; {component}.{name}\" ) for wire_hash in pipeline . wiring : self . li ( wire_hash . replace ( \"->\" , \"&#8594;\" )) for component , name in pipeline . outputs : self . li ( f \"{component}.{name} &#8594; {name}\" ) self . ul . close () def html_generate_timeshifting ( self , pipeline : Pipeline ): if pipeline . periodicity is not None : self . strong ( \"Timeshifting\" ) self . ul () self . li ( f \"Periodicity: {pipeline.periodicity} ms\" ) self . ul . close () if len ( pipeline . timeshift_reference ) > 0 : self . strong ( \"References\" ) self . ul () for ref in pipeline . timeshift_reference : self . li ( ref ) self . ul . close () def html_generate_resources ( self , component : Component ): if isinstance ( component , PythonComponent ) and component . resources is not None and len ( component . resources ) > 0 : self . strong ( \"Resources\" ) self . ul () for path , base in component . resources . items (): self . li ( f \"{base}/{path.name}\" . replace ( './' , '' )) self . ul . close () def html_generate_entrypoints ( self , component : Component ): if isinstance ( component , PythonComponent ) and component . entrypoint is not None : self . strong ( \"Entrypoint\" ) self . ul () self . li ( component . entrypoint . name ) self . ul . close () def html_generate_metrics ( self , component : Component ): if isinstance ( component , PythonComponent ) and component . metrics is not None and len ( component . metrics ) > 0 : self . strong ( \"Metrics\" ) self . ul () for name , metric in component . metrics . items (): self . li () self . i ( name ) if metric . get ( 'desc' ) is not None : self . br () self . span ( metric [ 'desc' ]) self . li . close () self . ul . close () class GPURuntimeComponent ( Component ): \"\"\" The GPURuntimeComponent is used to define a component that runs on a GPU device. The component works only with ONNX models and can be used in an Inference Pipeline. Attributes: name (str): Component name. version (str): Component version. desc (str): Component description. Methods: use_model(self, path: Union[Path, str], max_batch_size: int, optimization: Optional[model_config.TensorRTOptimization] = None, warmup: model_config.Warmup = None) Add an ONNX model file for the component. use_config(self, path: Union[Path, str]) Use a custom config.pbtxt file instead of the autogenerated one. save(self, destination: Union[Path, str], validate = False) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. \"\"\" def __init__ ( self , name : str = \"inference\" , version : str = \"1\" , desc : str = \"\" ): \"\"\" Creates a new, empty GPU Runtime component. Args: name (str): Component name. (default: inference) version (str): Component version. (default: 1) desc (str): Component description (optional) \"\"\" super () . __init__ ( name = name , desc = desc ) self . version = version self . entrypoint : Union [ Path , None ] = None self . model_path : Union [ Path , None ] = None self . model_version : str = \"1\" self . config : Union [ Path , None ] = None self . auto_config = None def _to_dict ( self ): return { ** super () . _to_dict (), 'version' : self . version , 'entrypoint' : f \"{self.model_version}/{self.entrypoint.name}\" , 'hwType' : 'GPU' , 'runtime' : { 'type' : 'gpuruntime' , 'version' : '0.1.0' , } } def use_model ( self , path : Union [ Path , str ], max_batch_size : int , optimization : Optional [ model_config . TensorRTOptimization ] = None , warmup : model_config . Warmup = None ): \"\"\" Add the ONNX model file for the component. Args: path (Union[Path, str]): The path to the ONNX model file. max_batch_size (int): The maximum batch size for the model. optimization (model_config.TensorRTOptimization, optional): The optimization configuration for the model. Defaults to None. warmup (model_config.Warmup, optional): The warmup configuration for the model. Defaults to None. Raises: AssertionError: If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified model file not found: '{path}'\" ) if path . suffix != \".onnx\" : raise AssertionError ( f \"model file extension is not '.onnx': '{path}'\" ) if max_batch_size < 0 : raise AssertionError ( \"max_batch_size must be greater or equal to 0\" ) self . entrypoint = Path ( \"model.onnx\" ) self . model_path = path if self . config is not None : _logger . warning ( \"Previously added configuration was removed. Component will use the default configuration unless you specify your own.\" ) self . config = None # Remove old automatic variables if self . auto_config is not None : for var in self . auto_config . inputs : self . delete_input ( var [ \"name\" ]) for var in self . auto_config . outputs : self . delete_output ( var [ \"name\" ]) self . auto_config = model_config . ModelConfig ( onnx_path = path , max_batch_size = max_batch_size , warmup = warmup , optimization = optimization ) for var in self . auto_config . inputs : self . add_input ( var [ \"name\" ], var [ \"type\" ]) for var in self . auto_config . outputs : self . add_output ( var [ \"name\" ], var [ \"type\" ]) def use_config ( self , path : Union [ Path , str ]): \"\"\" Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Args: path (Union[Path, str]): The path to the configuration file. Raises: AssertionError: If the specified config file is not found or has an invalid extension. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified config file not found: '{path}'\" ) if path . suffix != \".pbtxt\" : raise AssertionError ( f \"config file extension is not '.pbtxt': '{path}'\" ) _validate_gpuruntime_config ( path ) self . config = path def save ( self , destination : Union [ Path , str ], validate = False ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: - An `.onnx` model file - A `.pbtxt` configuration file Args: destination (path-like): Target directory to which the component will be saved. \"\"\" if self . entrypoint is None : raise AssertionError ( \"An ONNX model file must be specified before the component can be saved.\" ) component_dir = Path ( destination ) / self . name component_dir . mkdir ( parents = True , exist_ok = True ) model_dir = component_dir / self . model_version model_dir . mkdir ( exist_ok = True ) shutil . copy ( self . model_path , model_dir / \"model.onnx\" ) if self . config is None : _logger . warning ( \"Configuration was not specified. Model will be saved with default configuration.\" ) ( component_dir / \"config.pbtxt\" ) . write_text ( f \"{self.auto_config}\" ) else : shutil . copy ( self . config , component_dir ) def _validate_gpuruntime_config ( path : Union [ Path , str ]): with open ( path , 'r' ) as file : text_format . Parse ( file . read (), model_config_pb2 . ModelConfig ())","title":"Module simaticai.deployment"},{"location":"reference/simaticai/deployment.html#variables","text":"DATALINK_METADATA MSG_NOT_FOUND PIPELINE_CONFIG PIPELINE_SIZE_LIMIT PYTHON_PACKAGES PYTHON_PACKAGES_ZIP README_HTML REQUIREMENTS_TXT RUNTIME_CONFIG TELEMETRY_YAML supported_types","title":"Variables"},{"location":"reference/simaticai/deployment.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/deployment.html#convert_package","text":"def convert_package ( zip_path : str , report_writer : Optional [ simaticai . helpers . reporter . PipelineReportWriter ] = None ) -> pathlib . Path @Deprecated, reason: only edge package generation will be supported in the future. Use Pipeline.export(...) instead. Create an Edge Configuration Package from a given Pipeline Configuration Package. If the input zip file is {path}/{name}_{version}.zip , the output file will be created as {path}/{name}-edge_{version}.zip . Please make sure that the given zip file comes from a trusted source! If a file with such a name already exists, it is overwritten. First, this method verifies that the requirements identified by name and version are either included in PythonPackages.zip or available on pypi.org for the target platform. Currently, the supported edge devices run Linux on 64-bit x86 architecture, so the accepted Python libraries are restricted to the platform independent ones and packages built for 'x86_64' platforms. AI Inference Server also provides a Python 3.10 and runtime environment, so the supported Python libraries are restricted to Python 3.10 and 3.11 compatible packages. If for the target platform the required dependency is not available on pypi.org and not present in PythonPackages.zip , it will log the problem at ERROR level. Then it downloads all dependencies (either direct or transitive), and creates a new zip file, which is validated against the AI Inference Server's schema. This functionality requires pip with version of 21.3.1 or greater. This method can be used from the command line too. Example usage: python - m simaticai convert_package < path_to_pipeline_configuration_package . zip > Parameters: Name Type Description Default zip_path path-like path to the pipeline configuration package zip file. None report_writer ReportWriter a ReportWriter object to write the report for a pipeline. Defaults to None. None Returns: Type Description os.PathLike The path of the created zip file. Raises: Type Description PipelineValidationError If the validation fails. See the logger output for details. View Source def convert_package ( zip_path : str or os . PathLike , report_writer : Optional [ PipelineReportWriter ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use Pipeline.export(...) instead. Create an Edge Configuration Package from a given Pipeline Configuration Package. If the input zip file is `{path}/{name}_{version}.zip`, the output file will be created as `{path}/{name}-edge_{version}.zip`. Please make sure that the given zip file comes from a trusted source! If a file with such a name already exists, it is overwritten. First, this method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. Currently, the supported edge devices run Linux on 64-bit x86 architecture, so the accepted Python libraries are restricted to the platform independent ones and packages built for 'x86_64' platforms. AI Inference Server also provides a Python 3.10 and runtime environment, so the supported Python libraries are restricted to Python 3.10 and 3.11 compatible packages. If for the target platform the required dependency is not available on pypi.org and not present in `PythonPackages.zip`, it will log the problem at ERROR level. Then it downloads all dependencies (either direct or transitive), and creates a new zip file, which is validated against the AI Inference Server's schema. This functionality requires pip with version of 21.3.1 or greater. This method can be used from the command line too. Example usage: ``` python -m simaticai convert_package <path_to_pipeline_configuration_package.zip> ``` Args: zip_path (path-like): path to the pipeline configuration package zip file. report_writer (ReportWriter, optional): a ReportWriter object to write the report for a pipeline. Defaults to None. Returns: os.PathLike: The path of the created zip file. Exceptions: PipelineValidationError: If the validation fails. See the logger output for details. \"\"\" zip_path = Path ( zip_path ) if zip_path . stem . find ( '_' ) < 0 : raise AssertionError ( \"The input zip file name must contain an underscore character.\" ) with tempfiles . OpenZipInTemp ( zip_path ) as zip_dir : top_level_items = list ( zip_dir . iterdir ()) if len ( top_level_items ) != 1 : raise AssertionError ( \"The Pipeline Configuration Package must contain a single top level directory.\" ) package_dir = zip_dir / top_level_items [ 0 ] runtime_dir = zip_dir / \"edge_config_package\" runtime_dir . mkdir ( parents = True , exist_ok = True ) config = yaml_helper . read_yaml ( package_dir / PIPELINE_CONFIG ) _validate_with_schema ( \"input pipeline_config.yml\" , config , \"pipeline.schema.json\" ) runtime_config = _generate_runtime_config ( config ) if report_writer is not None : # TODO: consider moving zip_path to the parameter of report_writer.write_report() report_writer . set_path ( Path ( zip_path . parent / f \"{zip_path.stem}_package_report.md\" )) report_writer . set_pipeline_config ( config ) for component in config [ 'dataFlowPipeline' ][ 'components' ]: source_dir = package_dir / component [ \"name\" ] if component [ \"runtime\" ][ \"type\" ] == \"python\" : python_version = component [ 'runtime' ][ 'version' ] try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) dependency_set = _package_component_dependencies ( source_dir , python_version ) if report_writer is not None : report_writer . add_full_dependency_set ( component_name = component [ \"name\" ], dependency_set = dependency_set ) runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"Python\" , }) if component [ \"runtime\" ][ \"type\" ] == \"gpuruntime\" : runtime_config [ \"runtimeConfiguration\" ][ \"components\" ] . append ({ \"name\" : component [ \"name\" ], \"device\" : \"IED1\" , \"targetRuntime\" : \"gpuruntime\" , }) _package_component ( source_dir , runtime_dir / 'components' / f \"{component['name']}_{component['version']}\" ) if report_writer is not None : report_writer . write_report () _logger . info ( f \"Report on {zip_path.stem} is saved to {zip_path.parent}.\" ) shutil . copy ( str ( package_dir / PIPELINE_CONFIG ), str ( runtime_dir / PIPELINE_CONFIG )) datalink_metadata_yaml = package_dir / DATALINK_METADATA if datalink_metadata_yaml . is_file (): shutil . copy ( str ( datalink_metadata_yaml ), runtime_dir / DATALINK_METADATA ) _validate_with_schema ( f \"generated {RUNTIME_CONFIG}\" , runtime_config , \"runtime.schema.json\" ) with open ( runtime_dir / RUNTIME_CONFIG , \"w\" , encoding = \"utf8\" ) as file : yaml . dump ( runtime_config , file ) readme_html = package_dir / README_HTML if readme_html . exists (): ( runtime_dir / README_HTML ) . write_text ( readme_html . read_text ()) telemetry_yaml = package_dir / TELEMETRY_YAML if telemetry_yaml . exists (): ( runtime_dir / TELEMETRY_YAML ) . write_text ( telemetry_yaml . read_text ()) edge_package_path = Path ( shutil . make_archive ( # One Pythonic Way to replace the last occurrence of \"_\" with \"-edge\". base_name = str ( PurePath ( zip_path ) . parent / \"-edge_\" . join ( zip_path . stem . rsplit ( \"_\" , 1 ))), format = 'zip' , root_dir = runtime_dir , verbose = True , logger = _logger )) sha256_hash = calc_sha ( edge_package_path ) sha_format = f \"{sha256_hash} {edge_package_path.name}\" edge_package_path . with_suffix ( '.sha256' ) . write_text ( sha_format ) return edge_package_path","title":"convert_package"},{"location":"reference/simaticai/deployment.html#create_delta_package","text":"def create_delta_package ( origin_edge_package_zip_path : str , new_edge_package_zip_path : str ) Creates a Delta Edge Configuration Package from two given Edge Configuration Packages. The created Delta Configuration Package is applicable to import into AI Inference Server, if the Original Edge Configuration Package is already imported there. The Delta Configuration Package only contains the additions and modifications in the New Edge Configuration Package compared to the Original one. That also means that no file deletion is possible in a deployed pipeline via this option. Please make sure that both of the given zip files come from a trusted source! Usage: delta_package_path = deployment . create_delta_package ( 'Edge-Config-edge-1.0.0.zip' , 'Edge-Config-edge-1.1.0.zip' ) This method can be used from the command line, too. python - m simaticai create_delta_package < origin_package . zip > < modified_package . zip > Once the package is calculated, you will have an Edge-Config-edge-delta-1.1.0.zip file beside the updated package zip file. This package will contain the three configuration file for the package; pipeline_config.yml runtime_config.yml datalink_metadata.yml the newly added files, and the updated files. The package will not contain any information on the deleted files and they will be copied from the original pipeline. Caution! If you change the version of a component in the pipeline, the delta package will contain all the files of the component because AI Inference Server identifies a component with a different version as a different component! Parameters: Name Type Description Default origin_edge_package_zip_path str Path to the origin edge configuration package zip file. None new_edge_package_zip_path str Path to the new edge configuration package zip file. None Returns: Type Description os.PathLike The path of the created delta edge package zip file. Raises: Type Description AssertionError When: - either of the given edge packages is a delta package or - the names of the given edge packages differ or - the versions of the given edge packages are equal. View Source def create_delta_package ( origin_edge_package_zip_path : str , new_edge_package_zip_path : str ): \"\"\" Creates a Delta Edge Configuration Package from two given Edge Configuration Packages. The created Delta Configuration Package is applicable to import into AI Inference Server, if the Original Edge Configuration Package is already imported there. The Delta Configuration Package only contains the additions and modifications in the New Edge Configuration Package compared to the Original one. That also means that no file deletion is possible in a deployed pipeline via this option. Please make sure that both of the given zip files come from a trusted source! Usage: ~~~python delta_package_path = deployment.create_delta_package('Edge-Config-edge-1.0.0.zip', 'Edge-Config-edge-1.1.0.zip') ~~~ This method can be used from the command line, too. ``` python -m simaticai create_delta_package <origin_package.zip> <modified_package.zip> ``` Once the package is calculated, you will have an `Edge-Config-edge-delta-1.1.0.zip` file beside the updated package zip file. <ul>This package will contain <li><ul>the three configuration file for the package; <li>pipeline_config.yml</li> <li>runtime_config.yml</li> <li>datalink_metadata.yml</li> </li></ul> <li>the newly added files,</li> <li>and the updated files.</li> </ul> The package will not contain any information on the deleted files and they will be copied from the original pipeline. **Caution!** *If you change the version of a component in the pipeline, the delta package will contain all the files of the component because AI Inference Server identifies a component with a different version as a different component!* Args: origin_edge_package_zip_path (str): Path to the origin edge configuration package zip file. new_edge_package_zip_path (str): Path to the new edge configuration package zip file. Returns: os.PathLike: The path of the created delta edge package zip file. Raises: AssertionError: When: - either of the given edge packages is a delta package or - the names of the given edge packages differ or - the versions of the given edge packages are equal. \"\"\" workdir = Path ( tempfile . mkdtemp ( prefix = \"aisdk_deltapack-\" )) delta_dir = Path ( workdir / \"delta\" ) delta_dir . mkdir ( parents = True ) origin_dir = _extract_edge_package ( origin_edge_package_zip_path , Path ( workdir / \"orig\" )) new_dir = _extract_edge_package ( new_edge_package_zip_path , Path ( workdir / \"new\" )) origin_package_info = _get_pipeline_info ( origin_dir / PIPELINE_CONFIG ) new_package_info = _get_pipeline_info ( new_dir / PIPELINE_CONFIG ) _validate_delta_package_inputs ( origin_package_info , new_package_info ) files_in_new_package = new_dir . rglob ( \"*\" ) for f in files_in_new_package : if f . is_dir (): continue orig_file_path = origin_dir / f . relative_to ( new_dir ) if not orig_file_path . exists (): _copy_file ( f , new_dir , delta_dir ) else : checksum_original = calc_sha ( orig_file_path ) checksum_new = calc_sha ( f ) if checksum_original != checksum_new : _copy_file ( f , new_dir , delta_dir ) _change_pipeline_config ( delta_dir / PIPELINE_CONFIG , origin_package_info [ \"dataFlowPipelineVersion\" ]) new_edge_package_zip_path = Path ( new_edge_package_zip_path ) delta_path = _zip_delta_package ( delta_dir , new_edge_package_zip_path ) shutil . rmtree ( workdir , ignore_errors = True ) return Path ( delta_path )","title":"create_delta_package"},{"location":"reference/simaticai/deployment.html#find_dependencies","text":"def find_dependencies ( name : str , dependencies : dict ) @Deprecated, reason: uses 'pip show' which only works for installed packages on the current platform. Collects all dependencies of the Python module given with its name in the current Python environment. All inherited dependencies will be added to the dependencies dictionary with the installed version of the module. The method executes an OS command like python -m pip show scikit-learn . Parameters: Name Type Description Default name str Name of the Python module to be searched through for its dependencies. None dependencies dict Dictionary to collect the dependencies with the module name as key, and the installed version as value. None Returns: Type Description dict The dependencies dictionary with the collected module names and versions. View Source def find_dependencies ( name : str , dependencies : dict ) : \" \"\" @Deprecated, reason: uses 'pip show' which only works for installed packages on the current platform. Collects all dependencies of the Python module given with its `name` in the current Python environment. All inherited dependencies will be added to the `dependencies` dictionary with the installed version of the module. The method executes an OS command like `python -m pip show scikit-learn`. Args: name (str): Name of the Python module to be searched through for its dependencies. dependencies (dict): Dictionary to collect the dependencies with the module name as key, and the installed version as value. Returns: dict: The `dependencies` dictionary with the collected module names and versions. \"\" \" cmd_line = [ sys . executable , '-m' , 'pip' , 'show' , name ] result = subprocess . run ( cmd_line , stdout = subprocess . PIPE , text = True ) if result . returncode != 0 : print ( f \"Dependency {name} is not found and cannot be added.\" ) return dependencies version = None for line in result . stdout . splitlines () : version_matches = _version_matcher . match ( line ) if version_matches : version = version_matches . groups () [ 0 ] . strip () transitive_matches = _transitive_matcher . match ( line ) if transitive_matches : transitives = transitive_matches . groups () [ 0 ] . split ( \", \" ) for dependency in transitives : if dependency not in dependencies : find_dependencies ( dependency , dependencies ) if name not in dependencies : spec = pep508 . Spec ( name , [] , [ ( '==' , version ) ] if version else [] , None ) dependencies [ name ] = spec print ( \"Found:\" , spec ) return dependencies","title":"find_dependencies"},{"location":"reference/simaticai/deployment.html#python_version_validator","text":"def python_version_validator ( version : str ) Checks if Python version string is valid and describes supported version. Only version 3.10 and 3.11 is supported. A patch version is optional and accepted but logs a warning. Accepted syntaxes are: - {major}.{minor} - {major}.{minor}.{patch} Parameters: Name Type Description Default version str Python version string None Raises: Type Description ValueError if the provided version is not supported View Source def python_version_validator(version: str): \"\"\" Checks if Python version string is valid and describes supported version. Only version 3.10 and 3.11 is supported. A patch version is optional and accepted but logs a warning. Accepted syntaxes are: - {major}.{minor} - {major}.{minor}.{patch} Args: version (str): Python version string Raises: ValueError: if the provided version is not supported \"\"\" supported_versions = [\"3.10\", \"3.11\"] error_message = \"The defined python version is not supported. Currently supported Python versions are 3.10 and 3.11. Python version must be specified only with major and minor version, e.g. '3.10'.\" warning_message = \"\"\"Required Python version was specified with patch version. Please note that the patch digit of the required Python version is often not taken into account by the Python ecosystem, so there is no guarantee it has the desired effect.\"\"\" python_version_matcher = re.match(r'^(3)\\.(0|[1-9][0-9]*)\\.?(0|[1-9][0-9]*)?$', str(version)) major_minor_version = \"0.0\" has_patch_version = False if python_version_matcher is not None: major_minor_version = f\"{python_version_matcher.group(1)}.{python_version_matcher.group(2)}\" has_patch_version = python_version_matcher.group(3) is not None if major_minor_version not in supported_versions: raise ValueError(error_message) if has_patch_version: _logger.warning(warning_message)","title":"python_version_validator"},{"location":"reference/simaticai/deployment.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/deployment.html#component","text":"Base class for pipeline components, with name, description, and a list of inputs and outputs. A new component is created with the given name and an empty input and output list. class Component ( name : str , desc : str = '' )","title":"Component"},{"location":"reference/simaticai/deployment.html#attributes","text":"Name Type Description Default name str Name of the component None desc str Optional description of the component None inputs dict Dictionary of (name, type) pairs, which describe the input variables None outputs dict Dictionary of (name, type) pairs, which describe the output variables None View Source class Component : \"\"\" Base class for pipeline components, with name, description, and a list of inputs and outputs. A new component is created with the given name and an empty input and output list. Args: name (str): Name of the component desc (str): Optional description of the component inputs (dict): Dictionary of (name, type) pairs, which describe the input variables outputs (dict): Dictionary of (name, type) pairs, which describe the output variables \"\"\" reserved_names = [ \"timestamp\" ] @ dataclass class BatchInfo : \"\"\" Batch information for the component. This attribute specifies whether the component can handle batch input or output data. When set to True, the component will receive data in the form of a list of dictionaries instead of a single dictionary. It is important to note that the input and output variables on the component should still be defined as if they are single variables. If the input of the pipeline is configured for batch processing, it is recommended not to configure timeshifting, as the list will have the same timestamp for all elements, potentially resulting in data loss. \"\"\" inputBatch : bool = False outputBatch : bool = False def dict ( self ): return { 'inputBatch' : 'Yes' if self . inputBatch is True else 'No' , 'outputBatch' : 'Yes' if self . outputBatch is True else 'No' } def __init__ ( self , name : str , desc : str = \"\" ): \"\"\" Creates a new component with the given name and an empty input and output list. Args: name (str): Name of the component. desc (str): Optional description of the component \"\"\" self . name = name self . desc = desc self . inputs = {} self . outputs = {} self . batch = self . BatchInfo ( False , False ) def __repr__ ( self ) -> str : text = f \"[{self.__class__.__name__}] {self.name} \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Component Inputs: \\n \" for name , input in self . inputs . items (): text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Component Outputs: \\n \" for name , output in self . outputs . items (): text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" return text def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name ) def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name ) def _to_dict ( self ): inputs = [] inputs += [{ 'name' : name , 'type' : self . inputs [ name ][ 'type' ], } for name in self . inputs ] outputs = [] outputs += [{ 'name' : name , 'type' : self . outputs [ name ][ 'type' ], 'metric' : False , } for name in self . outputs ] return { 'name' : self . name , 'description' : self . desc , 'batch' : self . batch . dict (), 'inputType' : inputs , 'outputType' : outputs , } def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass def save ( self , destination , validate ): \"\"\" Empty method for child classess to implement. \"\"\" pass","title":"Attributes"},{"location":"reference/simaticai/deployment.html#descendants","text":"simaticai.deployment.PythonComponent simaticai.deployment.GPURuntimeComponent","title":"Descendants"},{"location":"reference/simaticai/deployment.html#class-variables","text":"BatchInfo reserved_names","title":"Class variables"},{"location":"reference/simaticai/deployment.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/deployment.html#add_input","text":"def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector payload = { \"image\" : { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height , \"mimeType\" : [ \"image/raw\" ], \"dataType\" : \"uint8\" , \"channelsPerPixel\" : 3 , \"image\" : _swap_bytes ( image . tobytes ()) } } Between components the format is the same format as the format of Object as an output. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new input. None _type str Type of the new input. None desc str Description of the input. (optional) None View Source def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"add_input"},{"location":"reference/simaticai/deployment.html#add_output","text":"def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type Object the entrypoint must return with a dictionary containing two fields, where one field has type str and the other field has type bytes . The example below shows the required format, assuming that 'image' is a PIL Image. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new output. None _type str Type of the new output. None desc str Description of the output. (optional) None View Source def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc","title":"add_output"},{"location":"reference/simaticai/deployment.html#change_input","text":"def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the inputs of the component. Parameters: Name Type Description Default name str Name of the input to be changed. None _type str New type of the input. None desc str Description of the input. (optional) None View Source def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"change_input"},{"location":"reference/simaticai/deployment.html#change_output","text":"def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the outputs of the component. Parameters: Name Type Description Default name str Name of the output to be changed. None _type str The new type of the output. None desc str Description of the output. (optional) None View Source def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"change_output"},{"location":"reference/simaticai/deployment.html#delete_input","text":"def delete_input ( self , name : str ) Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use package.delete_input_wire(...) with default parameter with_input=True . Parameters: Name Type Description Default name str Name of the input to be deleted. None View Source def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name )","title":"delete_input"},{"location":"reference/simaticai/deployment.html#delete_output","text":"def delete_output ( self , name : str ) Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Parameters: Name Type Description Default name str Name of the output to be deleted. None View Source def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name )","title":"delete_output"},{"location":"reference/simaticai/deployment.html#save","text":"def save ( self , destination , validate ) Empty method for child classess to implement. View Source def save ( self , destination , validate ): \"\"\" Empty method for child classess to implement. \"\"\" pass","title":"save"},{"location":"reference/simaticai/deployment.html#validate","text":"def validate ( self ) Empty method for child classess to implement. View Source def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass","title":"validate"},{"location":"reference/simaticai/deployment.html#gpuruntimecomponent","text":"The GPURuntimeComponent is used to define a component that runs on a GPU device. The component works only with ONNX models and can be used in an Inference Pipeline. class GPURuntimeComponent ( name : str = 'inference' , version : str = '1' , desc : str = '' )","title":"GPURuntimeComponent"},{"location":"reference/simaticai/deployment.html#attributes_1","text":"Name Type Description Default name str Component name. None version str Component version. None desc str Component description. None View Source class GPURuntimeComponent ( Component ): \"\"\" The GPURuntimeComponent is used to define a component that runs on a GPU device. The component works only with ONNX models and can be used in an Inference Pipeline. Attributes: name (str): Component name. version (str): Component version. desc (str): Component description. Methods: use_model(self, path: Union[Path, str], max_batch_size: int, optimization: Optional[model_config.TensorRTOptimization] = None, warmup: model_config.Warmup = None) Add an ONNX model file for the component. use_config(self, path: Union[Path, str]) Use a custom config.pbtxt file instead of the autogenerated one. save(self, destination: Union[Path, str], validate = False) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. \"\"\" def __init__ ( self , name : str = \"inference\" , version : str = \"1\" , desc : str = \"\" ): \"\"\" Creates a new, empty GPU Runtime component. Args: name (str): Component name. (default: inference) version (str): Component version. (default: 1) desc (str): Component description (optional) \"\"\" super (). __init__ ( name = name , desc = desc ) self . version = version self . entrypoint : Union [ Path , None ] = None self . model_path : Union [ Path , None ] = None self . model_version : str = \"1\" self . config : Union [ Path , None ] = None self . auto_config = None def _to_dict ( self ): return { ** super (). _to_dict (), ' version ' : self . version , ' entrypoint ' : f \"{self.model_version}/{self.entrypoint.name}\" , ' hwType ' : ' GPU ' , ' runtime ' : { ' type ' : ' gpuruntime ' , ' version ' : ' 0.1.0 ' , } } def use_model ( self , path : Union [ Path , str ], max_batch_size : int , optimization : Optional [ model_config . TensorRTOptimization ] = None , warmup : model_config . Warmup = None ): \"\"\" Add the ONNX model file for the component. Args: path (Union[Path, str]): The path to the ONNX model file. max_batch_size (int): The maximum batch size for the model. optimization (model_config.TensorRTOptimization, optional): The optimization configuration for the model. Defaults to None. warmup (model_config.Warmup, optional): The warmup configuration for the model. Defaults to None. Raises: AssertionError: If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified model file not found: '{path}'\" ) if path . suffix != \".onnx\" : raise AssertionError ( f \"model file extension is not '.onnx': '{path}'\" ) if max_batch_size < 0 : raise AssertionError ( \"max_batch_size must be greater or equal to 0\" ) self . entrypoint = Path ( \"model.onnx\" ) self . model_path = path if self . config is not None : _logger . warning ( \"Previously added configuration was removed. Component will use the default configuration unless you specify your own.\" ) self . config = None # Remove old automatic variables if self . auto_config is not None : for var in self . auto_config . inputs : self . delete_input ( var [ \"name\" ]) for var in self . auto_config . outputs : self . delete_output ( var [ \"name\" ]) self . auto_config = model_config . ModelConfig ( onnx_path = path , max_batch_size = max_batch_size , warmup = warmup , optimization = optimization ) for var in self . auto_config . inputs : self . add_input ( var [ \"name\" ], var [ \"type\" ]) for var in self . auto_config . outputs : self . add_output ( var [ \"name\" ], var [ \"type\" ]) def use_config ( self , path : Union [ Path , str ]): \"\"\" Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Args: path (Union[Path, str]): The path to the configuration file. Raises: AssertionError: If the specified config file is not found or has an invalid extension. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified config file not found: '{path}'\" ) if path . suffix != \".pbtxt\" : raise AssertionError ( f \"config file extension is not '.pbtxt': '{path}'\" ) _validate_gpuruntime_config ( path ) self . config = path def save ( self , destination : Union [ Path , str ], validate = False ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: - An `.onnx` model file - A `.pbtxt` configuration file Args: destination (path-like): Target directory to which the component will be saved. \"\"\" if self . entrypoint is None : raise AssertionError ( \"An ONNX model file must be specified before the component can be saved.\" ) component_dir = Path ( destination ) / self . name component_dir . mkdir ( parents = True , exist_ok = True ) model_dir = component_dir / self . model_version model_dir . mkdir ( exist_ok = True ) shutil . copy ( self . model_path , model_dir / \"model.onnx\" ) if self . config is None : _logger . warning ( \"Configuration was not specified. Model will be saved with default configuration.\" ) ( component_dir / \"config.pbtxt\" ). write_text ( f \"{self.auto_config}\" ) else : shutil . copy ( self . config , component_dir )","title":"Attributes"},{"location":"reference/simaticai/deployment.html#ancestors-in-mro","text":"simaticai.deployment.Component","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/deployment.html#class-variables_1","text":"BatchInfo reserved_names","title":"Class variables"},{"location":"reference/simaticai/deployment.html#methods_1","text":"","title":"Methods"},{"location":"reference/simaticai/deployment.html#add_input_1","text":"def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector payload = { \"image\" : { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height , \"mimeType\" : [ \"image/raw\" ], \"dataType\" : \"uint8\" , \"channelsPerPixel\" : 3 , \"image\" : _swap_bytes ( image . tobytes ()) } } Between components the format is the same format as the format of Object as an output. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new input. None _type str Type of the new input. None desc str Description of the input. (optional) None View Source def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"add_input"},{"location":"reference/simaticai/deployment.html#add_output_1","text":"def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type Object the entrypoint must return with a dictionary containing two fields, where one field has type str and the other field has type bytes . The example below shows the required format, assuming that 'image' is a PIL Image. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new output. None _type str Type of the new output. None desc str Description of the output. (optional) None View Source def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc","title":"add_output"},{"location":"reference/simaticai/deployment.html#change_input_1","text":"def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the inputs of the component. Parameters: Name Type Description Default name str Name of the input to be changed. None _type str New type of the input. None desc str Description of the input. (optional) None View Source def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"change_input"},{"location":"reference/simaticai/deployment.html#change_output_1","text":"def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the outputs of the component. Parameters: Name Type Description Default name str Name of the output to be changed. None _type str The new type of the output. None desc str Description of the output. (optional) None View Source def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"change_output"},{"location":"reference/simaticai/deployment.html#delete_input_1","text":"def delete_input ( self , name : str ) Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use package.delete_input_wire(...) with default parameter with_input=True . Parameters: Name Type Description Default name str Name of the input to be deleted. None View Source def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name )","title":"delete_input"},{"location":"reference/simaticai/deployment.html#delete_output_1","text":"def delete_output ( self , name : str ) Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Parameters: Name Type Description Default name str Name of the output to be deleted. None View Source def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name )","title":"delete_output"},{"location":"reference/simaticai/deployment.html#save_1","text":"def save ( self , destination : Union [ pathlib . Path , str ], validate = False ) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: An .onnx model file A .pbtxt configuration file Parameters: Name Type Description Default destination path-like Target directory to which the component will be saved. None View Source def save ( self , destination : Union [ Path , str ], validate = False ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. The component folder contains the following: - An `.onnx` model file - A `.pbtxt` configuration file Args: destination (path-like): Target directory to which the component will be saved. \"\"\" if self . entrypoint is None : raise AssertionError ( \"An ONNX model file must be specified before the component can be saved.\" ) component_dir = Path ( destination ) / self . name component_dir . mkdir ( parents = True , exist_ok = True ) model_dir = component_dir / self . model_version model_dir . mkdir ( exist_ok = True ) shutil . copy ( self . model_path , model_dir / \"model.onnx\" ) if self . config is None : _logger . warning ( \"Configuration was not specified. Model will be saved with default configuration.\" ) ( component_dir / \"config.pbtxt\" ). write_text ( f \"{self.auto_config}\" ) else : shutil . copy ( self . config , component_dir )","title":"save"},{"location":"reference/simaticai/deployment.html#use_config","text":"def use_config ( self , path : Union [ pathlib . Path , str ] ) Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Parameters: Name Type Description Default path Union[Path, str] The path to the configuration file. None Raises: Type Description AssertionError If the specified config file is not found or has an invalid extension. View Source def use_config(self, path: Union[Path, str]): \"\"\" Sets the configuration file to be used for inference. Intended usage is to use a custom configuration file instead of the autogenerated one. This way extra configurations can be added to the component, such as the execution accelerator. Args: path (Union[Path, str]): The path to the configuration file. Raises: AssertionError: If the specified config file is not found or has an invalid extension. \"\"\" path = Path(path) if not path.is_file(): raise AssertionError(f\"specified config file not found: '{path}'\") if path.suffix != \".pbtxt\": raise AssertionError(f\"config file extension is not '.pbtxt': '{path}'\") _validate_gpuruntime_config(path) self.config = path","title":"use_config"},{"location":"reference/simaticai/deployment.html#use_model","text":"def use_model ( self , path : Union [ pathlib . Path , str ], max_batch_size : int , optimization : Optional [ simaticai . helpers . model_config . TensorRTOptimization ] = None , warmup : simaticai . helpers . model_config . Warmup = None ) Add the ONNX model file for the component. Parameters: Name Type Description Default path Union[Path, str] The path to the ONNX model file. None max_batch_size int The maximum batch size for the model. None optimization model_config.TensorRTOptimization The optimization configuration for the model. Defaults to None. None warmup model_config.Warmup The warmup configuration for the model. Defaults to None. None Raises: Type Description AssertionError If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. View Source def use_model ( self , path : Union [ Path , str ], max_batch_size : int , optimization : Optional [ model_config . TensorRTOptimization ] = None , warmup : model_config . Warmup = None ): \"\"\" Add the ONNX model file for the component. Args: path (Union[Path, str]): The path to the ONNX model file. max_batch_size (int): The maximum batch size for the model. optimization (model_config.TensorRTOptimization, optional): The optimization configuration for the model. Defaults to None. warmup (model_config.Warmup, optional): The warmup configuration for the model. Defaults to None. Raises: AssertionError: If the specified model file is not found, has an invalid extension, or if max_batch_size is less than 0. \"\"\" path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"specified model file not found: '{path}'\" ) if path . suffix != \".onnx\" : raise AssertionError ( f \"model file extension is not '.onnx': '{path}'\" ) if max_batch_size < 0 : raise AssertionError ( \"max_batch_size must be greater or equal to 0\" ) self . entrypoint = Path ( \"model.onnx\" ) self . model_path = path if self . config is not None : _logger . warning ( \"Previously added configuration was removed. Component will use the default configuration unless you specify your own.\" ) self . config = None # Remove old automatic variables if self . auto_config is not None : for var in self . auto_config . inputs : self . delete_input ( var [ \"name\" ]) for var in self . auto_config . outputs : self . delete_output ( var [ \"name\" ]) self . auto_config = model_config . ModelConfig ( onnx_path = path , max_batch_size = max_batch_size , warmup = warmup , optimization = optimization ) for var in self . auto_config . inputs : self . add_input ( var [ \"name\" ], var [ \"type\" ]) for var in self . auto_config . outputs : self . add_output ( var [ \"name\" ], var [ \"type\" ])","title":"use_model"},{"location":"reference/simaticai/deployment.html#validate_1","text":"def validate ( self ) Empty method for child classess to implement. View Source def validate ( self ): \"\"\" Empty method for child classess to implement. \"\"\" pass","title":"validate"},{"location":"reference/simaticai/deployment.html#pipeline","text":"Pipeline represents a pipeline configuration package with Components and wires to provide a data flow on the AI Inference Server. The Components have inputs and outputs to transfer data to each other and the wires describe this data flow between them. The package also contains configuration files required to deploy a pipeline on an Industrial Edge device. A newly initialized Pipeline does not contain any Component or wire, only its name and version will be set. The name and version together will define the name of the zip file when the package is saved. class Pipeline ( name : str , version : Optional [ str ] = None , desc : str = '' )","title":"Pipeline"},{"location":"reference/simaticai/deployment.html#attributes_2","text":"Name Type Description Default name str Name of the package None version str Version of the package None View Source class Pipeline : \"\"\" `Pipeline` represents a pipeline configuration package with `Components` and wires to provide a data flow on the AI Inference Server. The `Components` have inputs and outputs to transfer data to each other and the wires describe this data flow between them. The package also contains configuration files required to deploy a pipeline on an Industrial Edge device. A newly initialized `Pipeline` does not contain any `Component` or wire, only its name and version will be set. The name and version together will define the name of the zip file when the package is saved. Args: name (str): Name of the package version (str): Version of the package \"\"\" _wire_hash_string = \"{}.{} -> {}.{}\" def __init__ ( self , name : str , version : Optional [ str ] = None , desc : str = \"\" ): \"\"\" A newly initialized `Pipeline` will contain no `Component` or wire, just its name and version will be set. The name and version will define together the name of the zip file when the package is saved. Args: name (str): Name of the package desc (str): Package description (optional) version (str): Version of the package \"\"\" self . name = name self . desc = desc self . version = version self . package_id : Optional [ uuid . UUID ] = None self . save_version = None self . save_package_id : Optional [ uuid . UUID ] = None self . author = 'AI SDK' self . components = {} self . wiring = {} self . parameters = {} self . periodicity = None self . timeshift_reference = [] self . inputs = [] self . outputs = [] self . log_level = logging . INFO self . report_writer = PipelineReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) _python_dependencies_logger . addHandler ( report_writer_handler ) _wheelhouse_logger . addHandler ( report_writer_handler ) def _set_log_level ( self , log_level : int ): self . log_level = log_level _logger . setLevel ( self . log_level ) @ staticmethod def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = \"\" ) -> \"Pipeline\" : \"\"\" Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Args: components (list): List of PythonComponents name (str): Name of the pipeline version (str): Version information of the pipeline. (Optional) Returns: Pipeline: Pipeline object with the auto-wired components \"\"\" pipeline = Pipeline ( name , version , desc = desc ) first_component = components [ 0 ] pipeline . add_component ( first_component ) pipeline . inputs = [( first_component . name , component_input ) for component_input in first_component . inputs ] pipeline . outputs = [( first_component . name , output ) for output in first_component . outputs ] for component in components [ 1 :]: pipeline . add_component ( component ) for ( wire_component , wire_name ) in pipeline . outputs : try : pipeline . add_wiring ( wire_component , wire_name , component . name , wire_name ) except Exception as e : _logger . warning ( f \"Output variable {wire_component}.{wire_name} couldn't be auto-wired. \\n Cause: {e}\" ) unwired_variables = [ f '{component.name}.{x}' for x in component . inputs if not any ( s . endswith ( f '{component.name}.{x}' ) for s in pipeline . wiring )] if len ( unwired_variables ) > 0 : for variable in unwired_variables : _logger . warning ( f \"Input variable {variable} couldn't be auto-wired. \\n \" ) pipeline . outputs = [( component . name , output ) for output in component . outputs ] return pipeline def __repr__ ( self ) -> str : \"\"\" Textual representation of the configured package. The method shows the `Components` with their inputs, outputs and parameters as well as the wiring between these `Components`. Returns: [str]: Textual representation of the package \"\"\" text = f \"[{self.__class__.__name__}] {self.name} ({self.version}) \\n \" if self . desc != \"\" : text += f \"{self.desc} \\n \" if len ( self . parameters ) > 0 : text += \" \\n Pipeline Parameters: \\n \" for name , parameter in self . parameters . items (): text += f \"- {name} ({parameter['type']}, default: '{parameter['defaultValue']}'){(': ' + parameter['desc']) if parameter.get('desc') is not None else ''} \\n \" if len ( self . inputs ) > 0 : text += \" \\n Pipeline Inputs: \\n \" for component , name in self . inputs : input = self . components [ component ] . inputs [ name ] text += f \"> {name} ({input['type']}){': ' + input['desc'] if input.get('desc') is not None else ''} \\n \" if len ( self . outputs ) > 0 : text += \" \\n Pipeline Outputs: \\n \" for component , name in self . outputs : output = self . components [ component ] . outputs [ name ] text += f \"< {name} ({output['type']}){': ' + output['desc'] if output.get('desc') is not None else ''} \\n \" metrics = [( name , metric , component_name ) for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name , metric in component . metrics . items ()] if len ( metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric , _ in metrics : text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . wiring ) > 0 : text += \" \\n I/O Wiring: \\n \" for component , name in self . inputs : text += f \" {name} -> {component}.{name} \\n \" for wire_hash in self . wiring : text += f \" {wire_hash} \\n \" for component , name in self . outputs : text += f \" {component}.{name} -> {name} \\n \" for name , metric , component_name in metrics : text += f \" {component_name}.{name} -> {name} \\n \" if self . periodicity is not None : text += \" \\n Timeshifting: \\n \" text += f \" Periodicity: {self.periodicity} ms \\n \" if len ( self . timeshift_reference ) > 0 : text += \" References: \\n \" for ref in self . timeshift_reference : text += f \" - {ref} \\n \" for component in self . components . values (): text += \" \\n \" + component . __repr__ () return text def add_input ( self , component , variable ): \"\"\" Defines an input variable on the given component as a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" try : _ = self . components [ component ] . inputs [ variable ] except KeyError : raise AssertionError ( \"The component with input variable must exist in the pipeline.\" ) if self . inputs is None : self . inputs = [] if ( component , variable ) in self . inputs : raise AssertionError ( \"The pipeline input already exists.\" ) self . inputs . append (( component , variable )) def delete_input ( self , component : str , variable : str ): \"\"\" Deletes a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . inputs : raise AssertionError ( \"The pipeline input does not exist.\" ) self . inputs . remove (( component , variable )) def add_output ( self , component , variable ): \"\"\" Defines an output variable on the given component as a pipeline output. Args: component (str): Name of the component variable (str): Name of the output variable \"\"\" try : _ = self . components [ component ] . outputs [ variable ] except KeyError : raise AssertionError ( \"The component with output variable must exist in the pipeline.\" ) if self . outputs is None : self . outputs = [] if ( component , variable ) in self . outputs : raise AssertionError ( \"The pipeline output already exists.\" ) self . outputs . append (( component , variable )) def delete_output ( self , component : str , variable : str ): \"\"\" Deletes a pipeline output. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . outputs : raise AssertionError ( \"The pipeline output does not exist.\" ) self . outputs . remove (( component , variable )) def add_component ( self , component : Component ): \"\"\" Adds a `Component` to the pipeline configuration without any connection. The `Component` can be marked as an input or output component of the pipeline. When these parameters are True, the `Component` is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Args: component (Component): `Component` to be added \"\"\" if component . name in self . components : raise AssertionError ( f \"Component with name {component.name} already exists. Please rename the component.\" ) self . components [ component . name ] = component def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ): \"\"\" Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: - The components exist with the given inputs/outputs - The given inputs and outputs are not connected to any wire - The types of the connected input and output are compatible Args: from_component (str): Name of the component which provides data to the `to_component` from_output (str): Name of the output variable of the `from_component` to_component (str): Name of the component which consumes data from the `from_component` to_input (str): Name of the input variable of the `to_component` \"\"\" if from_component not in self . components : raise AssertionError ( f \"No component named '{from_component}'\" ) if to_component not in self . components : raise AssertionError ( f \"No component named '{to_component}'\" ) if from_output not in self . components [ from_component ] . outputs : raise AssertionError ( f \"Component '{from_component}' has no output named '{from_output}'\" ) if to_input not in self . components [ to_component ] . inputs : raise AssertionError ( f \"Component '{to_component}' has no input named '{to_input}'\" ) if self . get_wire_for_input ( to_component , to_input ) is not None : raise AssertionError ( f \"Input '{to_input}' of component '{to_component}' is already wired\" ) _output_type = self . components [ from_component ] . outputs [ from_output ][ \"type\" ] _input_type = self . components [ to_component ] . inputs [ to_input ][ \"type\" ] if _output_type != _input_type : raise AssertionError ( \"Output and input types do not match\" ) wire_hash = self . _wire_hash_string . format ( from_component , from_output , to_component , to_input ) self . wiring [ wire_hash ] = { \"fromComponent\" : from_component , \"fromOutput\" : from_output , \"toComponent\" : to_component , \"toInput\" : to_input , } def get_wire_for_output ( self , component_name : str , output_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data provider through its output with name output_name. Args: component_name (str): Name of the data provider component. output_name (str): Name of the output variable of `component_name`. Returns: [dict]: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"fromComponent\" ] == component_name and x [ \"fromOutput\" ] == output_name ] return wires [ 0 ] if wires else None def get_wire_for_input ( self , component_name : str , input_name : str ): \"\"\" Searches for the wire which connects a component with `component_name` as data consumer through its input with name `input_name`. Args: component_name (str): Name of the data consumer component. input_name (str): Name of the input variable of `component_name`. Returns: dict: Wire which contains the data provider and receiver with their names and the names of their variables. \"\"\" wires = [ x for x in self . wiring . values () if x [ \"toComponent\" ] == component_name and x [ \"toInput\" ] == input_name ] return wires [ 0 ] if wires else None def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ): \"\"\" Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Args: component (str): Name of the component which has the input given the name variable variable (str): Name of the input variable on the component which connected by the wire with_input (bool, optional): If set, the input variable will be also deleted from the component. Defaults to True. Raises: AssertionError: When the variable acts as inter signal alignment reference, it cannot be deleted, and an `AssertionError` will be raised. \"\"\" wire = self . get_wire_for_input ( component , variable ) if wire is None : raise AssertionError ( f \"There is no wiring for input '{variable}' of component '{component}'\" ) if variable in self . timeshift_reference : raise AssertionError ( \"Inter signal alignment reference variables can not be deleted.\" ) wire_hash = self . _wire_hash_string . format ( wire [ 'fromComponent' ], wire [ 'fromOutput' ], wire [ 'toComponent' ], wire [ 'toInput' ]) self . wiring . pop ( wire_hash ) if with_input : self . components [ component ] . delete_input ( variable ) def add_dependencies ( self , packages : list ): \"\"\" @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type `PythonComponent`. This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the `requirements.txt` file when the package is saved. Args: packages (list): List of the necessary python packages to execute the script defined by self.entrypoint \"\"\" python_components = [ self . components [ name ] for name in self . components if type ( self . components [ name ]) is PythonComponent ] for component in python_components : component . add_dependencies ( packages ) def set_timeshifting_periodicity ( self , periodicity : int ): \"\"\" Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, `startingPoint` property is set to `First timestamp`, which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to `Signal reference` by adding inter-signal alignment reference variables via the `add_timeshifting_reference(..)` method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Args: periodicity (int): Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). \"\"\" periodicity = int ( periodicity ) if periodicity not in range ( 10 , int ( math . pow ( 2 , 31 ))): raise AssertionError ( \"Inter signal alignment periodicity must be an integer and in range [10, 2^31)\" ) self . periodicity = periodicity _logger . info ( f \"Inter signal alignment periodicity has been set to {self.periodicity}.\" ) def add_timeshifting_reference ( self , reference : str ): \"\"\" Enables signal alignment mode `Signal reference` by declaring input variables as reference variables. Args: reference (str): Variable name to be added to `self.timeshift_reference` list. \"\"\" if reference not in [ name for _ , name in self . inputs ]: raise AssertionError ( f \"There is no input variable defined with name '{reference}'\" ) if reference in self . timeshift_reference : _logger . warning ( f \"Reference variable with name '{reference}' has been already added.\" ) return self . timeshift_reference . append ( reference ) def remove_timeshifting_reference ( self , reference : str ): \"\"\" Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the `startingPoint` will be `First timestamp`. Args: reference (str): Variable name to be removed from `self.timeshift_reference` list. \"\"\" if reference not in self . timeshift_reference : raise AssertionError ( f \"Reference variable with name {'reference'} does not exist.\" ) self . timeshift_reference . remove ( reference ) def get_pipeline_config ( self ): \"\"\" Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the `destination` folder with name `pipeline_config.yml` \"\"\" if self . save_version is None : self . save_version = self . version if self . save_package_id is None : self . save_package_id = self . package_id pipeline_inputs = [] pipeline_inputs += [{ 'name' : name , 'type' : self . components [ component_name ] . inputs [ name ][ 'type' ] } for component_name , name in self . inputs ] pipeline_outputs = [] pipeline_outputs += [{ 'name' : name , 'type' : self . components [ component_name ] . outputs [ name ][ 'type' ], 'metric' : False , } for component_name , name in self . outputs ] pipeline_outputs += [{ 'name' : name , 'type' : 'String' , 'metric' : True , 'topic' : f \"/siemens/edge/aiinference/{self.name}/{self.save_version}/metrics/{component_name}/{name}\" , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] pipeline_dag = [{ 'source' : f \"{wire['fromComponent']}.{wire['fromOutput']}\" , 'target' : f \"{wire['toComponent']}.{wire['toInput']}\" , } for wire in self . wiring . values ()] pipeline_dag += [{ 'source' : f 'Databus.{name}' , 'target' : f '{component_name}.{name}' , } for component_name , name in self . inputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , name in self . outputs ] pipeline_dag += [{ 'source' : f '{component_name}.{name}' , 'target' : f 'Databus.{name}' , } for component_name , component in self . components . items () if isinstance ( component , PythonComponent ) for name in component . metrics . keys ()] config_yml_content = { 'fileFormatVersion' : '1.2.0' , 'dataFlowPipelineInfo' : { 'author' : self . author , 'createdOn' : datetime . now (), 'dataFlowPipelineVersion' : self . save_version , 'description' : self . desc if self . desc else 'Created by AI SDK' , 'projectName' : self . name , 'packageId' : str ( self . save_package_id ) }, 'dataFlowPipeline' : { 'components' : [ component . _to_dict () for component in self . components . values ()], 'pipelineDag' : pipeline_dag , 'pipelineInputs' : pipeline_inputs , 'pipelineOutputs' : pipeline_outputs , }, 'packageType' : 'full' } if len ( self . parameters . items ()) != 0 : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] = [] for name , parameter in self . parameters . items (): if parameter [ \"topicBased\" ]: config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ], 'topicBased' : parameter [ 'topicBased' ], 'valueTopic' : parameter [ 'valueTopic' ] }) else : config_yml_content [ \"dataFlowPipeline\" ][ \"pipelineParameters\" ] . append ({ 'name' : name , 'type' : parameter [ 'type' ], 'defaultValue' : parameter [ 'defaultValue' ] }) return config_yml_content def save_pipeline_config ( self , destination ): \"\"\" Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the `destination` folder with name `pipeline_config.yml` Args: destination (path-like): Path of the `destination` directory. \"\"\" with open ( Path ( destination ) / PIPELINE_CONFIG , \"w\" ) as f : yaml . dump ( self . get_pipeline_config (), f ) def get_datalink_metadata ( self ): \"\"\" The method generates metadata information based on available information. Returns: dict: Dictionary with the necessary information for the AI Inference Server. \"\"\" timeshifting = { \"id\" : None , \"enabled\" : False , \"periodicity\" : self . periodicity , \"startingPoint\" : None , } if self . periodicity is not None : timeshifting [ \"enabled\" ] = True timeshifting [ \"startingPoint\" ] = 'First timestamp' if len ( self . timeshift_reference ) > 0 : timeshifting [ \"startingPoint\" ] = 'Signal reference' exported_metadata = { \"fileFormatVersion\" : \"1.0.0\" , \"id\" : None , \"version\" : None , \"createdOn\" : datetime . now (), \"updatedOn\" : datetime . now (), \"timeShifting\" : timeshifting , \"inputs\" : [ { 'name' : _name , 'mapping' : None , 'timeShiftingReference' : _name in self . timeshift_reference , 'type' : self . components [ _component ] . inputs [ _name ][ 'type' ] } for _component , _name in self . inputs ] } return exported_metadata def save_datalink_metadata ( self , destination ): \"\"\" Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the `destination` folder with the name `datalink_metadata.yml` Args: destination (path-like): Path of the destination directory. \"\"\" with open ( Path ( destination ) / DATALINK_METADATA , \"w\" ) as f : yaml . dump ( self . get_datalink_metadata (), f ) def save_telemetry_data ( self , destination : Path ): \"\"\" Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None \"\"\" telemetry_path = destination / TELEMETRY_YAML telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"simaticai package not found\" ) try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : _logger . debug ( \"vep-template-sdk package not found\" ) telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ] . python_version for component in self . components if isinstance ( self . components [ component ], PythonComponent ))) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = list ( set ( f . suffix for f in Path ( destination ) . rglob ( \"*\" ) if f . suffix not in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ])) yaml . dump ( telemetry_data , open ( telemetry_path , 'w' )) def save_readme_html ( self , destination ): \"\"\" Saves a `README.html` in the `destination` folder that describes the pipeline. Args: destination (path-like): Path of the destination folder. \"\"\" pipelinePage = _PipelinePage ( self ) readme_html_path = Path ( destination ) / README_HTML readme_html_path . write_text ( pipelinePage . __str__ ()) def validate ( self , destination = \".\" ): \"\"\" Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: - If the package has at least one component - If all wires create connections between existing components and their variables - If metadata is defined and valid. - If a package with the same name already exists in the `destination` folder. In this case a warning message appears and the `save(..)` method overwrites the existing package. - If the package has multiple components and if they are using the same Python version Args: destination (str, optional): Path of the expected destination folder. Defaults to \".\". \"\"\" if len ( self . components ) < 1 : raise AssertionError ( \"The package must have at least one component.\" ) for name , variable in self . outputs : if self . components [ name ] . batch . outputBatch : raise AssertionError ( f \"The component '{name}' has pipeline output defined with variable name '{variable}'. \\ None of component with pipeline output is allowed to provide batch output . \") for wire_hash in self . wiring . copy (): wire = self . wiring [ wire_hash ] self . _check_wiring ( wire , wire_hash ) pipeline_inputs = [ variable for _ , variable in self . inputs ] pipeline_outputs = [ variable for _ , variable in self . outputs ] if any ( variable in pipeline_outputs for variable in pipeline_inputs ): conflicts = set ( pipeline_inputs ) . intersection ( set ( pipeline_outputs )) raise AssertionError ( f \"Pipeline input and output variables must be unique. Conflicting variables: {conflicts}\" ) self . _check_timeshifting () package_path = Path ( destination ) / f \"{self.name}_{self.version}\" . replace ( \" \" , \"-\" ) if package_path . is_dir (): _logger . warning ( f \"Target folder ({package_path}) already exists! Unless changing the package name the package could be invalid and your files will be overwritten!\" ) python_versions = set () for component in self . components : self . components [ component ] . validate () if isinstance ( self . components [ component ], PythonComponent ): python_versions . add ( self . components [ component ] . python_version ) if ( 1 < len ( python_versions )): _logger . warning ( \"The use of multiple python version in a single pipeline is not recommended. We recommend using only one of the supported versions, which are Python 3.10 or 3.11.\" ) _logger . info ( f \"Package '{self.name}' is valid and ready to save.\" ) def _check_timeshifting ( self ): if len ( self . timeshift_reference ) > 0 and self . periodicity is None : raise AssertionError ( \"When using inter signal alignment reference variables, the periodicity must be set.\" ) def _check_wiring ( self , wire , wire_hash ): error_messages = [] if wire [ 'fromComponent' ] not in self . components : error_messages . append ( f \"From component {wire['fromComponent']} does not exist\" ) if wire [ 'toComponent' ] not in self . components : error_messages . append ( f \"To component {wire['toComponent']} does not exist\" ) if wire [ 'fromOutput' ] not in self . components [ wire [ 'fromComponent' ]] . outputs : error_messages . append ( f \"Output variable {wire['fromOutput']} does not exist on component {wire['fromComponent']}\" ) if wire [ 'toInput' ] not in self . components [ wire [ 'toComponent' ]] . inputs : error_messages . append ( f \"Input variable {wire['toInput']} does not exist on component {wire['toComponent']}\" ) if len ( error_messages ) == 0 : from_type_ = self . components [ wire [ 'fromComponent' ]] . outputs [ wire [ 'fromOutput' ]][ 'type' ] to_type_ = self . components [ wire [ 'toComponent' ]] . inputs [ wire [ 'toInput' ]][ 'type' ] if from_type_ != to_type_ : error_messages . append ( f \"The types of input and output variables does not match for wiring {wire_hash}.\" ) if len ( error_messages ) > 0 : self . wiring . pop ( wire_hash ) error_messages . append ( \"The wire has been deleted, please check the variables and re-create the connection.\" ) raise AssertionError ( error_messages . __str__ ()) def save ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as `{package_name}_{package_version}.zip`. If a file with such a name already exists in the `destination` folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name `{package_name}_{package_version}`. If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: - Package folder with name `{package_name}_{package_version}` - `datalink-metadata.yml` - `pipeline-config.yml` - Component folder with name `{component_name}` When the component is a `PythonComponent`, this folder contains: - `requirements.txt` - Entrypoint script defined by the entrypoint of the component - Extra files as added to the specified folders - Source folder with name `src` with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the `destination` folder, an error is raised. Args: destination (str, optional): Target directory for saving the package. Defaults to \".\". package_id (UUID): The optional package ID. If None, a new UUID is generated. \"\"\" self . validate ( destination ) destination = Path ( destination ) if package_id is not None and not isinstance ( package_id , uuid . UUID ): package_id = uuid . UUID ( package_id ) prev_id = None prev_id_version = None prev_version , prev_with_id = self . _find_previous ( destination , package_id ) if prev_with_id is not None : prev_id_version , prev_id = prev_with_id if package_id == prev_id and ( version or self . version ) == str ( prev_id_version ): raise RuntimeError ( f \"package with version '{version or self.version}' and id '{package_id}' already exists\" ) self . save_version = version if version is not None \\ else self . version if self . version is not None \\ else \"1\" if package_id is not None and prev_id is not None and package_id != prev_id \\ else str ( prev_id_version + 1 ) if prev_id_version is not None \\ else str ( prev_version + 1 ) if prev_version is not None and prev_id is None \\ else \"1\" self . save_package_id = package_id if package_id is not None \\ else prev_id if prev_id is not None \\ else uuid . uuid4 () name = self . name . replace ( \" \" , \"-\" ) package_name = f \"{name}_{self.save_version}\" package_file = destination / f \"{package_name}.zip\" specified_version_is_decimal = version is not None and version . isdecimal () or self . version is not None and self . version . isdecimal () if specified_version_is_decimal : if package_file . exists (): p_id = self . _extract_package_id ( package_file , False ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{package_file}'\" ) edge_package_file = destination / f \"{name}-edge_{self.save_version}.zip\" if edge_package_file . exists (): p_id = self . _extract_package_id ( edge_package_file , True ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{edge_package_file}'\" ) destination = destination / package_name destination . mkdir ( parents = True , exist_ok = True ) # Save for component in self . components : self . components [ component ] . save ( destination , False ) if isinstance ( self . components [ component ], PythonComponent ): self . report_writer . add_direct_dependencies ( self . components [ component ] . name , self . components [ component ] . python_dependencies . dependencies ) self . save_datalink_metadata ( destination ) self . save_pipeline_config ( destination ) self . save_readme_html ( destination ) self . save_telemetry_data ( destination ) zip_destination = shutil . make_archive ( base_name = str ( destination . parent / package_name ), format = 'zip' , root_dir = destination . parent , base_dir = package_name , verbose = True , logger = _logger ) pipeline_size = os . path . getsize ( zip_destination ) # zipped package size in bytes pipeline_size_GB = \"{:.2f}\" . format ( pipeline_size / 1000 / 1000 / 1000 ) pipeline_size_limit_GB = \"{:.2f}\" . format ( PIPELINE_SIZE_LIMIT / 1000 / 1000 / 1000 ) if pipeline_size > PIPELINE_SIZE_LIMIT : error_msg = f \"Pipeline size {pipeline_size} bytes ({pipeline_size_GB} GB) exceeds the limit of \" \\ f \"{PIPELINE_SIZE_LIMIT} bytes ({pipeline_size_limit_GB} GB). \" \\ \"Please remove unnecessary files and dependencies and try again.\" _logger . error ( error_msg ) raise RuntimeError ( error_msg ) return Path ( zip_destination ) # TODO: refactor the business logic in PBI 1662648 def _find_previous ( self , destination : Path , package_id : Optional [ uuid . UUID ] = None ) -> Tuple [ Optional [ int ], Optional [ Tuple [ int , uuid . UUID ]]]: latest_version = None latest_with_id = None if Path ( destination ) . is_dir () is False : return None , None for file in destination . glob ( f \"{self.name.replace(' ', '-')}*.zip\" ): zip_version , zip_package_id = self . _extract_package_info ( file ) if not zip_version . isdecimal (): continue zip_version = int ( zip_version ) if zip_package_id is None : # package id in the zip package not present if latest_version is None or zip_version > latest_version : latest_version = zip_version elif package_id is None : if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) else : if package_id != zip_package_id : continue if latest_with_id is None or zip_version > latest_with_id [ 0 ]: latest_with_id = ( zip_version , zip_package_id ) return latest_version , latest_with_id def _extract_package_info ( self , zip_path : Path ) -> Tuple : with zipfile . ZipFile ( zip_path ) as zip_file : config_path = next ( f for f in zip_file . namelist () if f . endswith ( \"pipeline_config.yml\" )) with zip_file . open ( config_path ) as config_file : config = yaml . load ( config_file , Loader = yaml . SafeLoader ) pipeline_info = config . get ( \"dataFlowPipelineInfo\" , {}) version = pipeline_info . get ( \"dataFlowPipelineVersion\" , None ) package_id = pipeline_info . get ( \"packageId\" , None ) package_id = uuid . UUID ( package_id ) if package_id is not None else None return version , package_id def _extract_package_id ( self , package_file : Path , is_edge_package : bool ) -> Optional [ uuid . UUID ]: try : with OpenZipInTemp ( package_file ) as package_dir : if not is_edge_package : package_dir = package_dir / package_file . stem with open ( package_dir / 'pipeline_config.yml' ) as config_file : return uuid . UUID ( yaml . load ( config_file , Loader = yaml . SafeLoader )[ \"dataFlowPipelineInfo\" ][ \"packageId\" ]) except Exception : _logger . debug ( f \"Could not extract Package ID from '{package_file}'\" ) return None def export ( self , destination = \".\" , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" Export a runnable pipeline package. Args: destination (str): optional target directory for saving the package. Defaults to \".\". package_id (UUID): optional package ID. If None, a new UUID is generated. version (str): optional version. If None, an automatic version number is generated. \"\"\" config_package = None try : config_package = self . save ( destination , package_id , version ) runtime_package = convert_package ( config_package , self . report_writer ) return runtime_package finally : if config_package is not None : Path ( config_package ) . unlink ( missing_ok = True ) def add_parameter ( self , name , default_value , type_name : str = \"String\" , topic_based : bool = False , desc : str = None ): \"\"\" Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Args: name (str): Name of the parameter desc (str): Description of the parameter (optional) type_name (str, optional): Data type of the parameter. Defaults to \"String\". default_value (str): Default value of the parameter topic_based (bool, optional): If true, the parameter can be updated from a message queue. Raises: ValueError: When: - the default value of the parameter is not of the specified data type (`type_name`) or - the specified data type itself is not an allowed data type (not a part of `parameter_types` dict) or - the specified data type is not given in the right format or - the type of the given `topic_based` parameter is not `bool`. \"\"\" parameter_types = { \"String\" : 'str' , \"Integer\" : 'int' , \"Double\" : 'float' , \"Boolean\" : 'bool' } default_value_type = type ( default_value ) . __name__ if type_name not in parameter_types . keys (): raise ValueError ( f \"The given value type is not supported. Please use one of these: {parameter_types.keys()}\" ) if default_value_type != parameter_types [ type_name ]: raise ValueError ( f \"The given value type does not match the type of '{type_name}'. Please use the correct one from these: {list(parameter_types.keys())}\" ) if not isinstance ( topic_based , bool ): raise ValueError ( \"Type of the given `topic_based` parameter is not `bool`.\" ) self . parameters [ name ] = { \"name\" : name , \"type\" : type_name , \"defaultValue\" : default_value , \"topicBased\" : topic_based , \"valueTopic\" : None } if desc is not None : self . parameters [ name ][ \"desc\" ] = desc","title":"Attributes"},{"location":"reference/simaticai/deployment.html#static-methods","text":"","title":"Static methods"},{"location":"reference/simaticai/deployment.html#from_components","text":"def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = '' ) -> 'Pipeline' Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Parameters: Name Type Description Default components list List of PythonComponents None name str Name of the pipeline None version str Version information of the pipeline. (Optional) None Returns: Type Description Pipeline Pipeline object with the auto-wired components View Source @staticmethod def from_components ( components : list , name : str , version : Optional [ str ] = None , desc : str = \"\" ) -> \"Pipeline\" : \"\"\" Creates a pipeline configuration from the given components. The components are linked in a linear sequence with inputs and outputs auto-wired based on the name of the inputs and outputs of the components. The inputs of the first component will be wired as the pipeline inputs and the outputs of the last component will be wired as the pipeline outputs. The components must have unique names. Two or more versions of the same component can not be packaged simultaneously without renaming them. Args: components (list): List of PythonComponents name (str): Name of the pipeline version (str): Version information of the pipeline. (Optional) Returns: Pipeline: Pipeline object with the auto-wired components \"\"\" pipeline = Pipeline ( name , version , desc = desc ) first_component = components [ 0 ] pipeline . add_component ( first_component ) pipeline . inputs = [ (first_component.name, component_input) for component_input in first_component.inputs ] pipeline . outputs = [ (first_component.name, output) for output in first_component.outputs ] for component in components [ 1: ] : pipeline . add_component ( component ) for ( wire_component , wire_name ) in pipeline . outputs : try : pipeline . add_wiring ( wire_component , wire_name , component . name , wire_name ) except Exception as e : _logger . warning ( f \"Output variable {wire_component}.{wire_name} couldn't be auto-wired.\\nCause: {e}\" ) unwired_variables = [ f'{component.name}.{x}' for x in component.inputs if not any(s.endswith(f'{component.name}.{x}') for s in pipeline.wiring) ] if len ( unwired_variables ) > 0 : for variable in unwired_variables : _logger . warning ( f \"Input variable {variable} couldn't be auto-wired.\\n\" ) pipeline . outputs = [ (component.name, output) for output in component.outputs ] return pipeline","title":"from_components"},{"location":"reference/simaticai/deployment.html#methods_2","text":"","title":"Methods"},{"location":"reference/simaticai/deployment.html#add_component","text":"def add_component ( self , component : simaticai . deployment . Component ) Adds a Component to the pipeline configuration without any connection. The Component can be marked as an input or output component of the pipeline. When these parameters are True, the Component is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Parameters: Name Type Description Default component Component Component to be added None View Source def add_component ( self , component : Component ) : \" \"\" Adds a `Component` to the pipeline configuration without any connection. The `Component` can be marked as an input or output component of the pipeline. When these parameters are True, the `Component` is responsible for input or output data of the pipeline. The component must have a unique name. Two or more versions of the same component can not be added to the same pipeline with the same component name. Args: component (Component): `Component` to be added \"\" \" if component . name in self . components : raise AssertionError ( f \"Component with name {component.name} already exists. Please rename the component.\" ) self . components [ component . name ] = component","title":"add_component"},{"location":"reference/simaticai/deployment.html#add_dependencies","text":"def add_dependencies ( self , packages : list ) @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type PythonComponent . This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the requirements.txt file when the package is saved. Parameters: Name Type Description Default packages list List of the necessary python packages to execute the script defined by self.entrypoint None View Source def add_dependencies ( self , packages : list ) : \"\"\" @Deprecated, reason: components can have different Python versions and/or platform, therefore it's better to specify dependencies on a case-by-case basis. Collects the given Python packages with their versions from the executing Python environment and add them to all components of type `PythonComponent`. This step is necessary in order to execute the pipeline configuration on the Edge side. The method can be called multiple times but each time the previously-collected dependencies are cleared. The reason for this is to ensure a consistent dependency list for the `requirements.txt` file when the package is saved. Args: packages (list): List of the necessary python packages to execute the script defined by self.entrypoint \"\"\" python_components = [ self.components[name ] for name in self . components if type ( self . components [ name ] ) is PythonComponent ] for component in python_components : component . add_dependencies ( packages )","title":"add_dependencies"},{"location":"reference/simaticai/deployment.html#add_input_2","text":"def add_input ( self , component , variable ) Defines an input variable on the given component as a pipeline input. Parameters: Name Type Description Default component str Name of the component None variable str Name of the input variable None View Source def add_input ( self , component , variable ) : \"\"\" Defines an input variable on the given component as a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" try : _ = self . components [ component ] . inputs [ variable ] except KeyError : raise AssertionError ( \"The component with input variable must exist in the pipeline.\" ) if self . inputs is None : self . inputs = [] if ( component , variable ) in self . inputs : raise AssertionError ( \"The pipeline input already exists.\" ) self . inputs . append (( component , variable ))","title":"add_input"},{"location":"reference/simaticai/deployment.html#add_output_2","text":"def add_output ( self , component , variable ) Defines an output variable on the given component as a pipeline output. Parameters: Name Type Description Default component str Name of the component None variable str Name of the output variable None View Source def add_output ( self , component , variable ) : \"\"\" Defines an output variable on the given component as a pipeline output. Args: component (str): Name of the component variable (str): Name of the output variable \"\"\" try : _ = self . components [ component ] . outputs [ variable ] except KeyError : raise AssertionError ( \"The component with output variable must exist in the pipeline.\" ) if self . outputs is None : self . outputs = [] if ( component , variable ) in self . outputs : raise AssertionError ( \"The pipeline output already exists.\" ) self . outputs . append (( component , variable ))","title":"add_output"},{"location":"reference/simaticai/deployment.html#add_parameter","text":"def add_parameter ( self , name , default_value , type_name : str = 'String' , topic_based : bool = False , desc : str = None ) Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Parameters: Name Type Description Default name str Name of the parameter None desc str Description of the parameter (optional) None type_name str Data type of the parameter. Defaults to \"String\". \"String\" default_value str Default value of the parameter None topic_based bool If true, the parameter can be updated from a message queue. None Raises: Type Description ValueError When: - the default value of the parameter is not of the specified data type ( type_name ) or - the specified data type itself is not an allowed data type (not a part of parameter_types dict) or - the specified data type is not given in the right format or - the type of the given topic_based parameter is not bool . View Source def add_parameter ( self , name , default_value , type_name : str = \"String\" , topic_based : bool = False , desc : str = None ) : \" \"\" Adds a parameter to the pipeline configuration, which alters the behavior of the pipeline. The parameter's default value and its properties are saved in the pipeline configuration and the value of the parameter can later be changed on AI Inference Server. Args: name (str): Name of the parameter desc (str): Description of the parameter (optional) type_name (str, optional): Data type of the parameter. Defaults to \" String \". default_value (str): Default value of the parameter topic_based (bool, optional): If true, the parameter can be updated from a message queue. Raises: ValueError: When: - the default value of the parameter is not of the specified data type (`type_name`) or - the specified data type itself is not an allowed data type (not a part of `parameter_types` dict) or - the specified data type is not given in the right format or - the type of the given `topic_based` parameter is not `bool`. \"\" \" parameter_types = { \"String\" : 'str' , \"Integer\" : 'int' , \"Double\" : 'float' , \"Boolean\" : 'bool' } default_value_type = type ( default_value ). __name__ if type_name not in parameter_types . keys () : raise ValueError ( f \"The given value type is not supported. Please use one of these: {parameter_types.keys()}\" ) if default_value_type != parameter_types [ type_name ] : raise ValueError ( f \"The given value type does not match the type of '{type_name}'. Please use the correct one from these: {list(parameter_types.keys())}\" ) if not isinstance ( topic_based , bool ) : raise ValueError ( \"Type of the given `topic_based` parameter is not `bool`.\" ) self . parameters [ name ] = { \"name\" : name , \"type\" : type_name , \"defaultValue\" : default_value , \"topicBased\" : topic_based , \"valueTopic\" : None } if desc is not None : self . parameters [ name ][ \"desc\" ] = desc","title":"add_parameter"},{"location":"reference/simaticai/deployment.html#add_timeshifting_reference","text":"def add_timeshifting_reference ( self , reference : str ) Enables signal alignment mode Signal reference by declaring input variables as reference variables. Parameters: Name Type Description Default reference str Variable name to be added to self.timeshift_reference list. None View Source def add_timeshifting_reference ( self , reference : str ): \"\"\" Enables signal alignment mode `Signal reference` by declaring input variables as reference variables. Args: reference (str): Variable name to be added to `self.timeshift_reference` list. \"\"\" if reference not in [ name for _ , name in self . inputs ]: raise AssertionError ( f \"There is no input variable defined with name '{reference}'\" ) if reference in self . timeshift_reference : _logger . warning ( f \"Reference variable with name '{reference}' has been already added.\" ) return self . timeshift_reference . append ( reference )","title":"add_timeshifting_reference"},{"location":"reference/simaticai/deployment.html#add_wiring","text":"def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ) Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: The components exist with the given inputs/outputs The given inputs and outputs are not connected to any wire The types of the connected input and output are compatible Parameters: Name Type Description Default from_component str Name of the component which provides data to the to_component None from_output str Name of the output variable of the from_component None to_component str Name of the component which consumes data from the from_component None to_input str Name of the input variable of the to_component None View Source def add_wiring ( self , from_component : str , from_output : str , to_component : str , to_input : str ): \"\"\" Creates a one-to-one connection between the input and output of two components. The method checks if the connection is allowed with the following requirements: - The components exist with the given inputs/outputs - The given inputs and outputs are not connected to any wire - The types of the connected input and output are compatible Args: from_component (str): Name of the component which provides data to the `to_component` from_output (str): Name of the output variable of the `from_component` to_component (str): Name of the component which consumes data from the `from_component` to_input (str): Name of the input variable of the `to_component` \"\"\" if from_component not in self . components : raise AssertionError ( f \"No component named '{from_component}'\" ) if to_component not in self . components : raise AssertionError ( f \"No component named '{to_component}'\" ) if from_output not in self . components [ from_component ] . outputs : raise AssertionError ( f \"Component '{from_component}' has no output named '{from_output}'\" ) if to_input not in self . components [ to_component ] . inputs : raise AssertionError ( f \"Component '{to_component}' has no input named '{to_input}'\" ) if self . get_wire_for_input ( to_component , to_input ) is not None : raise AssertionError ( f \"Input '{to_input}' of component '{to_component}' is already wired\" ) _output_type = self . components [ from_component ] . outputs [ from_output ][ \"type\" ] _input_type = self . components [ to_component ] . inputs [ to_input ][ \"type\" ] if _output_type != _input_type : raise AssertionError ( \"Output and input types do not match\" ) wire_hash = self . _wire_hash_string . format ( from_component , from_output , to_component , to_input ) self . wiring [ wire_hash ] = { \"fromComponent\" : from_component , \"fromOutput\" : from_output , \"toComponent\" : to_component , \"toInput\" : to_input , }","title":"add_wiring"},{"location":"reference/simaticai/deployment.html#delete_input_2","text":"def delete_input ( self , component : str , variable : str ) Deletes a pipeline input. Parameters: Name Type Description Default component str Name of the component None variable str Name of the input variable None View Source def delete_input ( self , component : str , variable : str ): \"\"\" Deletes a pipeline input. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . inputs : raise AssertionError ( \"The pipeline input does not exist.\" ) self . inputs . remove (( component , variable ))","title":"delete_input"},{"location":"reference/simaticai/deployment.html#delete_input_wire","text":"def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ) Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Parameters: Name Type Description Default component str Name of the component which has the input given the name variable None variable str Name of the input variable on the component which connected by the wire None with_input bool If set, the input variable will be also deleted from the component. Defaults to True. True Raises: Type Description AssertionError When the variable acts as inter signal alignment reference, it cannot be deleted, and an AssertionError will be raised. View Source def delete_input_wire ( self , component : str , variable : str , with_input : bool = True ): \"\"\" Deletes an existing connection between two components. The connection must be given with the name of the consumer component and its input variable. If an inter signal alignment reference variable is affected it cannot be deleted. By default, the input variable will be also deleted. Args: component (str): Name of the component which has the input given the name variable variable (str): Name of the input variable on the component which connected by the wire with_input (bool, optional): If set, the input variable will be also deleted from the component. Defaults to True. Raises: AssertionError: When the variable acts as inter signal alignment reference, it cannot be deleted, and an `AssertionError` will be raised. \"\"\" wire = self . get_wire_for_input ( component , variable ) if wire is None : raise AssertionError ( f \"There is no wiring for input '{variable}' of component '{component}'\" ) if variable in self . timeshift_reference : raise AssertionError ( \"Inter signal alignment reference variables can not be deleted.\" ) wire_hash = self . _wire_hash_string . format ( wire [ 'fromComponent' ], wire [ 'fromOutput' ], wire [ 'toComponent' ], wire [ 'toInput' ]) self . wiring . pop ( wire_hash ) if with_input : self . components [ component ] . delete_input ( variable )","title":"delete_input_wire"},{"location":"reference/simaticai/deployment.html#delete_output_2","text":"def delete_output ( self , component : str , variable : str ) Deletes a pipeline output. Parameters: Name Type Description Default component str Name of the component None variable str Name of the input variable None View Source def delete_output ( self , component : str , variable : str ): \"\"\" Deletes a pipeline output. Args: component (str): Name of the component variable (str): Name of the input variable \"\"\" if ( component , variable ) not in self . outputs : raise AssertionError ( \"The pipeline output does not exist.\" ) self . outputs . remove (( component , variable ))","title":"delete_output"},{"location":"reference/simaticai/deployment.html#export","text":"def export ( self , destination = '.' , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> pathlib . Path Export a runnable pipeline package. Parameters: Name Type Description Default destination str optional target directory for saving the package. Defaults to \".\". \".\" package_id UUID optional package ID. If None, a new UUID is generated. None version str optional version. If None, an automatic version number is generated. None View Source def export ( self , destination = \".\" , package_id : Optional [ uuid.UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" Export a runnable pipeline package. Args: destination (str): optional target directory for saving the package. Defaults to \" . \". package_id (UUID): optional package ID. If None, a new UUID is generated. version (str): optional version. If None, an automatic version number is generated. \"\"\" config_package = None try : config_package = self . save ( destination , package_id , version ) runtime_package = convert_package ( config_package , self . report_writer ) return runtime_package finally : if config_package is not None : Path ( config_package ). unlink ( missing_ok = True )","title":"export"},{"location":"reference/simaticai/deployment.html#get_datalink_metadata","text":"def get_datalink_metadata ( self ) The method generates metadata information based on available information. Returns: Type Description dict Dictionary with the necessary information for the AI Inference Server. View Source def get_datalink_metadata ( self ) : \"\"\" The method generates metadata information based on available information. Returns: dict: Dictionary with the necessary information for the AI Inference Server. \"\"\" timeshifting = { \"id\" : None , \"enabled\" : False , \"periodicity\" : self . periodicity , \"startingPoint\" : None , } if self . periodicity is not None : timeshifting [ \"enabled\" ] = True timeshifting [ \"startingPoint\" ] = 'First timestamp' if len ( self . timeshift_reference ) > 0 : timeshifting [ \"startingPoint\" ] = 'Signal reference' exported_metadata = { \"fileFormatVersion\" : \"1.0.0\" , \"id\" : None , \"version\" : None , \"createdOn\" : datetime . now (), \"updatedOn\" : datetime . now (), \"timeShifting\" : timeshifting , \"inputs\" : [ { 'name': _name, 'mapping': None, 'timeShiftingReference': _name in self.timeshift_reference, 'type': self.components[_component ] . inputs [ _name ][ 'type' ] } for _component , _name in self . inputs ] } return exported_metadata","title":"get_datalink_metadata"},{"location":"reference/simaticai/deployment.html#get_pipeline_config","text":"def get_pipeline_config ( self ) Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the destination folder with name pipeline_config.yml View Source def get_pipeline_config ( self ) : \"\"\" Saves the information on the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for the AI Inference Server. The file is created in the `destination` folder with name `pipeline_config.yml` \"\"\" if self . save_version is None : self . save_version = self . version if self . save_package_id is None : self . save_package_id = self . package_id pipeline_inputs = [] pipeline_inputs += [ { 'name': name, 'type': self.components[component_name ] . inputs [ name ][ 'type' ] } for component_name , name in self . inputs ] pipeline_outputs = [] pipeline_outputs += [ { 'name': name, 'type': self.components[component_name ] . outputs [ name ][ 'type' ] , 'metric' : False , } for component_name , name in self . outputs ] pipeline_outputs += [ { 'name': name, 'type': 'String', 'metric': True, 'topic': f\"/siemens/edge/aiinference/{self.name}/{self.save_version}/metrics/{component_name}/{name}\", } for component_name, component in self.components.items() if isinstance(component, PythonComponent) for name in component.metrics.keys() ] pipeline_dag = [ { 'source': f\"{wire['fromComponent' ] } . { wire [ 'fromOutput' ] } \", 'target': f\" { wire [ 'toComponent' ] } . { wire [ 'toInput' ] } \", } for wire in self.wiring.values()] pipeline_dag += [{ 'source': f'Databus.{name}', 'target': f'{component_name}.{name}', } for component_name, name in self.inputs] pipeline_dag += [{ 'source': f'{component_name}.{name}', 'target': f'Databus.{name}', } for component_name, name in self.outputs] pipeline_dag += [{ 'source': f'{component_name}.{name}', 'target': f'Databus.{name}', } for component_name, component in self.components.items() if isinstance(component, PythonComponent) for name in component.metrics.keys()] config_yml_content = { 'fileFormatVersion': '1.2.0', 'dataFlowPipelineInfo': { 'author': self.author, 'createdOn': datetime.now(), 'dataFlowPipelineVersion': self.save_version, 'description': self.desc if self.desc else 'Created by AI SDK', 'projectName': self.name, 'packageId': str(self.save_package_id) }, 'dataFlowPipeline': { 'components': [component._to_dict() for component in self.components.values()], 'pipelineDag': pipeline_dag, 'pipelineInputs': pipeline_inputs, 'pipelineOutputs': pipeline_outputs, }, 'packageType': 'full' } if len(self.parameters.items()) != 0: config_yml_content[\" dataFlowPipeline \"][\" pipelineParameters \"] = [] for name, parameter in self.parameters.items(): if parameter[\" topicBased \"]: config_yml_content[\" dataFlowPipeline \"][\" pipelineParameters \"].append({ 'name': name, 'type': parameter['type'], 'defaultValue': parameter['defaultValue'], 'topicBased': parameter['topicBased'], 'valueTopic': parameter['valueTopic'] }) else: config_yml_content[\" dataFlowPipeline \"][\" pipelineParameters \"] . append ( { 'name' : name , 'type' : parameter [ 'type' ] , 'defaultValue' : parameter [ 'defaultValue' ] } ) return config_yml_content","title":"get_pipeline_config"},{"location":"reference/simaticai/deployment.html#get_wire_for_input","text":"def get_wire_for_input ( self , component_name : str , input_name : str ) Searches for the wire which connects a component with component_name as data consumer through its input with name input_name . Parameters: Name Type Description Default component_name str Name of the data consumer component. None input_name str Name of the input variable of component_name . None Returns: Type Description dict Wire which contains the data provider and receiver with their names and the names of their variables. View Source def get_wire_for_input ( self , component_name : str , input_name : str ) : \" \"\" Searches for the wire which connects a component with `component_name` as data consumer through its input with name `input_name`. Args: component_name (str): Name of the data consumer component. input_name (str): Name of the input variable of `component_name`. Returns: dict: Wire which contains the data provider and receiver with their names and the names of their variables. \"\" \" wires = [ x for x in self . wiring . values () if x [ \"toComponent\" ] == component_name and x [ \"toInput\" ] == input_name ] return wires [ 0 ] if wires else None","title":"get_wire_for_input"},{"location":"reference/simaticai/deployment.html#get_wire_for_output","text":"def get_wire_for_output ( self , component_name : str , output_name : str ) Searches for the wire which connects a component with component_name as data provider through its output with name output_name. Parameters: Name Type Description Default component_name str Name of the data provider component. None output_name str Name of the output variable of component_name . None Returns: Type Description [dict] Wire which contains the data provider and receiver with their names and the names of their variables. View Source def get_wire_for_output ( self , component_name : str , output_name : str ) : \" \"\" Searches for the wire which connects a component with `component_name` as data provider through its output with name output_name. Args: component_name (str): Name of the data provider component. output_name (str): Name of the output variable of `component_name`. Returns: [dict]: Wire which contains the data provider and receiver with their names and the names of their variables. \"\" \" wires = [ x for x in self . wiring . values () if x [ \"fromComponent\" ] == component_name and x [ \"fromOutput\" ] == output_name ] return wires [ 0 ] if wires else None","title":"get_wire_for_output"},{"location":"reference/simaticai/deployment.html#remove_timeshifting_reference","text":"def remove_timeshifting_reference ( self , reference : str ) Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the startingPoint will be First timestamp . Parameters: Name Type Description Default reference str Variable name to be removed from self.timeshift_reference list. None View Source def remove_timeshifting_reference ( self , reference : str ) : \" \"\" Removes previously-defined inter-signal alignment reference variables. If no reference variables remain, the `startingPoint` will be `First timestamp`. Args: reference (str): Variable name to be removed from `self.timeshift_reference` list. \"\" \" if reference not in self . timeshift_reference : raise AssertionError ( f \"Reference variable with name {'reference'} does not exist.\" ) self . timeshift_reference . remove ( reference )","title":"remove_timeshifting_reference"},{"location":"reference/simaticai/deployment.html#save_2","text":"def save ( self , destination = '.' , package_id : Optional [ uuid . UUID ] = None , version : Optional [ str ] = None ) -> pathlib . Path @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as {package_name}_{package_version}.zip . If a file with such a name already exists in the destination folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name {package_name}_{package_version} . If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: Package folder with name {package_name}_{package_version} datalink-metadata.yml pipeline-config.yml Component folder with name {component_name} When the component is a PythonComponent , this folder contains: requirements.txt Entrypoint script defined by the entrypoint of the component Extra files as added to the specified folders Source folder with name src with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the destination folder, an error is raised. Parameters: Name Type Description Default destination str Target directory for saving the package. Defaults to \".\". \".\" package_id UUID The optional package ID. If None, a new UUID is generated. None View Source def save ( self , destination = \".\" , package_id : Optional [ uuid.UUID ] = None , version : Optional [ str ] = None ) -> Path : \"\"\" @Deprecated, reason: only edge package generation will be supported in the future. Use export instead. Saves the assembled package in a zip format. The name of the file is defined as `{package_name}_{package_version}.zip`. If a file with such a name already exists in the `destination` folder, it gets overwritten and a warning message appears. The package is also available as a subfolder on the destination path with the name `{package_name}_{package_version}`. If the assembled content does not meet the expected one, this content can be changed and simply packed into a zip file. The package contains files and folders in the following structure: - Package folder with name `{package_name}_{package_version}` - `datalink-metadata.yml` - `pipeline-config.yml` - Component folder with name `{component_name}` When the component is a `PythonComponent`, this folder contains: - `requirements.txt` - Entrypoint script defined by the entrypoint of the component - Extra files as added to the specified folders - Source folder with name `src` with necessary python scripts If a package ID is specified, and a package with the same ID and version is already present in the `destination` folder, an error is raised. Args: destination (str, optional): Target directory for saving the package. Defaults to \" . \". package_id (UUID): The optional package ID. If None, a new UUID is generated. \"\"\" self . validate ( destination ) destination = Path ( destination ) if package_id is not None and not isinstance ( package_id , uuid . UUID ) : package_id = uuid . UUID ( package_id ) prev_id = None prev_id_version = None prev_version , prev_with_id = self . _find_previous ( destination , package_id ) if prev_with_id is not None : prev_id_version , prev_id = prev_with_id if package_id == prev_id and ( version or self . version ) == str ( prev_id_version ) : raise RuntimeError ( f \"package with version '{version or self.version}' and id '{package_id}' already exists\" ) self . save_version = version if version is not None \\ else self . version if self . version is not None \\ else \"1\" if package_id is not None and prev_id is not None and package_id != prev_id \\ else str ( prev_id_version + 1 ) if prev_id_version is not None \\ else str ( prev_version + 1 ) if prev_version is not None and prev_id is None \\ else \"1\" self . save_package_id = package_id if package_id is not None \\ else prev_id if prev_id is not None \\ else uuid . uuid4 () name = self . name . replace ( \" \" , \"-\" ) package_name = f \"{name}_{self.save_version}\" package_file = destination / f \"{package_name}.zip\" specified_version_is_decimal = version is not None and version . isdecimal () or self . version is not None and self . version . isdecimal () if specified_version_is_decimal : if package_file . exists () : p_id = self . _extract_package_id ( package_file , False ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{package_file}'\" ) edge_package_file = destination / f \"{name}-edge_{self.save_version}.zip\" if edge_package_file . exists () : p_id = self . _extract_package_id ( edge_package_file , True ) if p_id is not None and p_id == package_id : raise RuntimeError ( f \"package with version '{self.save_version}' and id '{self.save_package_id}' already exists: '{edge_package_file}'\" ) destination = destination / package_name destination . mkdir ( parents = True , exist_ok = True ) # Save for component in self . components : self . components [ component ] . save ( destination , False ) if isinstance ( self . components [ component ] , PythonComponent ) : self . report_writer . add_direct_dependencies ( self . components [ component ] . name , self . components [ component ] . python_dependencies . dependencies ) self . save_datalink_metadata ( destination ) self . save_pipeline_config ( destination ) self . save_readme_html ( destination ) self . save_telemetry_data ( destination ) zip_destination = shutil . make_archive ( base_name = str ( destination . parent / package_name ), format = 'zip' , root_dir = destination . parent , base_dir = package_name , verbose = True , logger = _logger ) pipeline_size = os . path . getsize ( zip_destination ) # zipped package size in bytes pipeline_size_GB = \"{:.2f}\" . format ( pipeline_size / 1000 / 1000 / 1000 ) pipeline_size_limit_GB = \"{:.2f}\" . format ( PIPELINE_SIZE_LIMIT / 1000 / 1000 / 1000 ) if pipeline_size > PIPELINE_SIZE_LIMIT : error_msg = f \"Pipeline size {pipeline_size} bytes ({pipeline_size_GB} GB) exceeds the limit of \" \\ f \"{PIPELINE_SIZE_LIMIT} bytes ({pipeline_size_limit_GB} GB). \" \\ \"Please remove unnecessary files and dependencies and try again.\" _logger . error ( error_msg ) raise RuntimeError ( error_msg ) return Path ( zip_destination )","title":"save"},{"location":"reference/simaticai/deployment.html#save_datalink_metadata","text":"def save_datalink_metadata ( self , destination ) Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the destination folder with the name datalink_metadata.yml Parameters: Name Type Description Default destination path-like Path of the destination directory. None View Source def save_datalink_metadata ( self , destination ) : \" \"\" Saves metadata for pipeline input variables. This method saves metadata for the AI Inference Server into a YAML file. This metadata determines how the AI Inference Server feeds input to the pipeline, especially inter-signal alignment. The file is created in the `destination` folder with the name `datalink_metadata.yml` Args: destination (path-like): Path of the destination directory. \"\" \" with open ( Path ( destination ) / DATALINK_METADATA , \"w\" ) as f : yaml . dump ( self . get_datalink_metadata (), f )","title":"save_datalink_metadata"},{"location":"reference/simaticai/deployment.html#save_pipeline_config","text":"def save_pipeline_config ( self , destination ) Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the destination folder with name pipeline_config.yml Parameters: Name Type Description Default destination path-like Path of the destination directory. None View Source def save_pipeline_config ( self , destination ) : \" \"\" Saves the information about the composed pipeline configuration package into a YAML file. This YAML file describes the components and the data flow between them for AI Inference Server. The file will be created in the `destination` folder with name `pipeline_config.yml` Args: destination (path-like): Path of the `destination` directory. \"\" \" with open ( Path ( destination ) / PIPELINE_CONFIG , \"w\" ) as f : yaml . dump ( self . get_pipeline_config (), f )","title":"save_pipeline_config"},{"location":"reference/simaticai/deployment.html#save_readme_html","text":"def save_readme_html ( self , destination ) Saves a README.html in the destination folder that describes the pipeline. Parameters: Name Type Description Default destination path-like Path of the destination folder. None View Source def save_readme_html ( self , destination ) : \" \"\" Saves a `README.html` in the `destination` folder that describes the pipeline. Args: destination (path-like): Path of the destination folder. \"\" \" pipelinePage = _PipelinePage ( self ) readme_html_path = Path ( destination ) / README_HTML readme_html_path . write_text ( pipelinePage . __str__ ())","title":"save_readme_html"},{"location":"reference/simaticai/deployment.html#save_telemetry_data","text":"def save_telemetry_data ( self , destination : pathlib . Path ) Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None View Source def save_telemetry_data ( self , destination : Path ) : \"\"\" Save telemetry data to a specified destination. Args: destination (Path): The path where the telemetry data should be saved. Returns: None Raises: None \"\"\" telemetry_path = destination / TELEMETRY_YAML telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\", \"get_ipython\" ] ) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ). version except pkg_resources . DistributionNotFound : _logger . debug ( \"simaticai package not found\" ) try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ). version except pkg_resources . DistributionNotFound : _logger . debug ( \"vep-template-sdk package not found\" ) telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ] . python_version for component in self . components if isinstance ( self . components [ component ] , PythonComponent ))) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = list ( set ( f . suffix for f in Path ( destination ). rglob ( \"*\" ) if f . suffix not in [ \"\", \".zip\", \".yml\", \".yaml\", \".html\" ] )) yaml . dump ( telemetry_data , open ( telemetry_path , 'w' ))","title":"save_telemetry_data"},{"location":"reference/simaticai/deployment.html#set_timeshifting_periodicity","text":"def set_timeshifting_periodicity ( self , periodicity : int ) Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, startingPoint property is set to First timestamp , which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to Signal reference by adding inter-signal alignment reference variables via the add_timeshifting_reference(..) method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Parameters: Name Type Description Default periodicity int Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). None View Source def set _timeshifting_periodicity ( self , periodicity : int ) : \" \"\" Enables inter-signal alignment with the given sampling period. With inter-signal alignment enabled, the AI Inference Server collects data for different input variables before it triggers the model. By default, `startingPoint` property is set to `First timestamp`, which means that inter-signal alignment is started at the first incoming value for any input variable. This property can be changed to `Signal reference` by adding inter-signal alignment reference variables via the `add_timeshifting_reference(..)` method. In this case, inter-signal alignment is started when the first value arrives for the defined input variables. Args: periodicity (int): Periodicity time in milliseconds for the AI Inference Server to perform inter-signal alignment. Valid range is [10, 2^31). \"\" \" periodicity = int ( periodicity ) if periodicity not in range ( 10 , int ( math . pow ( 2 , 31 ))) : raise AssertionError ( \"Inter signal alignment periodicity must be an integer and in range [10, 2^31)\" ) self . periodicity = periodicity _logger . info ( f \"Inter signal alignment periodicity has been set to {self.periodicity}.\" )","title":"set_timeshifting_periodicity"},{"location":"reference/simaticai/deployment.html#validate_2","text":"def validate ( self , destination = '.' ) Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: If the package has at least one component If all wires create connections between existing components and their variables If metadata is defined and valid. If a package with the same name already exists in the destination folder. In this case a warning message appears and the save(..) method overwrites the existing package. If the package has multiple components and if they are using the same Python version Parameters: Name Type Description Default destination str Path of the expected destination folder. Defaults to \".\". \".\" View Source def validate ( self , destination = \".\" ) : \"\"\" Validates whether the package configuration is compatible with the expected runtime environment. The method verifies: - If the package has at least one component - If all wires create connections between existing components and their variables - If metadata is defined and valid. - If a package with the same name already exists in the `destination` folder. In this case a warning message appears and the `save(..)` method overwrites the existing package. - If the package has multiple components and if they are using the same Python version Args: destination (str, optional): Path of the expected destination folder. Defaults to \" . \". \"\"\" if len ( self . components ) < 1 : raise AssertionError ( \"The package must have at least one component.\" ) for name , variable in self . outputs : if self . components [ name ] . batch . outputBatch : raise AssertionError ( f \"The component '{name}' has pipeline output defined with variable name '{variable}'. \\ None of component with pipeline output is allowed to provide batch output.\" ) for wire_hash in self . wiring . copy () : wire = self . wiring [ wire_hash ] self . _check_wiring ( wire , wire_hash ) pipeline_inputs = [ variable for _, variable in self.inputs ] pipeline_outputs = [ variable for _, variable in self.outputs ] if any ( variable in pipeline_outputs for variable in pipeline_inputs ) : conflicts = set ( pipeline_inputs ). intersection ( set ( pipeline_outputs )) raise AssertionError ( f \"Pipeline input and output variables must be unique. Conflicting variables: {conflicts}\" ) self . _check_timeshifting () package_path = Path ( destination ) / f \"{self.name}_{self.version}\" . replace ( \" \" , \"-\" ) if package_path . is_dir () : _logger . warning ( f \"Target folder ({package_path}) already exists! Unless changing the package name the package could be invalid and your files will be overwritten!\" ) python_versions = set () for component in self . components : self . components [ component ] . validate () if isinstance ( self . components [ component ] , PythonComponent ) : python_versions . add ( self . components [ component ] . python_version ) if ( 1 < len ( python_versions )) : _logger . warning ( \"The use of multiple python version in a single pipeline is not recommended. We recommend using only one of the supported versions, which are Python 3.10 or 3.11.\" ) _logger . info ( f \"Package '{self.name}' is valid and ready to save.\" )","title":"validate"},{"location":"reference/simaticai/deployment.html#pythoncomponent","text":"A pipeline component implemented using Python scripts and libraries. A PythonComponent wraps Python code resource files such as saved models into a structured folder, which can be added to a pipeline configuration package. For a comprehensive overview on how to wrap ML models into Python components, we recommend you refer to the AI SDK User Manual, especially the guideline for writing pipeline components. We also recommend you study the example Python components in the E2E Tutorials for the AI SDK. A new PythonComponent is empty. class PythonComponent ( name = 'inference' , version = '0.0.1' , python_version = '3.10' , desc : str = '' )","title":"PythonComponent"},{"location":"reference/simaticai/deployment.html#attributes_3","text":"Name Type Description Default name str Component name. (default: inference) None desc str Component description (optional) None version str Component version. (default: 0.0.1) None python_version str Python version on the target AI Inference Server. At the moment of writing, the current version supports Python 3.10 and 3.11. None View Source class PythonComponent ( Component ): \"\"\" A pipeline component implemented using Python scripts and libraries. A `PythonComponent` wraps Python code resource files such as saved models into a structured folder, which can be added to a pipeline configuration package. For a comprehensive overview on how to wrap ML models into Python components, we recommend you refer to the AI SDK User Manual, especially the guideline for writing pipeline components. We also recommend you study the example Python components in the E2E Tutorials for the AI SDK. A new `PythonComponent` is empty. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, the current version supports Python 3.10 and 3.11. \"\"\" def __init__ ( self , name = \"inference\" , version = \"0.0.1\" , python_version = '3.10' , desc : str = \"\" ): \"\"\" Creates a new, empty Python component. Args: name (str): Component name. (default: inference) desc (str): Component description (optional) version (str): Component version. (default: 0.0.1) python_version (str): Python version on the target AI Inference Server. At the moment of writing, AI Inference Server supports Python 3.10 and 3.11. \"\"\" super () . __init__ ( name = name , desc = desc ) try : python_version_validator ( python_version ) except ValueError as error : raise AssertionError ( error ) self . python_version = python_version self . version = version self . metrics = {} self . entrypoint : Optional [ Path ] = None self . resources = {} self . python_dependencies = PythonDependencies ( python_version ) self . _replicas = 1 self . is_valid = False def __repr__ ( self ) -> str : text = super () . __repr__ () if len ( self . metrics ) > 0 : text += \" \\n Metrics: \\n \" for name , metric in self . metrics . items (): text += f \"< {name}{': ' + metric['desc'] if metric.get('desc') is not None else ''} \\n \" if len ( self . resources ): text += \" \\n Resources: \\n \" for path , base in self . resources . items (): text += f \" {base}/{path.name} \\n \" . replace ( './' , '' ) if self . entrypoint is not None : text += f \"Entrypoint: {self.entrypoint} \\n \" return text def set_entrypoint ( self , entrypoint : str ): \"\"\" Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. ```python import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys.path.insert(0, str(Path('./src').resolve())) from my_module import processor # then the processor module can be imported def run(data: str): input_data = json.loads(data) # incoming JSON string is loaded as a dictionary result = processor.process_data(input_data) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None: answer = {\"ready\": False, \"output\": None} else: answer = {\"ready\": True, \"output\": json.dumps(result)} return answer ``` Args: entrypoint (str): Name of the new entrypoint script to be copied \"\"\" self . is_valid = False if not any ( key . name for key , value in self . resources . items () if key . name == entrypoint and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) self . entrypoint = Path ( entrypoint ) def add_resources ( self , base_dir : os . PathLike , resources : Union [ os . PathLike , list ]): \"\"\" Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in '__pycache__' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Args: base_dir (path-like): Root folder of your code from which the resources are referred resources (os.PathLike or List): A single path or list of relative paths to resource files \"\"\" self . is_valid = False base_dir = Path ( base_dir ) . resolve () . absolute () if not base_dir . is_dir (): raise AssertionError ( f \"Parameter 'base_dir' must be a directory and available in path {base_dir}.\" ) resources = resources if type ( resources ) is list else [ resources ] for resource in resources : self . _add_resource ( base_dir , resource ) def _add_resource ( self , base_dir : Path , resource : os . PathLike ): self . is_valid = False if Path ( resource ) . is_absolute () or '..' in resource : raise AssertionError ( \"The resource path must be relative and cannot contain '/../' elements.\" ) resource_path = base_dir / resource if resource_path . is_file (): self . _add_resource_file ( base_dir , resource_path ) return if resource_path . is_dir (): for glob_path in resource_path . rglob ( \"*\" ): if glob_path . is_file (): self . _add_resource_file ( base_dir , glob_path ) return raise AssertionError ( f \"Specified resource is not a file or directory: '{resource}'\" ) def _add_resource_file ( self , base_dir : Path , resource_path : Path ): self . is_valid = False for parent in resource_path . parents : if parent . name == '__pycache__' : return if resource_path in self . resources . keys (): _logger . warning ( f \"Resource '{resource_path}' is already added to target directory '{self.resources[resource_path]}'\" ) return self . resources [ resource_path ] = f \"{resource_path.parent.relative_to(base_dir)}\" def add_dependencies ( self , packages : list ): \"\"\" Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Args: packages (list): Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution \"\"\" self . is_valid = False self . python_dependencies . add_dependencies ( packages ) def set_requirements ( self , requirements_path : os . PathLike ): \"\"\" Reads the defined dependencies from the given `requirements.txt` file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of `--extra-index-url=my.repo.example.com`. Args: requirements_path (str): Path of the given `requirements.txt` file \"\"\" self . is_valid = False self . python_dependencies . set_requirements ( requirements_path ) def add_python_packages ( self , path : str ) -> None : \"\"\" Adds Python package(s) to the `PythonPackages.zip` file of the component. The `path` parameter can refer to either a `whl`, a `zip` or a `tar.gz` file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the `tempfile.tempdir` folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Args: path (str): Path of the distribution file Examples: `component.add_python_packages('../resources/my_package-0.0.1-py3-none-any.wheel')` adds the wheel file to `PythonPackages.zip` and adds dictionary item `component.dependencies['my_package'] = '0.0.1'` `component.add_python_packages('../resources/inference-wheels.zip')` adds all the wheel files in the zip to `PythonPackages.zip` and `component.dependencies` \"\"\" self . is_valid = False self . python_dependencies . add_python_packages ( path ) def set_parallel_steps ( self , replicas ): \"\"\" Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Args: replicas (int): Number of parallel executors. Default is 1. Raises: ValueError: if the given argument is not a positive integer. \"\"\" self . is_valid = False if ( not isinstance ( replicas , int )) or replicas < 1 : raise ValueError ( \"Replica count must be a positive integer.\" ) if 8 < replicas : _logger . warning ( \"The current maximum of parallel executors is 8.\" ) self . _replicas = replicas def add_metric ( self , name : str , desc : Optional [ str ] = None ): \"\"\" Adds a metric that will be automatically used as a pipeline output. Args: name (str): Name of the metric. desc (str): Description of the metric. (optional) \"\"\" if \"_\" not in name : raise AssertionError ( \"The metric name must contain at least one underscore\" ) if self . metrics is None : self . metrics = {} if name in self . metrics : raise AssertionError ( f \"Metric '{name}' already exists\" ) self . metrics [ name ] = {} if desc is not None : self . metrics [ name ][ 'desc' ] = desc def delete_metric ( self , name : str ): \"\"\" Remove a previously added metric. Args: name (str): Name of the metric to be deleted. \"\"\" if name not in self . metrics : raise AssertionError ( f \"Component '{self.name}' has no metric '{name}'\" ) self . metrics . pop ( name ) def _to_dict ( self ): component_dict = { ** super () . _to_dict (), 'version' : self . version , 'entrypoint' : f \"./{self.entrypoint.name}\" , 'hwType' : 'CPU' , 'runtime' : { 'type' : 'python' , 'version' : self . python_version }, 'replicas' : self . _replicas } component_dict [ \"outputType\" ] += [{ 'name' : name , 'type' : 'String' , 'metric' : True , } for name in self . metrics . keys ()] return component_dict def enable_dependency_optimization ( self ): \"\"\" Allows changing repository URLs to optimize the package size Allows the replacement of the `--index-url` argument during `pip download` to download CPU runtime optimized dependencies only. Enabling this optimization, the present `--index-url` will be prepended to the `--extra-index-url` list, and the Pytorch CPU only repository will be set as the `--index-url`. A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . enable_dependency_optimization () def disable_dependency_optimization ( self ): \"\"\" Disables any modification to repository URLs Disables the replacement of the `--index-url` argument during `pip download`. This way all `--index-url` or `--extra-index-url` arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\"\" self . python_dependencies . disable_dependency_optimization () def validate ( self ): \"\"\" Validates that the component is ready to be serialized and packaged as part of a pipeline. \"\"\" if not self . is_valid : if self . entrypoint is None : raise AssertionError ( \"Entrypoint must be defined\" ) if not any ( key . name for key , value in self . resources . items () if key . name == self . entrypoint . name and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) if len ( self . python_dependencies . dependencies ) < 1 : _logger . warning ( f \"WARNING! There are no dependencies defined for component '{self.name}'. Please make sure that all necessary dependencies have been added.\" ) self . python_dependencies . validate () self . is_valid = True _logger . info ( f \"Component '{self.name}' is valid and ready to use.\" ) def save ( self , destination , validate = True ): \"\"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter `validate` to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: - `requirements.txt` with a list of Python dependencies - Entry point script defined by the `entrypoint` attribute of the component - Extra files as added to the specified folders - `PythonPackages.zip` with the wheel binaries for the environment to be installed Args: destination (path-like): Target directory to which the component will be saved. validate (bool): With value True, triggers component validation. Defaults to True. \"\"\" if validate : self . validate () folder_path = Path ( destination ) / self . name folder_path . mkdir ( parents = True , exist_ok = True ) for file_path in self . resources : dir_path = folder_path / self . resources [ file_path ] os . makedirs ( dir_path , exist_ok = True ) shutil . copy ( file_path , dir_path / file_path . name ) self . python_dependencies . save ( folder_path )","title":"Attributes"},{"location":"reference/simaticai/deployment.html#ancestors-in-mro_1","text":"simaticai.deployment.Component","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/deployment.html#class-variables_2","text":"BatchInfo reserved_names","title":"Class variables"},{"location":"reference/simaticai/deployment.html#methods_3","text":"","title":"Methods"},{"location":"reference/simaticai/deployment.html#add_dependencies_1","text":"def add_dependencies ( self , packages : list ) Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Parameters: Name Type Description Default packages list Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution None View Source def add_dependencies ( self , packages : list ): \"\"\" Adds required dependencies for the Python code. The list must contain the name of the Python packages or tuples in the form of (name, version) which are required to execute the component on AI Inference Server. The method will search for the packages for the target platform and collect their transitive dependencies as well. Packages that are distributed only in source format can be added too, but only if they are pure Python packages. Args: packages (list): Can be a list of strings (name) or a list of tuples (name, version) of the required packages for component execution \"\"\" self . is_valid = False self . python_dependencies . add_dependencies ( packages )","title":"add_dependencies"},{"location":"reference/simaticai/deployment.html#add_input_3","text":"def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector payload = { \"image\" : { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height , \"mimeType\" : [ \"image/raw\" ], \"dataType\" : \"uint8\" , \"channelsPerPixel\" : 3 , \"image\" : _swap_bytes ( image . tobytes ()) } } Between components the format is the same format as the format of Object as an output. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new input. None _type str Type of the new input. None desc str Description of the input. (optional) None View Source def add_input ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new input to the component with its type. Name of the variables cannot be reserved name like 'timestamp'. Input variable 'timestamp' is a prebuilt key in the payload and its value contains the timestamp when the payload is created by AI Inference Server. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data received from Databus - Object: Object type variables are designed to receive from Vision Connect or transfer images between components - Numeric scalar types: Typically used for data received from S7 Connector The example payload below shows the format of image received from VCA Connector ```python payload = { \"image\": { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height, \"mimeType\": [\"image/raw\"], \"dataType\": \"uint8\", \"channelsPerPixel\": 3, \"image\": _swap_bytes(image.tobytes()) } } ``` Between components the format is the same format as the format of Object as an output. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new input. _type (str): Type of the new input. desc (str): Description of the input. (optional) \"\"\" if self . inputs is None : self . inputs = {} if name in self . inputs : raise AssertionError ( f \"Input '{name}' already exists.\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Input '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ] = { \"type\" : _type , } if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"add_input"},{"location":"reference/simaticai/deployment.html#add_metric","text":"def add_metric ( self , name : str , desc : Optional [ str ] = None ) Adds a metric that will be automatically used as a pipeline output. Parameters: Name Type Description Default name str Name of the metric. None desc str Description of the metric. (optional) None View Source def add_metric ( self , name : str , desc : Optional [ str ] = None ) : \"\"\" Adds a metric that will be automatically used as a pipeline output. Args: name (str): Name of the metric. desc (str): Description of the metric. (optional) \"\"\" if \"_\" not in name : raise AssertionError ( \"The metric name must contain at least one underscore\" ) if self . metrics is None : self . metrics = {} if name in self . metrics : raise AssertionError ( f \"Metric '{name}' already exists\" ) self . metrics [ name ] = {} if desc is not None : self . metrics [ name ][ 'desc' ] = desc","title":"add_metric"},{"location":"reference/simaticai/deployment.html#add_output_3","text":"def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the type_dictionary . Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type Object the entrypoint must return with a dictionary containing two fields, where one field has type str and the other field has type bytes . The example below shows the required format, assuming that 'image' is a PIL Image. \"processedImage\" : { \"metadata\" : json . dumps ( { \"resolutionWidth\" : image . width , \"resolutionHeight\" : image . height } ), \"bytes\" : image . tobytes () } Parameters: Name Type Description Default name str Name of the new output. None _type str Type of the new output. None desc str Description of the output. (optional) None View Source def add_output ( self , name : str , _type : str , desc : Optional [ str ] = None ): \"\"\" Adds a new output to the component. Types supported by AI Inference Server version 1.6 are contained in the `type_dictionary`. Newer AI Inference server version may support additional types. In case the type is not known by the AI SDK, a warning message will be printed. The most frequently used types are - String: Typically used for data to be sent to Databus - Object: Typically used for images to be sent to ZMQ Connector - Numeric scalar types: Typically used for data sent to S7 Connector For outputs of type `Object` the entrypoint must return with a `dictionary` containing two fields, where one field has type `str` and the other field has type `bytes`. The example below shows the required format, assuming that 'image' is a PIL Image. ```python \"processedImage\": { \"metadata\": json.dumps( { \"resolutionWidth\": image.width, \"resolutionHeight\": image.height } ), \"bytes\": image.tobytes() } ``` Args: name (str): Name of the new output. _type (str): Type of the new output. desc (str): Description of the output. (optional) \"\"\" if self . outputs is None : self . outputs = {} if name in self . outputs : raise AssertionError ( f \"Output '{name}' already exists\" ) if name . lower () in self . reserved_names : raise AssertionError ( f \"Output '{name}' is a reserved keyword.\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ] = { \"type\" : _type , } if desc is not None : self . outputs [ name ][ 'desc' ] = desc","title":"add_output"},{"location":"reference/simaticai/deployment.html#add_python_packages","text":"def add_python_packages ( self , path : str ) -> None Adds Python package(s) to the PythonPackages.zip file of the component. The path parameter can refer to either a whl , a zip or a tar.gz file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the tempfile.tempdir folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Parameters: Name Type Description Default path str Path of the distribution file None View Source def add_python_packages ( self , path : str ) -> None : \" \"\" Adds Python package(s) to the `PythonPackages.zip` file of the component. The `path` parameter can refer to either a `whl`, a `zip` or a `tar.gz` file. Zip files can be either a source distribution package or a collection of Python packages. Only pure Python source distributions are allowed. The dependency list of the component will be extended with the files added here, so that they will also get installed on the AI Inference Server. The method uses the `tempfile.tempdir` folder, so make sure that the folder is writeable. The wheel files must fulfill the requirements of the targeted device environment (e.g., the Python version must match the supported Python version of the targeted AI Inference Server, and the platform should be one of the supported ones too). Args: path (str): Path of the distribution file Examples: `component.add_python_packages('../resources/my_package-0.0.1-py3-none-any.wheel')` adds the wheel file to `PythonPackages.zip` and adds dictionary item `component.dependencies['my_package'] = '0.0.1'` `component.add_python_packages('../resources/inference-wheels.zip')` adds all the wheel files in the zip to `PythonPackages.zip` and `component.dependencies` \"\" \" self . is_valid = False self . python_dependencies . add_python_packages ( path )","title":"add_python_packages"},{"location":"reference/simaticai/deployment.html#add_resources","text":"def add_resources ( self , base_dir : os . PathLike , resources : Union [ os . PathLike , list ] ) Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in ' pycache ' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Parameters: Name Type Description Default base_dir path-like Root folder of your code from which the resources are referred None resources os.PathLike or List A single path or list of relative paths to resource files None View Source def add_resources ( self , base_dir : os . PathLike , resources : Union [ os.PathLike, list ] ) : \"\"\" Adds files to a component. To make your file resources available on the AI Inference Server you need to add them to the package resources. These resources can be Python or config files, serialized ML models or reference data. They are then available on path {component_root}/{resources} in the runtime environment. When saving the package they will be copied from {base_dir}/{resources} into the package. Files in '__pycache__' folders will be excluded. Until version 2.3.0 of AI SDK hidden files and folders (starting with '.') are also excluded. Args: base_dir (path-like): Root folder of your code from which the resources are referred resources (os.PathLike or List): A single path or list of relative paths to resource files \"\"\" self . is_valid = False base_dir = Path ( base_dir ). resolve (). absolute () if not base_dir . is_dir () : raise AssertionError ( f \"Parameter 'base_dir' must be a directory and available in path {base_dir}.\" ) resources = resources if type ( resources ) is list else [ resources ] for resource in resources : self . _add_resource ( base_dir , resource )","title":"add_resources"},{"location":"reference/simaticai/deployment.html#change_input_2","text":"def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the inputs of the component. Parameters: Name Type Description Default name str Name of the input to be changed. None _type str New type of the input. None desc str Description of the input. (optional) None View Source def change_input ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the inputs of the component. Args: name (str): Name of the input to be changed. _type (str): New type of the input. desc (str): Description of the input. (optional) \"\"\" if name not in self . inputs : raise AssertionError ( f \"There is no input with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . inputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"change_input"},{"location":"reference/simaticai/deployment.html#change_output_2","text":"def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) Changes one of the outputs of the component. Parameters: Name Type Description Default name str Name of the output to be changed. None _type str The new type of the output. None desc str Description of the output. (optional) None View Source def change_output ( self , name : str , _type : str , desc : Optional [ str ] = None ) : \"\"\" Changes one of the outputs of the component. Args: name (str): Name of the output to be changed. _type (str): The new type of the output. desc (str): Description of the output. (optional) \"\"\" if name not in self . outputs : raise AssertionError ( f \"There is no output with name '{name}'\" ) if _type not in supported_types : _logger . warning ( f \"WARNING! Unknown type '{_type}' for input variable '{name}'. Please check if the target Inference Server supports this type.\" ) self . outputs [ name ][ 'type' ] = _type if desc is not None : self . inputs [ name ][ 'desc' ] = desc","title":"change_output"},{"location":"reference/simaticai/deployment.html#delete_input_3","text":"def delete_input ( self , name : str ) Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use package.delete_input_wire(...) with default parameter with_input=True . Parameters: Name Type Description Default name str Name of the input to be deleted. None View Source def delete_input ( self , name : str ): \"\"\" Deletes an input from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. It is recommended to use `package.delete_input_wire(...)` with default parameter `with_input=True`. Args: name (str): Name of the input to be deleted. \"\"\" if name not in self . inputs : raise AssertionError ( f \"Component '{self.name}' has no input '{name}'\" ) self . inputs . pop ( name )","title":"delete_input"},{"location":"reference/simaticai/deployment.html#delete_metric","text":"def delete_metric ( self , name : str ) Remove a previously added metric. Parameters: Name Type Description Default name str Name of the metric to be deleted. None View Source def delete_metric(self, name: str): \"\"\" Remove a previously added metric. Args: name (str): Name of the metric to be deleted. \"\"\" if name not in self.metrics: raise AssertionError(f\"Component '{self.name}' has no metric '{name}'\") self.metrics.pop(name)","title":"delete_metric"},{"location":"reference/simaticai/deployment.html#delete_output_3","text":"def delete_output ( self , name : str ) Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Parameters: Name Type Description Default name str Name of the output to be deleted. None View Source def delete_output ( self , name : str ): \"\"\" Deletes an output from the component by name. Once the package has been created with the given component, it is recommended not to change the component directly. Instead, all necessary methods to change it are available through the package to avoid component inconsistencies. Deleting an output which is represented in any wire will cause package inconsistency. Args: name (str): Name of the output to be deleted. \"\"\" if name not in self . outputs : raise AssertionError ( f \"Component '{self.name}' has no output '{name}'\" ) self . outputs . pop ( name )","title":"delete_output"},{"location":"reference/simaticai/deployment.html#disable_dependency_optimization","text":"def disable_dependency_optimization ( self ) Disables any modification to repository URLs Disables the replacement of the --index-url argument during pip download . This way all --index-url or --extra-index-url arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a PythonComponent can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an ONNX model and use it within a GPURuntimeComponent . View Source def disable_dependency_optimization ( self ) : \" \"\" Disables any modification to repository URLs Disables the replacement of the `--index-url` argument during `pip download`. This way all `--index-url` or `--extra-index-url` arguments will be preserved if they were present in the requirements.txt file. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. A warning message will be printed about the package size if this optimization is disabled and the dependency list contains GPU optimized dependencies. Disabling this optimization will not allow the component to run on GPU. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\" \" self . python_dependencies . disable_dependency_optimization ()","title":"disable_dependency_optimization"},{"location":"reference/simaticai/deployment.html#enable_dependency_optimization","text":"def enable_dependency_optimization ( self ) Allows changing repository URLs to optimize the package size Allows the replacement of the --index-url argument during pip download to download CPU runtime optimized dependencies only. Enabling this optimization, the present --index-url will be prepended to the --extra-index-url list, and the Pytorch CPU only repository will be set as the --index-url . A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a PythonComponent can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an ONNX model and use it within a GPURuntimeComponent . View Source def enable_dependency_optimization ( self ) : \" \"\" Allows changing repository URLs to optimize the package size Allows the replacement of the `--index-url` argument during `pip download` to download CPU runtime optimized dependencies only. Enabling this optimization, the present `--index-url` will be prepended to the `--extra-index-url` list, and the Pytorch CPU only repository will be set as the `--index-url`. A warning message will be printed if the repository URL modification was necessary. Some dependencies have both CPU and GPU runtime versions, pytorch for example, but a `PythonComponent` can only run on CPU, so packaging the additional GPU runtime dependencies just enlarges the package size. If you want to run your model on GPU, convert it to an `ONNX` model and use it within a `GPURuntimeComponent`. \"\" \" self . python_dependencies . enable_dependency_optimization ()","title":"enable_dependency_optimization"},{"location":"reference/simaticai/deployment.html#save_3","text":"def save ( self , destination , validate = True ) Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter validate to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: requirements.txt with a list of Python dependencies Entry point script defined by the entrypoint attribute of the component Extra files as added to the specified folders PythonPackages.zip with the wheel binaries for the environment to be installed Parameters: Name Type Description Default destination path-like Target directory to which the component will be saved. None validate bool With value True, triggers component validation. Defaults to True. True View Source def save ( self , destination , validate = True ) : \" \"\" Saves the component to a folder structure, so it can be used as part of a pipeline configuration package. Validation can be skipped by setting parameter `validate` to False. This is useful when the component is already validated and only intended to be saved. The component folder contains the following: - `requirements.txt` with a list of Python dependencies - Entry point script defined by the `entrypoint` attribute of the component - Extra files as added to the specified folders - `PythonPackages.zip` with the wheel binaries for the environment to be installed Args: destination (path-like): Target directory to which the component will be saved. validate (bool): With value True, triggers component validation. Defaults to True. \"\" \" if validate : self . validate () folder_path = Path ( destination ) / self . name folder_path . mkdir ( parents = True , exist_ok = True ) for file_path in self . resources : dir_path = folder_path / self . resources [ file_path ] os . makedirs ( dir_path , exist_ok = True ) shutil . copy ( file_path , dir_path / file_path . name ) self . python_dependencies . save ( folder_path )","title":"save"},{"location":"reference/simaticai/deployment.html#set_entrypoint","text":"def set_entrypoint ( self , entrypoint : str ) Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys . path . insert ( 0 , str ( Path ( './src' ) . resolve ())) from my_module import processor # then the processor module can be imported def run ( data : str ): input_data = json . loads ( data ) # incoming JSON string is loaded as a dictionary result = processor . process_data ( input_data ) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None : answer = { \"ready\" : False , \"output\" : None } else : answer = { \"ready\" : True , \"output\" : json . dumps ( result )} return answer Parameters: Name Type Description Default entrypoint str Name of the new entrypoint script to be copied None View Source def set_entrypoint ( self , entrypoint : str ): \"\"\" Sets the entrypoint module for the component. The entrypoint is the Python code which is responsible for receiving the input data and producing a structured response with the output for the AI Inference Server. The script should consume a JSON string and produce another. See the short example below. The file will be copied into the root directory of the component on the AI Inference Server, so every file reference should be aligned. The example code below shows a basic structure of the entrypoint Python code. ```python import json import sys from pathlib import Path # by adding the parent folder of your modules to system path makes them available for relative import sys.path.insert(0, str(Path('./src').resolve())) from my_module import processor # then the processor module can be imported def run(data: str): input_data = json.loads(data) # incoming JSON string is loaded as a dictionary result = processor.process_data(input_data) # the process_data can be called to process the incoming data # the code below creates the formatted output for the AI Inference Server if result is None: answer = {\"ready\": False, \"output\": None} else: answer = {\"ready\": True, \"output\": json.dumps(result)} return answer ``` Args: entrypoint (str): Name of the new entrypoint script to be copied \"\"\" self . is_valid = False if not any ( key . name for key , value in self . resources . items () if key . name == entrypoint and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) self . entrypoint = Path ( entrypoint )","title":"set_entrypoint"},{"location":"reference/simaticai/deployment.html#set_parallel_steps","text":"def set_parallel_steps ( self , replicas ) Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Parameters: Name Type Description Default replicas int Number of parallel executors. Default is 1. None Raises: Type Description ValueError if the given argument is not a positive integer. View Source def set_parallel_steps ( self , replicas ): \"\"\" Sets the number of parallel executors. This method configures how many instances of the component can be executed at the same time. The component must be suitable for parallel execution. The inputs arriving to the component will be processed by different instances in parallel, and these instances do not share their state (e.g. variables). Every instance is initialized separately and receives only a fraction of the inputs. AI Inference Server supports at most 8 parallel instances.``` Args: replicas (int): Number of parallel executors. Default is 1. Raises: ValueError: if the given argument is not a positive integer. \"\"\" self . is_valid = False if ( not isinstance ( replicas , int )) or replicas < 1 : raise ValueError ( \"Replica count must be a positive integer.\" ) if 8 < replicas : _logger . warning ( \"The current maximum of parallel executors is 8.\" ) self . _replicas = replicas","title":"set_parallel_steps"},{"location":"reference/simaticai/deployment.html#set_requirements","text":"def set_requirements ( self , requirements_path : os . PathLike ) Reads the defined dependencies from the given requirements.txt file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of --extra-index-url=my.repo.example.com . Parameters: Name Type Description Default requirements_path str Path of the given requirements.txt file None View Source def set_requirements(self, requirements_path: os.PathLike): \"\"\" Reads the defined dependencies from the given `requirements.txt` file and creates a new dependency list. Previously added dependencies will be cleared. The file format must follow Python's requirements file format defined in PEP 508. It can contain URLs to additional repositories in the form of `--extra-index-url=my.repo.example.com` . Args: requirements_path (str): Path of the given `requirements.txt` file \"\"\" self.is_valid = False self.python_dependencies.set_requirements(requirements_path)","title":"set_requirements"},{"location":"reference/simaticai/deployment.html#validate_3","text":"def validate ( self ) Validates that the component is ready to be serialized and packaged as part of a pipeline. View Source def validate ( self ): \"\"\" Validates that the component is ready to be serialized and packaged as part of a pipeline. \"\"\" if not self . is_valid : if self . entrypoint is None : raise AssertionError ( \"Entrypoint must be defined\" ) if not any ( key . name for key , value in self . resources . items () if key . name == self . entrypoint . name and value == '.' ): raise AssertionError ( \"Entrypoint must be added as resource to the root directory before setting up as entrypoint.\" ) if len ( self . python_dependencies . dependencies ) < 1 : _logger . warning ( f \"WARNING! There are no dependencies defined for component '{self.name}'. Please make sure that all necessary dependencies have been added.\" ) self . python_dependencies . validate () self . is_valid = True _logger . info ( f \"Component '{self.name}' is valid and ready to use.\" )","title":"validate"},{"location":"reference/simaticai/model_config_pb2.html","text":"Module simaticai.model_config_pb2 None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. # Generated by the protocol buffer compiler. DO NOT EDIT! # source: model_config.proto import sys _b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( 'latin1' )) from google.protobuf.internal import enum_type_wrapper from google.protobuf import descriptor as _descriptor from google.protobuf import message as _message from google.protobuf import reflection as _reflection from google.protobuf import symbol_database as _symbol_database # @@protoc_insertion_point(imports) _sym_db = _symbol_database . Default () DESCRIPTOR = _descriptor . FileDescriptor ( name = 'model_config.proto' , package = 'inference' , syntax = 'proto3' , serialized_options = None , serialized_pb = _b ( ' \\n\\x12 model_config.proto \\x12\\t inference \\\"\\x96\\x01\\n\\x10 ModelRateLimiter \\x12\\x37\\n\\t resources \\x18\\x01 \\x03 ( \\x0b\\x32 $.inference.ModelRateLimiter.Resource \\x12\\x10\\n\\x08 priority \\x18\\x02 \\x01 ( \\r\\x1a\\x37\\n\\x08 Resource \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x0e\\n\\x06 global \\x18\\x02 \\x01 ( \\x08\\x12\\r\\n\\x05\\x63 ount \\x18\\x03 \\x01 ( \\r\\\"\\x87\\x04\\n\\x12 ModelInstanceGroup \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x30\\n\\x04 kind \\x18\\x04 \\x01 ( \\x0e\\x32\\\" .inference.ModelInstanceGroup.Kind \\x12\\r\\n\\x05\\x63 ount \\x18\\x02 \\x01 ( \\x05\\x12\\x31\\n\\x0c rate_limiter \\x18\\x06 \\x01 ( \\x0b\\x32\\x1b .inference.ModelRateLimiter \\x12\\x0c\\n\\x04 gpus \\x18\\x03 \\x03 ( \\x05\\x12 H \\n\\x11 secondary_devices \\x18\\x08 \\x03 ( \\x0b\\x32 -.inference.ModelInstanceGroup.SecondaryDevice \\x12\\x0f\\n\\x07 profile \\x18\\x05 \\x03 ( \\t\\x12\\x0f\\n\\x07 passive \\x18\\x07 \\x01 ( \\x08\\x12\\x13\\n\\x0b host_policy \\x18\\t \\x01 ( \\t\\x1a\\x9c\\x01\\n\\x0f SecondaryDevice \\x12 O \\n\\x04 kind \\x18\\x01 \\x01 ( \\x0e\\x32\\x41 .inference.ModelInstanceGroup.SecondaryDevice.SecondaryDeviceKind \\x12\\x11\\n\\t device_id \\x18\\x02 \\x01 ( \\x03\\\" % \\n\\x13 SecondaryDeviceKind \\x12\\x0e\\n\\n KIND_NVDLA \\x10\\x00\\\" A \\n\\x04 Kind \\x12\\r\\n\\t KIND_AUTO \\x10\\x00\\x12\\x0c\\n\\x08 KIND_GPU \\x10\\x01\\x12\\x0c\\n\\x08 KIND_CPU \\x10\\x02\\x12\\x0e\\n\\n KIND_MODEL \\x10\\x03\\\" # \\n\\x12 ModelTensorReshape \\x12\\r\\n\\x05 shape \\x18\\x01 \\x03 ( \\x03\\\"\\xb2\\x02\\n\\n ModelInput \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 & \\n\\t data_type \\x18\\x02 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12 , \\n\\x06\\x66 ormat \\x18\\x03 \\x01 ( \\x0e\\x32\\x1c .inference.ModelInput.Format \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x04 \\x03 ( \\x03\\x12 . \\n\\x07 reshape \\x18\\x05 \\x01 ( \\x0b\\x32\\x1d .inference.ModelTensorReshape \\x12\\x17\\n\\x0f is_shape_tensor \\x18\\x06 \\x01 ( \\x08\\x12\\x1a\\n\\x12\\x61 llow_ragged_batch \\x18\\x07 \\x01 ( \\x08\\x12\\x10\\n\\x08 optional \\x18\\x08 \\x01 ( \\x08\\\" ; \\n\\x06\\x46 ormat \\x12\\x0f\\n\\x0b\\x46 ORMAT_NONE \\x10\\x00\\x12\\x0f\\n\\x0b\\x46 ORMAT_NHWC \\x10\\x01\\x12\\x0f\\n\\x0b\\x46 ORMAT_NCHW \\x10\\x02\\\"\\xb2\\x01\\n\\x0b ModelOutput \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 & \\n\\t data_type \\x18\\x02 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x03 \\x03 ( \\x03\\x12 . \\n\\x07 reshape \\x18\\x05 \\x01 ( \\x0b\\x32\\x1d .inference.ModelTensorReshape \\x12\\x16\\n\\x0e label_filename \\x18\\x04 \\x01 ( \\t\\x12\\x17\\n\\x0f is_shape_tensor \\x18\\x06 \\x01 ( \\x08\\\"\\xd9\\x02\\n\\n BatchInput \\x12 ( \\n\\x04 kind \\x18\\x01 \\x01 ( \\x0e\\x32\\x1a .inference.BatchInput.Kind \\x12\\x13\\n\\x0b target_name \\x18\\x02 \\x03 ( \\t\\x12 & \\n\\t data_type \\x18\\x03 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x14\\n\\x0c source_input \\x18\\x04 \\x03 ( \\t\\\"\\xcd\\x01\\n\\x04 Kind \\x12\\x17\\n\\x13\\x42\\x41 TCH_ELEMENT_COUNT \\x10\\x00\\x12 # \\n\\x1f\\x42\\x41 TCH_ACCUMULATED_ELEMENT_COUNT \\x10\\x01\\x12 - \\n )BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO \\x10\\x02\\x12 $ \\n BATCH_MAX_ELEMENT_COUNT_AS_SHAPE \\x10\\x03\\x12\\x14\\n\\x10\\x42\\x41 TCH_ITEM_SHAPE \\x10\\x04\\x12\\x1c\\n\\x18\\x42\\x41 TCH_ITEM_SHAPE_FLATTEN \\x10\\x05\\\"\\x8f\\x01\\n\\x0b\\x42\\x61 tchOutput \\x12\\x13\\n\\x0b target_name \\x18\\x01 \\x03 ( \\t\\x12 ) \\n\\x04 kind \\x18\\x02 \\x01 ( \\x0e\\x32\\x1b .inference.BatchOutput.Kind \\x12\\x14\\n\\x0c source_input \\x18\\x03 \\x03 ( \\t\\\" * \\n\\x04 Kind \\x12\\\"\\n\\x1e\\x42\\x41 TCH_SCATTER_WITH_INPUT_SHAPE \\x10\\x00\\\"\\x90\\x02\\n\\x12 ModelVersionPolicy \\x12\\x36\\n\\x06 latest \\x18\\x01 \\x01 ( \\x0b\\x32 $.inference.ModelVersionPolicy.LatestH \\x00\\x12\\x30\\n\\x03\\x61 ll \\x18\\x02 \\x01 ( \\x0b\\x32 !.inference.ModelVersionPolicy.AllH \\x00\\x12 : \\n\\x08 specific \\x18\\x03 \\x01 ( \\x0b\\x32 &.inference.ModelVersionPolicy.SpecificH \\x00\\x1a\\x1e\\n\\x06 Latest \\x12\\x14\\n\\x0c num_versions \\x18\\x01 \\x01 ( \\r\\x1a\\x05\\n\\x03\\x41 ll \\x1a\\x1c\\n\\x08 Specific \\x12\\x10\\n\\x08 versions \\x18\\x01 \\x03 ( \\x03\\x42\\x0f\\n\\r policy_choice \\\"\\xfd\\r\\n\\x17 ModelOptimizationPolicy \\x12\\x37\\n\\x05 graph \\x18\\x01 \\x01 ( \\x0b\\x32 (.inference.ModelOptimizationPolicy.Graph \\x12\\x42\\n\\x08 priority \\x18\\x02 \\x01 ( \\x0e\\x32\\x30 .inference.ModelOptimizationPolicy.ModelPriority \\x12\\x35\\n\\x04\\x63 uda \\x18\\x03 \\x01 ( \\x0b\\x32\\' .inference.ModelOptimizationPolicy.Cuda \\x12 X \\n\\x16\\x65 xecution_accelerators \\x18\\x04 \\x01 ( \\x0b\\x32\\x38 .inference.ModelOptimizationPolicy.ExecutionAccelerators \\x12 R \\n\\x13 input_pinned_memory \\x18\\x05 \\x01 ( \\x0b\\x32\\x35 .inference.ModelOptimizationPolicy.PinnedMemoryBuffer \\x12 S \\n\\x14 output_pinned_memory \\x18\\x06 \\x01 ( \\x0b\\x32\\x35 .inference.ModelOptimizationPolicy.PinnedMemoryBuffer \\x12 & \\n\\x1e gather_kernel_buffer_threshold \\x18\\x07 \\x01 ( \\r\\x12\\x16\\n\\x0e\\x65\\x61 ger_batching \\x18\\x08 \\x01 ( \\x08\\x1a\\x16\\n\\x05 Graph \\x12\\r\\n\\x05 level \\x18\\x01 \\x01 ( \\x05\\x1a\\xba\\x05\\n\\x04\\x43 uda \\x12\\x0e\\n\\x06 graphs \\x18\\x01 \\x01 ( \\x08\\x12\\x18\\n\\x10\\x62 usy_wait_events \\x18\\x02 \\x01 ( \\x08\\x12\\x45\\n\\n graph_spec \\x18\\x03 \\x03 ( \\x0b\\x32\\x31 .inference.ModelOptimizationPolicy.Cuda.GraphSpec \\x12\\x1a\\n\\x12 output_copy_stream \\x18\\x04 \\x01 ( \\x08\\x1a\\xa4\\x04\\n\\t GraphSpec \\x12\\x12\\n\\n batch_size \\x18\\x01 \\x01 ( \\x05\\x12 K \\n\\x05 input \\x18\\x02 \\x03 ( \\x0b\\x32 <.inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry \\x12 W \\n\\x11 graph_lower_bound \\x18\\x03 \\x01 ( \\x0b\\x32 <.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound \\x1a\\x14\\n\\x05 Shape \\x12\\x0b\\n\\x03\\x64 im \\x18\\x01 \\x03 ( \\x03\\x1a\\xdf\\x01\\n\\n LowerBound \\x12\\x12\\n\\n batch_size \\x18\\x01 \\x01 ( \\x05\\x12 V \\n\\x05 input \\x18\\x02 \\x03 ( \\x0b\\x32 G.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry \\x1a\\x65\\n\\n InputEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\x46\\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x37 .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape: \\x02\\x38\\x01\\x1a\\x65\\n\\n InputEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\x46\\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x37 .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape: \\x02\\x38\\x01\\x1a\\xa4\\x03\\n\\x15\\x45 xecutionAccelerators \\x12 g \\n\\x19 gpu_execution_accelerator \\x18\\x01 \\x03 ( \\x0b\\x32\\x44 .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator \\x12 g \\n\\x19\\x63 pu_execution_accelerator \\x18\\x02 \\x03 ( \\x0b\\x32\\x44 .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator \\x1a\\xb8\\x01\\n\\x0b\\x41\\x63\\x63\\x65 lerator \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 h \\n\\n parameters \\x18\\x02 \\x03 ( \\x0b\\x32 T.inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry \\x1a\\x31\\n\\x0f ParametersEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a $ \\n\\x12 PinnedMemoryBuffer \\x12\\x0e\\n\\x06\\x65 nable \\x18\\x01 \\x01 ( \\x08\\\" I \\n\\r ModelPriority \\x12\\x14\\n\\x10 PRIORITY_DEFAULT \\x10\\x00\\x12\\x10\\n\\x0c PRIORITY_MAX \\x10\\x01\\x12\\x10\\n\\x0c PRIORITY_MIN \\x10\\x02\\\"\\xdb\\x01\\n\\x10 ModelQueuePolicy \\x12\\x41\\n\\x0e timeout_action \\x18\\x01 \\x01 ( \\x0e\\x32 ).inference.ModelQueuePolicy.TimeoutAction \\x12 $ \\n\\x1c\\x64\\x65\\x66\\x61 ult_timeout_microseconds \\x18\\x02 \\x01 ( \\x04\\x12\\x1e\\n\\x16\\x61 llow_timeout_override \\x18\\x03 \\x01 ( \\x08\\x12\\x16\\n\\x0e max_queue_size \\x18\\x04 \\x01 ( \\r\\\" & \\n\\r TimeoutAction \\x12\\n\\n\\x06 REJECT \\x10\\x00\\x12\\t\\n\\x05\\x44\\x45 LAY \\x10\\x01\\\"\\x9b\\x03\\n\\x14 ModelDynamicBatching \\x12\\x1c\\n\\x14 preferred_batch_size \\x18\\x01 \\x03 ( \\x05\\x12 $ \\n\\x1c max_queue_delay_microseconds \\x18\\x02 \\x01 ( \\x04\\x12\\x19\\n\\x11 preserve_ordering \\x18\\x03 \\x01 ( \\x08\\x12\\x17\\n\\x0f priority_levels \\x18\\x04 \\x01 ( \\x04\\x12\\x1e\\n\\x16\\x64\\x65\\x66\\x61 ult_priority_level \\x18\\x05 \\x01 ( \\x04\\x12\\x39\\n\\x14\\x64\\x65\\x66\\x61 ult_queue_policy \\x18\\x06 \\x01 ( \\x0b\\x32\\x1b .inference.ModelQueuePolicy \\x12 W \\n\\x15 priority_queue_policy \\x18\\x07 \\x03 ( \\x0b\\x32\\x38 .inference.ModelDynamicBatching.PriorityQueuePolicyEntry \\x1a W \\n\\x18 PriorityQueuePolicyEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\x04\\x12 * \\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x1b .inference.ModelQueuePolicy: \\x02\\x38\\x01\\\"\\x8b\\n\\n\\x15 ModelSequenceBatching \\x12\\x41\\n\\x06\\x64 irect \\x18\\x03 \\x01 ( \\x0b\\x32 /.inference.ModelSequenceBatching.StrategyDirectH \\x00\\x12\\x41\\n\\x06 oldest \\x18\\x04 \\x01 ( \\x0b\\x32 /.inference.ModelSequenceBatching.StrategyOldestH \\x00\\x12 & \\n\\x1e max_sequence_idle_microseconds \\x18\\x01 \\x01 ( \\x04\\x12\\x44\\n\\r control_input \\x18\\x02 \\x03 ( \\x0b\\x32 -.inference.ModelSequenceBatching.ControlInput \\x12\\x35\\n\\x05 state \\x18\\x05 \\x03 ( \\x0b\\x32 &.inference.ModelSequenceBatching.State \\x1a\\xb1\\x02\\n\\x07\\x43 ontrol \\x12 ; \\n\\x04 kind \\x18\\x01 \\x01 ( \\x0e\\x32 -.inference.ModelSequenceBatching.Control.Kind \\x12\\x18\\n\\x10 int32_false_true \\x18\\x02 \\x03 ( \\x05\\x12\\x17\\n\\x0f\\x66 p32_false_true \\x18\\x03 \\x03 ( \\x02\\x12\\x17\\n\\x0f\\x62 ool_false_true \\x18\\x05 \\x03 ( \\x08\\x12 & \\n\\t data_type \\x18\\x04 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\\" u \\n\\x04 Kind \\x12\\x1a\\n\\x16\\x43 ONTROL_SEQUENCE_START \\x10\\x00\\x12\\x1a\\n\\x16\\x43 ONTROL_SEQUENCE_READY \\x10\\x01\\x12\\x18\\n\\x14\\x43 ONTROL_SEQUENCE_END \\x10\\x02\\x12\\x1b\\n\\x17\\x43 ONTROL_SEQUENCE_CORRID \\x10\\x03\\x1a W \\n\\x0c\\x43 ontrolInput \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x39\\n\\x07\\x63 ontrol \\x18\\x02 \\x03 ( \\x0b\\x32 (.inference.ModelSequenceBatching.Control \\x1a\\x8a\\x01\\n\\x0c InitialState \\x12 & \\n\\t data_type \\x18\\x01 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x02 \\x03 ( \\x03\\x12\\x13\\n\\t zero_data \\x18\\x03 \\x01 ( \\x08 H \\x00\\x12\\x13\\n\\t data_file \\x18\\x04 \\x01 ( \\t H \\x00\\x12\\x0c\\n\\x04 name \\x18\\x05 \\x01 ( \\t B \\x0c\\n\\n state_data \\x1a\\xac\\x01\\n\\x05 State \\x12\\x12\\n\\n input_name \\x18\\x01 \\x01 ( \\t\\x12\\x13\\n\\x0b output_name \\x18\\x02 \\x01 ( \\t\\x12 & \\n\\t data_type \\x18\\x03 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x04 \\x03 ( \\x03\\x12\\x44\\n\\r initial_state \\x18\\x05 \\x03 ( \\x0b\\x32 -.inference.ModelSequenceBatching.InitialState \\x1a X \\n\\x0e StrategyDirect \\x12 $ \\n\\x1c max_queue_delay_microseconds \\x18\\x01 \\x01 ( \\x04\\x12 \\n\\x18 minimum_slot_utilization \\x18\\x02 \\x01 ( \\x02\\x1a\\x90\\x01\\n\\x0e StrategyOldest \\x12\\x1f\\n\\x17 max_candidate_sequences \\x18\\x01 \\x01 ( \\x05\\x12\\x1c\\n\\x14 preferred_batch_size \\x18\\x02 \\x03 ( \\x05\\x12 $ \\n\\x1c max_queue_delay_microseconds \\x18\\x03 \\x01 ( \\x04\\x12\\x19\\n\\x11 preserve_ordering \\x18\\x04 \\x01 ( \\x08\\x42\\x11\\n\\x0f strategy_choice \\\"\\xf6\\x02\\n\\x0f ModelEnsembling \\x12 - \\n\\x04 step \\x18\\x01 \\x03 ( \\x0b\\x32\\x1f .inference.ModelEnsembling.Step \\x1a\\xb3\\x02\\n\\x04 Step \\x12\\x12\\n\\n model_name \\x18\\x01 \\x01 ( \\t\\x12\\x15\\n\\r model_version \\x18\\x02 \\x01 ( \\x03\\x12 @ \\n\\t input_map \\x18\\x03 \\x03 ( \\x0b\\x32 -.inference.ModelEnsembling.Step.InputMapEntry \\x12\\x42\\n\\n output_map \\x18\\x04 \\x03 ( \\x0b\\x32 ..inference.ModelEnsembling.Step.OutputMapEntry \\x12\\x17\\n\\x0f model_namespace \\x18\\x05 \\x01 ( \\t\\x1a / \\n\\r InputMapEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a\\x30\\n\\x0e OutputMapEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\\" & \\n\\x0e ModelParameter \\x12\\x14\\n\\x0c string_value \\x18\\x01 \\x01 ( \\t\\\"\\xd9\\x02\\n\\x0b ModelWarmup \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x12\\n\\n batch_size \\x18\\x02 \\x01 ( \\r\\x12\\x32\\n\\x06 inputs \\x18\\x03 \\x03 ( \\x0b\\x32\\\" .inference.ModelWarmup.InputsEntry \\x12\\r\\n\\x05\\x63 ount \\x18\\x04 \\x01 ( \\r\\x1a\\x97\\x01\\n\\x05 Input \\x12 & \\n\\t data_type \\x18\\x01 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x02 \\x03 ( \\x03\\x12\\x13\\n\\t zero_data \\x18\\x03 \\x01 ( \\x08 H \\x00\\x12\\x15\\n\\x0b random_data \\x18\\x04 \\x01 ( \\x08 H \\x00\\x12\\x19\\n\\x0f input_data_file \\x18\\x05 \\x01 ( \\t H \\x00\\x42\\x11\\n\\x0f input_data_type \\x1a K \\n\\x0b InputsEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12 + \\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x1c .inference.ModelWarmup.Input: \\x02\\x38\\x01\\\" . \\n\\x0f ModelOperations \\x12\\x1b\\n\\x13 op_library_filename \\x18\\x01 \\x03 ( \\t\\\" + \\n\\x16 ModelTransactionPolicy \\x12\\x11\\n\\t decoupled \\x18\\x01 \\x01 ( \\x08\\\"\\xe6\\x01\\n\\x15 ModelRepositoryAgents \\x12\\x36\\n\\x06\\x61 gents \\x18\\x01 \\x03 ( \\x0b\\x32 &.inference.ModelRepositoryAgents.Agent \\x1a\\x94\\x01\\n\\x05\\x41 gent \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 J \\n\\n parameters \\x18\\x02 \\x03 ( \\x0b\\x32\\x36 .inference.ModelRepositoryAgents.Agent.ParametersEntry \\x1a\\x31\\n\\x0f ParametersEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\\" $ \\n\\x12 ModelResponseCache \\x12\\x0e\\n\\x06\\x65 nable \\x18\\x01 \\x01 ( \\x08\\\"\\xb2\\n\\n\\x0b ModelConfig \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x10\\n\\x08 platform \\x18\\x02 \\x01 ( \\t\\x12\\x0f\\n\\x07\\x62\\x61\\x63 kend \\x18\\x11 \\x01 ( \\t\\x12\\x35\\n\\x0e version_policy \\x18\\x03 \\x01 ( \\x0b\\x32\\x1d .inference.ModelVersionPolicy \\x12\\x16\\n\\x0e max_batch_size \\x18\\x04 \\x01 ( \\x05\\x12 $ \\n\\x05 input \\x18\\x05 \\x03 ( \\x0b\\x32\\x15 .inference.ModelInput \\x12 & \\n\\x06 output \\x18\\x06 \\x03 ( \\x0b\\x32\\x16 .inference.ModelOutput \\x12 * \\n\\x0b\\x62\\x61 tch_input \\x18\\x14 \\x03 ( \\x0b\\x32\\x15 .inference.BatchInput \\x12 , \\n\\x0c\\x62\\x61 tch_output \\x18\\x15 \\x03 ( \\x0b\\x32\\x16 .inference.BatchOutput \\x12\\x38\\n\\x0c optimization \\x18\\x0c \\x01 ( \\x0b\\x32\\\" .inference.ModelOptimizationPolicy \\x12 ; \\n\\x10\\x64 ynamic_batching \\x18\\x0b \\x01 ( \\x0b\\x32\\x1f .inference.ModelDynamicBatchingH \\x00\\x12 = \\n\\x11 sequence_batching \\x18\\r \\x01 ( \\x0b\\x32 .inference.ModelSequenceBatchingH \\x00\\x12\\x39\\n\\x13\\x65 nsemble_scheduling \\x18\\x0f \\x01 ( \\x0b\\x32\\x1a .inference.ModelEnsemblingH \\x00\\x12\\x35\\n\\x0e instance_group \\x18\\x07 \\x03 ( \\x0b\\x32\\x1d .inference.ModelInstanceGroup \\x12\\x1e\\n\\x16\\x64\\x65\\x66\\x61 ult_model_filename \\x18\\x08 \\x01 ( \\t\\x12 H \\n\\x12\\x63\\x63 _model_filenames \\x18\\t \\x03 ( \\x0b\\x32 ,.inference.ModelConfig.CcModelFilenamesEntry \\x12 ; \\n\\x0b metric_tags \\x18\\n \\x03 ( \\x0b\\x32 &.inference.ModelConfig.MetricTagsEntry \\x12 : \\n\\n parameters \\x18\\x0e \\x03 ( \\x0b\\x32 &.inference.ModelConfig.ParametersEntry \\x12 , \\n\\x0c model_warmup \\x18\\x10 \\x03 ( \\x0b\\x32\\x16 .inference.ModelWarmup \\x12\\x34\\n\\x10 model_operations \\x18\\x12 \\x01 ( \\x0b\\x32\\x1a .inference.ModelOperations \\x12\\x43\\n\\x18 model_transaction_policy \\x18\\x13 \\x01 ( \\x0b\\x32 !.inference.ModelTransactionPolicy \\x12\\x41\\n\\x17 model_repository_agents \\x18\\x17 \\x01 ( \\x0b\\x32 .inference.ModelRepositoryAgents \\x12\\x35\\n\\x0e response_cache \\x18\\x18 \\x01 ( \\x0b\\x32\\x1d .inference.ModelResponseCache \\x1a\\x37\\n\\x15\\x43\\x63 ModelFilenamesEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a\\x31\\n\\x0f MetricTagsEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a L \\n\\x0f ParametersEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12 ( \\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x19 .inference.ModelParameter: \\x02\\x38\\x01\\x42\\x13\\n\\x11 scheduling_choice* \\xfa\\x01\\n\\x08\\x44\\x61 taType \\x12\\x10\\n\\x0c TYPE_INVALID \\x10\\x00\\x12\\r\\n\\t TYPE_BOOL \\x10\\x01\\x12\\x0e\\n\\n TYPE_UINT8 \\x10\\x02\\x12\\x0f\\n\\x0b TYPE_UINT16 \\x10\\x03\\x12\\x0f\\n\\x0b TYPE_UINT32 \\x10\\x04\\x12\\x0f\\n\\x0b TYPE_UINT64 \\x10\\x05\\x12\\r\\n\\t TYPE_INT8 \\x10\\x06\\x12\\x0e\\n\\n TYPE_INT16 \\x10\\x07\\x12\\x0e\\n\\n TYPE_INT32 \\x10\\x08\\x12\\x0e\\n\\n TYPE_INT64 \\x10\\t\\x12\\r\\n\\t TYPE_FP16 \\x10\\n\\x12\\r\\n\\t TYPE_FP32 \\x10\\x0b\\x12\\r\\n\\t TYPE_FP64 \\x10\\x0c\\x12\\x0f\\n\\x0b TYPE_STRING \\x10\\r\\x12\\r\\n\\t TYPE_BF16 \\x10\\x0e\\x62\\x06 proto3' ) ) _DATATYPE = _descriptor . EnumDescriptor ( name = 'DataType' , full_name = 'inference.DataType' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'TYPE_INVALID' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_BOOL' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT8' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT16' , index = 3 , number = 3 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT32' , index = 4 , number = 4 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT64' , index = 5 , number = 5 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT8' , index = 6 , number = 6 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT16' , index = 7 , number = 7 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT32' , index = 8 , number = 8 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT64' , index = 9 , number = 9 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_FP16' , index = 10 , number = 10 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_FP32' , index = 11 , number = 11 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_FP64' , index = 12 , number = 12 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_STRING' , index = 13 , number = 13 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_BF16' , index = 14 , number = 14 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 8189 , serialized_end = 8439 , ) _sym_db . RegisterEnumDescriptor ( _DATATYPE ) DataType = enum_type_wrapper . EnumTypeWrapper ( _DATATYPE ) TYPE_INVALID = 0 TYPE_BOOL = 1 TYPE_UINT8 = 2 TYPE_UINT16 = 3 TYPE_UINT32 = 4 TYPE_UINT64 = 5 TYPE_INT8 = 6 TYPE_INT16 = 7 TYPE_INT32 = 8 TYPE_INT64 = 9 TYPE_FP16 = 10 TYPE_FP32 = 11 TYPE_FP64 = 12 TYPE_STRING = 13 TYPE_BF16 = 14 _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND = _descriptor . EnumDescriptor ( name = 'SecondaryDeviceKind' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice.SecondaryDeviceKind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'KIND_NVDLA' , index = 0 , number = 0 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 602 , serialized_end = 639 , ) _sym_db . RegisterEnumDescriptor ( _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND ) _MODELINSTANCEGROUP_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.ModelInstanceGroup.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'KIND_AUTO' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'KIND_GPU' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'KIND_CPU' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'KIND_MODEL' , index = 3 , number = 3 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 641 , serialized_end = 706 , ) _sym_db . RegisterEnumDescriptor ( _MODELINSTANCEGROUP_KIND ) _MODELINPUT_FORMAT = _descriptor . EnumDescriptor ( name = 'Format' , full_name = 'inference.ModelInput.Format' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'FORMAT_NONE' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'FORMAT_NHWC' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'FORMAT_NCHW' , index = 2 , number = 2 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 993 , serialized_end = 1052 , ) _sym_db . RegisterEnumDescriptor ( _MODELINPUT_FORMAT ) _BATCHINPUT_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.BatchInput.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'BATCH_ELEMENT_COUNT' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ACCUMULATED_ELEMENT_COUNT' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_MAX_ELEMENT_COUNT_AS_SHAPE' , index = 3 , number = 3 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ITEM_SHAPE' , index = 4 , number = 4 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ITEM_SHAPE_FLATTEN' , index = 5 , number = 5 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 1376 , serialized_end = 1581 , ) _sym_db . RegisterEnumDescriptor ( _BATCHINPUT_KIND ) _BATCHOUTPUT_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.BatchOutput.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'BATCH_SCATTER_WITH_INPUT_SHAPE' , index = 0 , number = 0 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 1685 , serialized_end = 1727 , ) _sym_db . RegisterEnumDescriptor ( _BATCHOUTPUT_KIND ) _MODELOPTIMIZATIONPOLICY_MODELPRIORITY = _descriptor . EnumDescriptor ( name = 'ModelPriority' , full_name = 'inference.ModelOptimizationPolicy.ModelPriority' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'PRIORITY_DEFAULT' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'PRIORITY_MAX' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'PRIORITY_MIN' , index = 2 , number = 2 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 3721 , serialized_end = 3794 , ) _sym_db . RegisterEnumDescriptor ( _MODELOPTIMIZATIONPOLICY_MODELPRIORITY ) _MODELQUEUEPOLICY_TIMEOUTACTION = _descriptor . EnumDescriptor ( name = 'TimeoutAction' , full_name = 'inference.ModelQueuePolicy.TimeoutAction' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'REJECT' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'DELAY' , index = 1 , number = 1 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 3978 , serialized_end = 4016 , ) _sym_db . RegisterEnumDescriptor ( _MODELQUEUEPOLICY_TIMEOUTACTION ) _MODELSEQUENCEBATCHING_CONTROL_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.ModelSequenceBatching.Control.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_START' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_READY' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_END' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_CORRID' , index = 3 , number = 3 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 4946 , serialized_end = 5063 , ) _sym_db . RegisterEnumDescriptor ( _MODELSEQUENCEBATCHING_CONTROL_KIND ) _MODELRATELIMITER_RESOURCE = _descriptor . Descriptor ( name = 'Resource' , full_name = 'inference.ModelRateLimiter.Resource' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelRateLimiter.Resource.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'global' , full_name = 'inference.ModelRateLimiter.Resource.global' , index = 1 , number = 2 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'count' , full_name = 'inference.ModelRateLimiter.Resource.count' , index = 2 , number = 3 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 129 , serialized_end = 184 , ) _MODELRATELIMITER = _descriptor . Descriptor ( name = 'ModelRateLimiter' , full_name = 'inference.ModelRateLimiter' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'resources' , full_name = 'inference.ModelRateLimiter.resources' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority' , full_name = 'inference.ModelRateLimiter.priority' , index = 1 , number = 2 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELRATELIMITER_RESOURCE , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 34 , serialized_end = 184 , ) _MODELINSTANCEGROUP_SECONDARYDEVICE = _descriptor . Descriptor ( name = 'SecondaryDevice' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice.kind' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'device_id' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice.device_id' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 483 , serialized_end = 639 , ) _MODELINSTANCEGROUP = _descriptor . Descriptor ( name = 'ModelInstanceGroup' , full_name = 'inference.ModelInstanceGroup' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelInstanceGroup.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.ModelInstanceGroup.kind' , index = 1 , number = 4 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'count' , full_name = 'inference.ModelInstanceGroup.count' , index = 2 , number = 2 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'rate_limiter' , full_name = 'inference.ModelInstanceGroup.rate_limiter' , index = 3 , number = 6 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'gpus' , full_name = 'inference.ModelInstanceGroup.gpus' , index = 4 , number = 3 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'secondary_devices' , full_name = 'inference.ModelInstanceGroup.secondary_devices' , index = 5 , number = 8 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'profile' , full_name = 'inference.ModelInstanceGroup.profile' , index = 6 , number = 5 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'passive' , full_name = 'inference.ModelInstanceGroup.passive' , index = 7 , number = 7 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'host_policy' , full_name = 'inference.ModelInstanceGroup.host_policy' , index = 8 , number = 9 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELINSTANCEGROUP_SECONDARYDEVICE , ], enum_types = [ _MODELINSTANCEGROUP_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 187 , serialized_end = 706 , ) _MODELTENSORRESHAPE = _descriptor . Descriptor ( name = 'ModelTensorReshape' , full_name = 'inference.ModelTensorReshape' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'shape' , full_name = 'inference.ModelTensorReshape.shape' , index = 0 , number = 1 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 708 , serialized_end = 743 , ) _MODELINPUT = _descriptor . Descriptor ( name = 'ModelInput' , full_name = 'inference.ModelInput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelInput.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelInput.data_type' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'format' , full_name = 'inference.ModelInput.format' , index = 2 , number = 3 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelInput.dims' , index = 3 , number = 4 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'reshape' , full_name = 'inference.ModelInput.reshape' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'is_shape_tensor' , full_name = 'inference.ModelInput.is_shape_tensor' , index = 5 , number = 6 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'allow_ragged_batch' , full_name = 'inference.ModelInput.allow_ragged_batch' , index = 6 , number = 7 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'optional' , full_name = 'inference.ModelInput.optional' , index = 7 , number = 8 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELINPUT_FORMAT , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 746 , serialized_end = 1052 , ) _MODELOUTPUT = _descriptor . Descriptor ( name = 'ModelOutput' , full_name = 'inference.ModelOutput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelOutput.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelOutput.data_type' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelOutput.dims' , index = 2 , number = 3 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'reshape' , full_name = 'inference.ModelOutput.reshape' , index = 3 , number = 5 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'label_filename' , full_name = 'inference.ModelOutput.label_filename' , index = 4 , number = 4 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'is_shape_tensor' , full_name = 'inference.ModelOutput.is_shape_tensor' , index = 5 , number = 6 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1055 , serialized_end = 1233 , ) _BATCHINPUT = _descriptor . Descriptor ( name = 'BatchInput' , full_name = 'inference.BatchInput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.BatchInput.kind' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'target_name' , full_name = 'inference.BatchInput.target_name' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.BatchInput.data_type' , index = 2 , number = 3 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'source_input' , full_name = 'inference.BatchInput.source_input' , index = 3 , number = 4 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _BATCHINPUT_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1236 , serialized_end = 1581 , ) _BATCHOUTPUT = _descriptor . Descriptor ( name = 'BatchOutput' , full_name = 'inference.BatchOutput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'target_name' , full_name = 'inference.BatchOutput.target_name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.BatchOutput.kind' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'source_input' , full_name = 'inference.BatchOutput.source_input' , index = 2 , number = 3 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _BATCHOUTPUT_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1584 , serialized_end = 1727 , ) _MODELVERSIONPOLICY_LATEST = _descriptor . Descriptor ( name = 'Latest' , full_name = 'inference.ModelVersionPolicy.Latest' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'num_versions' , full_name = 'inference.ModelVersionPolicy.Latest.num_versions' , index = 0 , number = 1 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1918 , serialized_end = 1948 , ) _MODELVERSIONPOLICY_ALL = _descriptor . Descriptor ( name = 'All' , full_name = 'inference.ModelVersionPolicy.All' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1950 , serialized_end = 1955 , ) _MODELVERSIONPOLICY_SPECIFIC = _descriptor . Descriptor ( name = 'Specific' , full_name = 'inference.ModelVersionPolicy.Specific' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'versions' , full_name = 'inference.ModelVersionPolicy.Specific.versions' , index = 0 , number = 1 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1957 , serialized_end = 1985 , ) _MODELVERSIONPOLICY = _descriptor . Descriptor ( name = 'ModelVersionPolicy' , full_name = 'inference.ModelVersionPolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'latest' , full_name = 'inference.ModelVersionPolicy.latest' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'all' , full_name = 'inference.ModelVersionPolicy.all' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'specific' , full_name = 'inference.ModelVersionPolicy.specific' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELVERSIONPOLICY_LATEST , _MODELVERSIONPOLICY_ALL , _MODELVERSIONPOLICY_SPECIFIC , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'policy_choice' , full_name = 'inference.ModelVersionPolicy.policy_choice' , index = 0 , containing_type = None , fields = []), ], serialized_start = 1730 , serialized_end = 2002 , ) _MODELOPTIMIZATIONPOLICY_GRAPH = _descriptor . Descriptor ( name = 'Graph' , full_name = 'inference.ModelOptimizationPolicy.Graph' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'level' , full_name = 'inference.ModelOptimizationPolicy.Graph.level' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2535 , serialized_end = 2557 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE = _descriptor . Descriptor ( name = 'Shape' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'dim' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.dim' , index = 0 , number = 1 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2909 , serialized_end = 2929 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY = _descriptor . Descriptor ( name = 'InputEntry' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3054 , serialized_end = 3155 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND = _descriptor . Descriptor ( name = 'LowerBound' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'batch_size' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.batch_size' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.input' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2932 , serialized_end = 3155 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY = _descriptor . Descriptor ( name = 'InputEntry' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3054 , serialized_end = 3155 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC = _descriptor . Descriptor ( name = 'GraphSpec' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'batch_size' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.batch_size' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.input' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'graph_lower_bound' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.graph_lower_bound' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE , _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND , _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2710 , serialized_end = 3258 , ) _MODELOPTIMIZATIONPOLICY_CUDA = _descriptor . Descriptor ( name = 'Cuda' , full_name = 'inference.ModelOptimizationPolicy.Cuda' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'graphs' , full_name = 'inference.ModelOptimizationPolicy.Cuda.graphs' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'busy_wait_events' , full_name = 'inference.ModelOptimizationPolicy.Cuda.busy_wait_events' , index = 1 , number = 2 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'graph_spec' , full_name = 'inference.ModelOptimizationPolicy.Cuda.graph_spec' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_copy_stream' , full_name = 'inference.ModelOptimizationPolicy.Cuda.output_copy_stream' , index = 3 , number = 4 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2560 , serialized_end = 3258 , ) _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY = _descriptor . Descriptor ( name = 'ParametersEntry' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3632 , serialized_end = 3681 , ) _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR = _descriptor . Descriptor ( name = 'Accelerator' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'parameters' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parameters' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3497 , serialized_end = 3681 , ) _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS = _descriptor . Descriptor ( name = 'ExecutionAccelerators' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'gpu_execution_accelerator' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'cpu_execution_accelerator' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3261 , serialized_end = 3681 , ) _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER = _descriptor . Descriptor ( name = 'PinnedMemoryBuffer' , full_name = 'inference.ModelOptimizationPolicy.PinnedMemoryBuffer' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'enable' , full_name = 'inference.ModelOptimizationPolicy.PinnedMemoryBuffer.enable' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3683 , serialized_end = 3719 , ) _MODELOPTIMIZATIONPOLICY = _descriptor . Descriptor ( name = 'ModelOptimizationPolicy' , full_name = 'inference.ModelOptimizationPolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'graph' , full_name = 'inference.ModelOptimizationPolicy.graph' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority' , full_name = 'inference.ModelOptimizationPolicy.priority' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'cuda' , full_name = 'inference.ModelOptimizationPolicy.cuda' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'execution_accelerators' , full_name = 'inference.ModelOptimizationPolicy.execution_accelerators' , index = 3 , number = 4 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input_pinned_memory' , full_name = 'inference.ModelOptimizationPolicy.input_pinned_memory' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_pinned_memory' , full_name = 'inference.ModelOptimizationPolicy.output_pinned_memory' , index = 5 , number = 6 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'gather_kernel_buffer_threshold' , full_name = 'inference.ModelOptimizationPolicy.gather_kernel_buffer_threshold' , index = 6 , number = 7 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'eager_batching' , full_name = 'inference.ModelOptimizationPolicy.eager_batching' , index = 7 , number = 8 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_GRAPH , _MODELOPTIMIZATIONPOLICY_CUDA , _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS , _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER , ], enum_types = [ _MODELOPTIMIZATIONPOLICY_MODELPRIORITY , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2005 , serialized_end = 3794 , ) _MODELQUEUEPOLICY = _descriptor . Descriptor ( name = 'ModelQueuePolicy' , full_name = 'inference.ModelQueuePolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'timeout_action' , full_name = 'inference.ModelQueuePolicy.timeout_action' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_timeout_microseconds' , full_name = 'inference.ModelQueuePolicy.default_timeout_microseconds' , index = 1 , number = 2 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'allow_timeout_override' , full_name = 'inference.ModelQueuePolicy.allow_timeout_override' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_queue_size' , full_name = 'inference.ModelQueuePolicy.max_queue_size' , index = 3 , number = 4 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELQUEUEPOLICY_TIMEOUTACTION , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3797 , serialized_end = 4016 , ) _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY = _descriptor . Descriptor ( name = 'PriorityQueuePolicyEntry' , full_name = 'inference.ModelDynamicBatching.PriorityQueuePolicyEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelDynamicBatching.PriorityQueuePolicyEntry.key' , index = 0 , number = 1 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelDynamicBatching.PriorityQueuePolicyEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 4343 , serialized_end = 4430 , ) _MODELDYNAMICBATCHING = _descriptor . Descriptor ( name = 'ModelDynamicBatching' , full_name = 'inference.ModelDynamicBatching' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'preferred_batch_size' , full_name = 'inference.ModelDynamicBatching.preferred_batch_size' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_queue_delay_microseconds' , full_name = 'inference.ModelDynamicBatching.max_queue_delay_microseconds' , index = 1 , number = 2 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'preserve_ordering' , full_name = 'inference.ModelDynamicBatching.preserve_ordering' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority_levels' , full_name = 'inference.ModelDynamicBatching.priority_levels' , index = 3 , number = 4 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_priority_level' , full_name = 'inference.ModelDynamicBatching.default_priority_level' , index = 4 , number = 5 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_queue_policy' , full_name = 'inference.ModelDynamicBatching.default_queue_policy' , index = 5 , number = 6 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority_queue_policy' , full_name = 'inference.ModelDynamicBatching.priority_queue_policy' , index = 6 , number = 7 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 4019 , serialized_end = 4430 , ) _MODELSEQUENCEBATCHING_CONTROL = _descriptor . Descriptor ( name = 'Control' , full_name = 'inference.ModelSequenceBatching.Control' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.ModelSequenceBatching.Control.kind' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'int32_false_true' , full_name = 'inference.ModelSequenceBatching.Control.int32_false_true' , index = 1 , number = 2 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'fp32_false_true' , full_name = 'inference.ModelSequenceBatching.Control.fp32_false_true' , index = 2 , number = 3 , type = 2 , cpp_type = 6 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'bool_false_true' , full_name = 'inference.ModelSequenceBatching.Control.bool_false_true' , index = 3 , number = 5 , type = 8 , cpp_type = 7 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelSequenceBatching.Control.data_type' , index = 4 , number = 4 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELSEQUENCEBATCHING_CONTROL_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 4758 , serialized_end = 5063 , ) _MODELSEQUENCEBATCHING_CONTROLINPUT = _descriptor . Descriptor ( name = 'ControlInput' , full_name = 'inference.ModelSequenceBatching.ControlInput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelSequenceBatching.ControlInput.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'control' , full_name = 'inference.ModelSequenceBatching.ControlInput.control' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5065 , serialized_end = 5152 , ) _MODELSEQUENCEBATCHING_INITIALSTATE = _descriptor . Descriptor ( name = 'InitialState' , full_name = 'inference.ModelSequenceBatching.InitialState' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelSequenceBatching.InitialState.data_type' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelSequenceBatching.InitialState.dims' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'zero_data' , full_name = 'inference.ModelSequenceBatching.InitialState.zero_data' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_file' , full_name = 'inference.ModelSequenceBatching.InitialState.data_file' , index = 3 , number = 4 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelSequenceBatching.InitialState.name' , index = 4 , number = 5 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'state_data' , full_name = 'inference.ModelSequenceBatching.InitialState.state_data' , index = 0 , containing_type = None , fields = []), ], serialized_start = 5155 , serialized_end = 5293 , ) _MODELSEQUENCEBATCHING_STATE = _descriptor . Descriptor ( name = 'State' , full_name = 'inference.ModelSequenceBatching.State' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'input_name' , full_name = 'inference.ModelSequenceBatching.State.input_name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_name' , full_name = 'inference.ModelSequenceBatching.State.output_name' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelSequenceBatching.State.data_type' , index = 2 , number = 3 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelSequenceBatching.State.dims' , index = 3 , number = 4 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'initial_state' , full_name = 'inference.ModelSequenceBatching.State.initial_state' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5296 , serialized_end = 5468 , ) _MODELSEQUENCEBATCHING_STRATEGYDIRECT = _descriptor . Descriptor ( name = 'StrategyDirect' , full_name = 'inference.ModelSequenceBatching.StrategyDirect' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'max_queue_delay_microseconds' , full_name = 'inference.ModelSequenceBatching.StrategyDirect.max_queue_delay_microseconds' , index = 0 , number = 1 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'minimum_slot_utilization' , full_name = 'inference.ModelSequenceBatching.StrategyDirect.minimum_slot_utilization' , index = 1 , number = 2 , type = 2 , cpp_type = 6 , label = 1 , has_default_value = False , default_value = float ( 0 ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5470 , serialized_end = 5558 , ) _MODELSEQUENCEBATCHING_STRATEGYOLDEST = _descriptor . Descriptor ( name = 'StrategyOldest' , full_name = 'inference.ModelSequenceBatching.StrategyOldest' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'max_candidate_sequences' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.max_candidate_sequences' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'preferred_batch_size' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.preferred_batch_size' , index = 1 , number = 2 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_queue_delay_microseconds' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.max_queue_delay_microseconds' , index = 2 , number = 3 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'preserve_ordering' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.preserve_ordering' , index = 3 , number = 4 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5561 , serialized_end = 5705 , ) _MODELSEQUENCEBATCHING = _descriptor . Descriptor ( name = 'ModelSequenceBatching' , full_name = 'inference.ModelSequenceBatching' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'direct' , full_name = 'inference.ModelSequenceBatching.direct' , index = 0 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'oldest' , full_name = 'inference.ModelSequenceBatching.oldest' , index = 1 , number = 4 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_sequence_idle_microseconds' , full_name = 'inference.ModelSequenceBatching.max_sequence_idle_microseconds' , index = 2 , number = 1 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'control_input' , full_name = 'inference.ModelSequenceBatching.control_input' , index = 3 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'state' , full_name = 'inference.ModelSequenceBatching.state' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELSEQUENCEBATCHING_CONTROL , _MODELSEQUENCEBATCHING_CONTROLINPUT , _MODELSEQUENCEBATCHING_INITIALSTATE , _MODELSEQUENCEBATCHING_STATE , _MODELSEQUENCEBATCHING_STRATEGYDIRECT , _MODELSEQUENCEBATCHING_STRATEGYOLDEST , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'strategy_choice' , full_name = 'inference.ModelSequenceBatching.strategy_choice' , index = 0 , containing_type = None , fields = []), ], serialized_start = 4433 , serialized_end = 5724 , ) _MODELENSEMBLING_STEP_INPUTMAPENTRY = _descriptor . Descriptor ( name = 'InputMapEntry' , full_name = 'inference.ModelEnsembling.Step.InputMapEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelEnsembling.Step.InputMapEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelEnsembling.Step.InputMapEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6004 , serialized_end = 6051 , ) _MODELENSEMBLING_STEP_OUTPUTMAPENTRY = _descriptor . Descriptor ( name = 'OutputMapEntry' , full_name = 'inference.ModelEnsembling.Step.OutputMapEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelEnsembling.Step.OutputMapEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelEnsembling.Step.OutputMapEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6053 , serialized_end = 6101 , ) _MODELENSEMBLING_STEP = _descriptor . Descriptor ( name = 'Step' , full_name = 'inference.ModelEnsembling.Step' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'model_name' , full_name = 'inference.ModelEnsembling.Step.model_name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_version' , full_name = 'inference.ModelEnsembling.Step.model_version' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input_map' , full_name = 'inference.ModelEnsembling.Step.input_map' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_map' , full_name = 'inference.ModelEnsembling.Step.output_map' , index = 3 , number = 4 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_namespace' , full_name = 'inference.ModelEnsembling.Step.model_namespace' , index = 4 , number = 5 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELENSEMBLING_STEP_INPUTMAPENTRY , _MODELENSEMBLING_STEP_OUTPUTMAPENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5794 , serialized_end = 6101 , ) _MODELENSEMBLING = _descriptor . Descriptor ( name = 'ModelEnsembling' , full_name = 'inference.ModelEnsembling' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'step' , full_name = 'inference.ModelEnsembling.step' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELENSEMBLING_STEP , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5727 , serialized_end = 6101 , ) _MODELPARAMETER = _descriptor . Descriptor ( name = 'ModelParameter' , full_name = 'inference.ModelParameter' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'string_value' , full_name = 'inference.ModelParameter.string_value' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6103 , serialized_end = 6141 , ) _MODELWARMUP_INPUT = _descriptor . Descriptor ( name = 'Input' , full_name = 'inference.ModelWarmup.Input' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelWarmup.Input.data_type' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelWarmup.Input.dims' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'zero_data' , full_name = 'inference.ModelWarmup.Input.zero_data' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'random_data' , full_name = 'inference.ModelWarmup.Input.random_data' , index = 3 , number = 4 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input_data_file' , full_name = 'inference.ModelWarmup.Input.input_data_file' , index = 4 , number = 5 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'input_data_type' , full_name = 'inference.ModelWarmup.Input.input_data_type' , index = 0 , containing_type = None , fields = []), ], serialized_start = 6261 , serialized_end = 6412 , ) _MODELWARMUP_INPUTSENTRY = _descriptor . Descriptor ( name = 'InputsEntry' , full_name = 'inference.ModelWarmup.InputsEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelWarmup.InputsEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelWarmup.InputsEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6414 , serialized_end = 6489 , ) _MODELWARMUP = _descriptor . Descriptor ( name = 'ModelWarmup' , full_name = 'inference.ModelWarmup' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelWarmup.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'batch_size' , full_name = 'inference.ModelWarmup.batch_size' , index = 1 , number = 2 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'inputs' , full_name = 'inference.ModelWarmup.inputs' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'count' , full_name = 'inference.ModelWarmup.count' , index = 3 , number = 4 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELWARMUP_INPUT , _MODELWARMUP_INPUTSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6144 , serialized_end = 6489 , ) _MODELOPERATIONS = _descriptor . Descriptor ( name = 'ModelOperations' , full_name = 'inference.ModelOperations' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'op_library_filename' , full_name = 'inference.ModelOperations.op_library_filename' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6491 , serialized_end = 6537 , ) _MODELTRANSACTIONPOLICY = _descriptor . Descriptor ( name = 'ModelTransactionPolicy' , full_name = 'inference.ModelTransactionPolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'decoupled' , full_name = 'inference.ModelTransactionPolicy.decoupled' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6539 , serialized_end = 6582 , ) _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY = _descriptor . Descriptor ( name = 'ParametersEntry' , full_name = 'inference.ModelRepositoryAgents.Agent.ParametersEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelRepositoryAgents.Agent.ParametersEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelRepositoryAgents.Agent.ParametersEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3632 , serialized_end = 3681 , ) _MODELREPOSITORYAGENTS_AGENT = _descriptor . Descriptor ( name = 'Agent' , full_name = 'inference.ModelRepositoryAgents.Agent' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelRepositoryAgents.Agent.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'parameters' , full_name = 'inference.ModelRepositoryAgents.Agent.parameters' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6667 , serialized_end = 6815 , ) _MODELREPOSITORYAGENTS = _descriptor . Descriptor ( name = 'ModelRepositoryAgents' , full_name = 'inference.ModelRepositoryAgents' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'agents' , full_name = 'inference.ModelRepositoryAgents.agents' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELREPOSITORYAGENTS_AGENT , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6585 , serialized_end = 6815 , ) _MODELRESPONSECACHE = _descriptor . Descriptor ( name = 'ModelResponseCache' , full_name = 'inference.ModelResponseCache' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'enable' , full_name = 'inference.ModelResponseCache.enable' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6817 , serialized_end = 6853 , ) _MODELCONFIG_CCMODELFILENAMESENTRY = _descriptor . Descriptor ( name = 'CcModelFilenamesEntry' , full_name = 'inference.ModelConfig.CcModelFilenamesEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelConfig.CcModelFilenamesEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelConfig.CcModelFilenamesEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 7981 , serialized_end = 8036 , ) _MODELCONFIG_METRICTAGSENTRY = _descriptor . Descriptor ( name = 'MetricTagsEntry' , full_name = 'inference.ModelConfig.MetricTagsEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelConfig.MetricTagsEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelConfig.MetricTagsEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 8038 , serialized_end = 8087 , ) _MODELCONFIG_PARAMETERSENTRY = _descriptor . Descriptor ( name = 'ParametersEntry' , full_name = 'inference.ModelConfig.ParametersEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelConfig.ParametersEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelConfig.ParametersEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 8089 , serialized_end = 8165 , ) _MODELCONFIG = _descriptor . Descriptor ( name = 'ModelConfig' , full_name = 'inference.ModelConfig' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelConfig.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'platform' , full_name = 'inference.ModelConfig.platform' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'backend' , full_name = 'inference.ModelConfig.backend' , index = 2 , number = 17 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'version_policy' , full_name = 'inference.ModelConfig.version_policy' , index = 3 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_batch_size' , full_name = 'inference.ModelConfig.max_batch_size' , index = 4 , number = 4 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input' , full_name = 'inference.ModelConfig.input' , index = 5 , number = 5 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output' , full_name = 'inference.ModelConfig.output' , index = 6 , number = 6 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'batch_input' , full_name = 'inference.ModelConfig.batch_input' , index = 7 , number = 20 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'batch_output' , full_name = 'inference.ModelConfig.batch_output' , index = 8 , number = 21 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'optimization' , full_name = 'inference.ModelConfig.optimization' , index = 9 , number = 12 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dynamic_batching' , full_name = 'inference.ModelConfig.dynamic_batching' , index = 10 , number = 11 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'sequence_batching' , full_name = 'inference.ModelConfig.sequence_batching' , index = 11 , number = 13 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'ensemble_scheduling' , full_name = 'inference.ModelConfig.ensemble_scheduling' , index = 12 , number = 15 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'instance_group' , full_name = 'inference.ModelConfig.instance_group' , index = 13 , number = 7 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_model_filename' , full_name = 'inference.ModelConfig.default_model_filename' , index = 14 , number = 8 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'cc_model_filenames' , full_name = 'inference.ModelConfig.cc_model_filenames' , index = 15 , number = 9 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'metric_tags' , full_name = 'inference.ModelConfig.metric_tags' , index = 16 , number = 10 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'parameters' , full_name = 'inference.ModelConfig.parameters' , index = 17 , number = 14 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_warmup' , full_name = 'inference.ModelConfig.model_warmup' , index = 18 , number = 16 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_operations' , full_name = 'inference.ModelConfig.model_operations' , index = 19 , number = 18 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_transaction_policy' , full_name = 'inference.ModelConfig.model_transaction_policy' , index = 20 , number = 19 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_repository_agents' , full_name = 'inference.ModelConfig.model_repository_agents' , index = 21 , number = 23 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'response_cache' , full_name = 'inference.ModelConfig.response_cache' , index = 22 , number = 24 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELCONFIG_CCMODELFILENAMESENTRY , _MODELCONFIG_METRICTAGSENTRY , _MODELCONFIG_PARAMETERSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'scheduling_choice' , full_name = 'inference.ModelConfig.scheduling_choice' , index = 0 , containing_type = None , fields = []), ], serialized_start = 6856 , serialized_end = 8186 , ) _MODELRATELIMITER_RESOURCE . containing_type = _MODELRATELIMITER _MODELRATELIMITER . fields_by_name [ 'resources' ] . message_type = _MODELRATELIMITER_RESOURCE _MODELINSTANCEGROUP_SECONDARYDEVICE . fields_by_name [ 'kind' ] . enum_type = _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND _MODELINSTANCEGROUP_SECONDARYDEVICE . containing_type = _MODELINSTANCEGROUP _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND . containing_type = _MODELINSTANCEGROUP_SECONDARYDEVICE _MODELINSTANCEGROUP . fields_by_name [ 'kind' ] . enum_type = _MODELINSTANCEGROUP_KIND _MODELINSTANCEGROUP . fields_by_name [ 'rate_limiter' ] . message_type = _MODELRATELIMITER _MODELINSTANCEGROUP . fields_by_name [ 'secondary_devices' ] . message_type = _MODELINSTANCEGROUP_SECONDARYDEVICE _MODELINSTANCEGROUP_KIND . containing_type = _MODELINSTANCEGROUP _MODELINPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELINPUT . fields_by_name [ 'format' ] . enum_type = _MODELINPUT_FORMAT _MODELINPUT . fields_by_name [ 'reshape' ] . message_type = _MODELTENSORRESHAPE _MODELINPUT_FORMAT . containing_type = _MODELINPUT _MODELOUTPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELOUTPUT . fields_by_name [ 'reshape' ] . message_type = _MODELTENSORRESHAPE _BATCHINPUT . fields_by_name [ 'kind' ] . enum_type = _BATCHINPUT_KIND _BATCHINPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _BATCHINPUT_KIND . containing_type = _BATCHINPUT _BATCHOUTPUT . fields_by_name [ 'kind' ] . enum_type = _BATCHOUTPUT_KIND _BATCHOUTPUT_KIND . containing_type = _BATCHOUTPUT _MODELVERSIONPOLICY_LATEST . containing_type = _MODELVERSIONPOLICY _MODELVERSIONPOLICY_ALL . containing_type = _MODELVERSIONPOLICY _MODELVERSIONPOLICY_SPECIFIC . containing_type = _MODELVERSIONPOLICY _MODELVERSIONPOLICY . fields_by_name [ 'latest' ] . message_type = _MODELVERSIONPOLICY_LATEST _MODELVERSIONPOLICY . fields_by_name [ 'all' ] . message_type = _MODELVERSIONPOLICY_ALL _MODELVERSIONPOLICY . fields_by_name [ 'specific' ] . message_type = _MODELVERSIONPOLICY_SPECIFIC _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] . fields . append ( _MODELVERSIONPOLICY . fields_by_name [ 'latest' ]) _MODELVERSIONPOLICY . fields_by_name [ 'latest' ] . containing_oneof = _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] . fields . append ( _MODELVERSIONPOLICY . fields_by_name [ 'all' ]) _MODELVERSIONPOLICY . fields_by_name [ 'all' ] . containing_oneof = _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] . fields . append ( _MODELVERSIONPOLICY . fields_by_name [ 'specific' ]) _MODELVERSIONPOLICY . fields_by_name [ 'specific' ] . containing_oneof = _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] _MODELOPTIMIZATIONPOLICY_GRAPH . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY . fields_by_name [ 'value' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND . fields_by_name [ 'input' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY . fields_by_name [ 'value' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC . fields_by_name [ 'input' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC . fields_by_name [ 'graph_lower_bound' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA _MODELOPTIMIZATIONPOLICY_CUDA . fields_by_name [ 'graph_spec' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY . containing_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR . fields_by_name [ 'parameters' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR . containing_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS . fields_by_name [ 'gpu_execution_accelerator' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS . fields_by_name [ 'cpu_execution_accelerator' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'graph' ] . message_type = _MODELOPTIMIZATIONPOLICY_GRAPH _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'priority' ] . enum_type = _MODELOPTIMIZATIONPOLICY_MODELPRIORITY _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'cuda' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'execution_accelerators' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'input_pinned_memory' ] . message_type = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'output_pinned_memory' ] . message_type = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER _MODELOPTIMIZATIONPOLICY_MODELPRIORITY . containing_type = _MODELOPTIMIZATIONPOLICY _MODELQUEUEPOLICY . fields_by_name [ 'timeout_action' ] . enum_type = _MODELQUEUEPOLICY_TIMEOUTACTION _MODELQUEUEPOLICY_TIMEOUTACTION . containing_type = _MODELQUEUEPOLICY _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY . fields_by_name [ 'value' ] . message_type = _MODELQUEUEPOLICY _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY . containing_type = _MODELDYNAMICBATCHING _MODELDYNAMICBATCHING . fields_by_name [ 'default_queue_policy' ] . message_type = _MODELQUEUEPOLICY _MODELDYNAMICBATCHING . fields_by_name [ 'priority_queue_policy' ] . message_type = _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY _MODELSEQUENCEBATCHING_CONTROL . fields_by_name [ 'kind' ] . enum_type = _MODELSEQUENCEBATCHING_CONTROL_KIND _MODELSEQUENCEBATCHING_CONTROL . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELSEQUENCEBATCHING_CONTROL . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_CONTROL_KIND . containing_type = _MODELSEQUENCEBATCHING_CONTROL _MODELSEQUENCEBATCHING_CONTROLINPUT . fields_by_name [ 'control' ] . message_type = _MODELSEQUENCEBATCHING_CONTROL _MODELSEQUENCEBATCHING_CONTROLINPUT . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELSEQUENCEBATCHING_INITIALSTATE . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] . fields . append ( _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'zero_data' ]) _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'zero_data' ] . containing_oneof = _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] . fields . append ( _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'data_file' ]) _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'data_file' ] . containing_oneof = _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] _MODELSEQUENCEBATCHING_STATE . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELSEQUENCEBATCHING_STATE . fields_by_name [ 'initial_state' ] . message_type = _MODELSEQUENCEBATCHING_INITIALSTATE _MODELSEQUENCEBATCHING_STATE . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_STRATEGYDIRECT . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_STRATEGYOLDEST . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING . fields_by_name [ 'direct' ] . message_type = _MODELSEQUENCEBATCHING_STRATEGYDIRECT _MODELSEQUENCEBATCHING . fields_by_name [ 'oldest' ] . message_type = _MODELSEQUENCEBATCHING_STRATEGYOLDEST _MODELSEQUENCEBATCHING . fields_by_name [ 'control_input' ] . message_type = _MODELSEQUENCEBATCHING_CONTROLINPUT _MODELSEQUENCEBATCHING . fields_by_name [ 'state' ] . message_type = _MODELSEQUENCEBATCHING_STATE _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] . fields . append ( _MODELSEQUENCEBATCHING . fields_by_name [ 'direct' ]) _MODELSEQUENCEBATCHING . fields_by_name [ 'direct' ] . containing_oneof = _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] . fields . append ( _MODELSEQUENCEBATCHING . fields_by_name [ 'oldest' ]) _MODELSEQUENCEBATCHING . fields_by_name [ 'oldest' ] . containing_oneof = _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] _MODELENSEMBLING_STEP_INPUTMAPENTRY . containing_type = _MODELENSEMBLING_STEP _MODELENSEMBLING_STEP_OUTPUTMAPENTRY . containing_type = _MODELENSEMBLING_STEP _MODELENSEMBLING_STEP . fields_by_name [ 'input_map' ] . message_type = _MODELENSEMBLING_STEP_INPUTMAPENTRY _MODELENSEMBLING_STEP . fields_by_name [ 'output_map' ] . message_type = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY _MODELENSEMBLING_STEP . containing_type = _MODELENSEMBLING _MODELENSEMBLING . fields_by_name [ 'step' ] . message_type = _MODELENSEMBLING_STEP _MODELWARMUP_INPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELWARMUP_INPUT . containing_type = _MODELWARMUP _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] . fields . append ( _MODELWARMUP_INPUT . fields_by_name [ 'zero_data' ]) _MODELWARMUP_INPUT . fields_by_name [ 'zero_data' ] . containing_oneof = _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] . fields . append ( _MODELWARMUP_INPUT . fields_by_name [ 'random_data' ]) _MODELWARMUP_INPUT . fields_by_name [ 'random_data' ] . containing_oneof = _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] . fields . append ( _MODELWARMUP_INPUT . fields_by_name [ 'input_data_file' ]) _MODELWARMUP_INPUT . fields_by_name [ 'input_data_file' ] . containing_oneof = _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] _MODELWARMUP_INPUTSENTRY . fields_by_name [ 'value' ] . message_type = _MODELWARMUP_INPUT _MODELWARMUP_INPUTSENTRY . containing_type = _MODELWARMUP _MODELWARMUP . fields_by_name [ 'inputs' ] . message_type = _MODELWARMUP_INPUTSENTRY _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY . containing_type = _MODELREPOSITORYAGENTS_AGENT _MODELREPOSITORYAGENTS_AGENT . fields_by_name [ 'parameters' ] . message_type = _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY _MODELREPOSITORYAGENTS_AGENT . containing_type = _MODELREPOSITORYAGENTS _MODELREPOSITORYAGENTS . fields_by_name [ 'agents' ] . message_type = _MODELREPOSITORYAGENTS_AGENT _MODELCONFIG_CCMODELFILENAMESENTRY . containing_type = _MODELCONFIG _MODELCONFIG_METRICTAGSENTRY . containing_type = _MODELCONFIG _MODELCONFIG_PARAMETERSENTRY . fields_by_name [ 'value' ] . message_type = _MODELPARAMETER _MODELCONFIG_PARAMETERSENTRY . containing_type = _MODELCONFIG _MODELCONFIG . fields_by_name [ 'version_policy' ] . message_type = _MODELVERSIONPOLICY _MODELCONFIG . fields_by_name [ 'input' ] . message_type = _MODELINPUT _MODELCONFIG . fields_by_name [ 'output' ] . message_type = _MODELOUTPUT _MODELCONFIG . fields_by_name [ 'batch_input' ] . message_type = _BATCHINPUT _MODELCONFIG . fields_by_name [ 'batch_output' ] . message_type = _BATCHOUTPUT _MODELCONFIG . fields_by_name [ 'optimization' ] . message_type = _MODELOPTIMIZATIONPOLICY _MODELCONFIG . fields_by_name [ 'dynamic_batching' ] . message_type = _MODELDYNAMICBATCHING _MODELCONFIG . fields_by_name [ 'sequence_batching' ] . message_type = _MODELSEQUENCEBATCHING _MODELCONFIG . fields_by_name [ 'ensemble_scheduling' ] . message_type = _MODELENSEMBLING _MODELCONFIG . fields_by_name [ 'instance_group' ] . message_type = _MODELINSTANCEGROUP _MODELCONFIG . fields_by_name [ 'cc_model_filenames' ] . message_type = _MODELCONFIG_CCMODELFILENAMESENTRY _MODELCONFIG . fields_by_name [ 'metric_tags' ] . message_type = _MODELCONFIG_METRICTAGSENTRY _MODELCONFIG . fields_by_name [ 'parameters' ] . message_type = _MODELCONFIG_PARAMETERSENTRY _MODELCONFIG . fields_by_name [ 'model_warmup' ] . message_type = _MODELWARMUP _MODELCONFIG . fields_by_name [ 'model_operations' ] . message_type = _MODELOPERATIONS _MODELCONFIG . fields_by_name [ 'model_transaction_policy' ] . message_type = _MODELTRANSACTIONPOLICY _MODELCONFIG . fields_by_name [ 'model_repository_agents' ] . message_type = _MODELREPOSITORYAGENTS _MODELCONFIG . fields_by_name [ 'response_cache' ] . message_type = _MODELRESPONSECACHE _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] . fields . append ( _MODELCONFIG . fields_by_name [ 'dynamic_batching' ]) _MODELCONFIG . fields_by_name [ 'dynamic_batching' ] . containing_oneof = _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] . fields . append ( _MODELCONFIG . fields_by_name [ 'sequence_batching' ]) _MODELCONFIG . fields_by_name [ 'sequence_batching' ] . containing_oneof = _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] . fields . append ( _MODELCONFIG . fields_by_name [ 'ensemble_scheduling' ]) _MODELCONFIG . fields_by_name [ 'ensemble_scheduling' ] . containing_oneof = _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] DESCRIPTOR . message_types_by_name [ 'ModelRateLimiter' ] = _MODELRATELIMITER DESCRIPTOR . message_types_by_name [ 'ModelInstanceGroup' ] = _MODELINSTANCEGROUP DESCRIPTOR . message_types_by_name [ 'ModelTensorReshape' ] = _MODELTENSORRESHAPE DESCRIPTOR . message_types_by_name [ 'ModelInput' ] = _MODELINPUT DESCRIPTOR . message_types_by_name [ 'ModelOutput' ] = _MODELOUTPUT DESCRIPTOR . message_types_by_name [ 'BatchInput' ] = _BATCHINPUT DESCRIPTOR . message_types_by_name [ 'BatchOutput' ] = _BATCHOUTPUT DESCRIPTOR . message_types_by_name [ 'ModelVersionPolicy' ] = _MODELVERSIONPOLICY DESCRIPTOR . message_types_by_name [ 'ModelOptimizationPolicy' ] = _MODELOPTIMIZATIONPOLICY DESCRIPTOR . message_types_by_name [ 'ModelQueuePolicy' ] = _MODELQUEUEPOLICY DESCRIPTOR . message_types_by_name [ 'ModelDynamicBatching' ] = _MODELDYNAMICBATCHING DESCRIPTOR . message_types_by_name [ 'ModelSequenceBatching' ] = _MODELSEQUENCEBATCHING DESCRIPTOR . message_types_by_name [ 'ModelEnsembling' ] = _MODELENSEMBLING DESCRIPTOR . message_types_by_name [ 'ModelParameter' ] = _MODELPARAMETER DESCRIPTOR . message_types_by_name [ 'ModelWarmup' ] = _MODELWARMUP DESCRIPTOR . message_types_by_name [ 'ModelOperations' ] = _MODELOPERATIONS DESCRIPTOR . message_types_by_name [ 'ModelTransactionPolicy' ] = _MODELTRANSACTIONPOLICY DESCRIPTOR . message_types_by_name [ 'ModelRepositoryAgents' ] = _MODELREPOSITORYAGENTS DESCRIPTOR . message_types_by_name [ 'ModelResponseCache' ] = _MODELRESPONSECACHE DESCRIPTOR . message_types_by_name [ 'ModelConfig' ] = _MODELCONFIG DESCRIPTOR . enum_types_by_name [ 'DataType' ] = _DATATYPE _sym_db . RegisterFileDescriptor ( DESCRIPTOR ) ModelRateLimiter = _reflection . GeneratedProtocolMessageType ( 'ModelRateLimiter' , ( _message . Message ,), dict ( Resource = _reflection . GeneratedProtocolMessageType ( 'Resource' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELRATELIMITER_RESOURCE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRateLimiter.Resource) )) , DESCRIPTOR = _MODELRATELIMITER , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRateLimiter) )) _sym_db . RegisterMessage ( ModelRateLimiter ) _sym_db . RegisterMessage ( ModelRateLimiter . Resource ) ModelInstanceGroup = _reflection . GeneratedProtocolMessageType ( 'ModelInstanceGroup' , ( _message . Message ,), dict ( SecondaryDevice = _reflection . GeneratedProtocolMessageType ( 'SecondaryDevice' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELINSTANCEGROUP_SECONDARYDEVICE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelInstanceGroup.SecondaryDevice) )) , DESCRIPTOR = _MODELINSTANCEGROUP , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelInstanceGroup) )) _sym_db . RegisterMessage ( ModelInstanceGroup ) _sym_db . RegisterMessage ( ModelInstanceGroup . SecondaryDevice ) ModelTensorReshape = _reflection . GeneratedProtocolMessageType ( 'ModelTensorReshape' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELTENSORRESHAPE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelTensorReshape) )) _sym_db . RegisterMessage ( ModelTensorReshape ) ModelInput = _reflection . GeneratedProtocolMessageType ( 'ModelInput' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELINPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelInput) )) _sym_db . RegisterMessage ( ModelInput ) ModelOutput = _reflection . GeneratedProtocolMessageType ( 'ModelOutput' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOUTPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOutput) )) _sym_db . RegisterMessage ( ModelOutput ) BatchInput = _reflection . GeneratedProtocolMessageType ( 'BatchInput' , ( _message . Message ,), dict ( DESCRIPTOR = _BATCHINPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.BatchInput) )) _sym_db . RegisterMessage ( BatchInput ) BatchOutput = _reflection . GeneratedProtocolMessageType ( 'BatchOutput' , ( _message . Message ,), dict ( DESCRIPTOR = _BATCHOUTPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.BatchOutput) )) _sym_db . RegisterMessage ( BatchOutput ) ModelVersionPolicy = _reflection . GeneratedProtocolMessageType ( 'ModelVersionPolicy' , ( _message . Message ,), dict ( Latest = _reflection . GeneratedProtocolMessageType ( 'Latest' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELVERSIONPOLICY_LATEST , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Latest) )) , All = _reflection . GeneratedProtocolMessageType ( 'All' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELVERSIONPOLICY_ALL , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.All) )) , Specific = _reflection . GeneratedProtocolMessageType ( 'Specific' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELVERSIONPOLICY_SPECIFIC , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Specific) )) , DESCRIPTOR = _MODELVERSIONPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy) )) _sym_db . RegisterMessage ( ModelVersionPolicy ) _sym_db . RegisterMessage ( ModelVersionPolicy . Latest ) _sym_db . RegisterMessage ( ModelVersionPolicy . All ) _sym_db . RegisterMessage ( ModelVersionPolicy . Specific ) ModelOptimizationPolicy = _reflection . GeneratedProtocolMessageType ( 'ModelOptimizationPolicy' , ( _message . Message ,), dict ( Graph = _reflection . GeneratedProtocolMessageType ( 'Graph' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_GRAPH , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Graph) )) , Cuda = _reflection . GeneratedProtocolMessageType ( 'Cuda' , ( _message . Message ,), dict ( GraphSpec = _reflection . GeneratedProtocolMessageType ( 'GraphSpec' , ( _message . Message ,), dict ( Shape = _reflection . GeneratedProtocolMessageType ( 'Shape' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape) )) , LowerBound = _reflection . GeneratedProtocolMessageType ( 'LowerBound' , ( _message . Message ,), dict ( InputEntry = _reflection . GeneratedProtocolMessageType ( 'InputEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound) )) , InputEntry = _reflection . GeneratedProtocolMessageType ( 'InputEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda) )) , ExecutionAccelerators = _reflection . GeneratedProtocolMessageType ( 'ExecutionAccelerators' , ( _message . Message ,), dict ( Accelerator = _reflection . GeneratedProtocolMessageType ( 'Accelerator' , ( _message . Message ,), dict ( ParametersEntry = _reflection . GeneratedProtocolMessageType ( 'ParametersEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators) )) , PinnedMemoryBuffer = _reflection . GeneratedProtocolMessageType ( 'PinnedMemoryBuffer' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.PinnedMemoryBuffer) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy) )) _sym_db . RegisterMessage ( ModelOptimizationPolicy ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Graph ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . Shape ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . LowerBound ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . LowerBound . InputEntry ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . InputEntry ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . ExecutionAccelerators ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . ExecutionAccelerators . Accelerator ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . ExecutionAccelerators . Accelerator . ParametersEntry ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . PinnedMemoryBuffer ) ModelQueuePolicy = _reflection . GeneratedProtocolMessageType ( 'ModelQueuePolicy' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELQUEUEPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelQueuePolicy) )) _sym_db . RegisterMessage ( ModelQueuePolicy ) ModelDynamicBatching = _reflection . GeneratedProtocolMessageType ( 'ModelDynamicBatching' , ( _message . Message ,), dict ( PriorityQueuePolicyEntry = _reflection . GeneratedProtocolMessageType ( 'PriorityQueuePolicyEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelDynamicBatching.PriorityQueuePolicyEntry) )) , DESCRIPTOR = _MODELDYNAMICBATCHING , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelDynamicBatching) )) _sym_db . RegisterMessage ( ModelDynamicBatching ) _sym_db . RegisterMessage ( ModelDynamicBatching . PriorityQueuePolicyEntry ) ModelSequenceBatching = _reflection . GeneratedProtocolMessageType ( 'ModelSequenceBatching' , ( _message . Message ,), dict ( Control = _reflection . GeneratedProtocolMessageType ( 'Control' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROL , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.Control) )) , ControlInput = _reflection . GeneratedProtocolMessageType ( 'ControlInput' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROLINPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.ControlInput) )) , InitialState = _reflection . GeneratedProtocolMessageType ( 'InitialState' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_INITIALSTATE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.InitialState) )) , State = _reflection . GeneratedProtocolMessageType ( 'State' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_STATE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.State) )) , StrategyDirect = _reflection . GeneratedProtocolMessageType ( 'StrategyDirect' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_STRATEGYDIRECT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyDirect) )) , StrategyOldest = _reflection . GeneratedProtocolMessageType ( 'StrategyOldest' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_STRATEGYOLDEST , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyOldest) )) , DESCRIPTOR = _MODELSEQUENCEBATCHING , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching) )) _sym_db . RegisterMessage ( ModelSequenceBatching ) _sym_db . RegisterMessage ( ModelSequenceBatching . Control ) _sym_db . RegisterMessage ( ModelSequenceBatching . ControlInput ) _sym_db . RegisterMessage ( ModelSequenceBatching . InitialState ) _sym_db . RegisterMessage ( ModelSequenceBatching . State ) _sym_db . RegisterMessage ( ModelSequenceBatching . StrategyDirect ) _sym_db . RegisterMessage ( ModelSequenceBatching . StrategyOldest ) ModelEnsembling = _reflection . GeneratedProtocolMessageType ( 'ModelEnsembling' , ( _message . Message ,), dict ( Step = _reflection . GeneratedProtocolMessageType ( 'Step' , ( _message . Message ,), dict ( InputMapEntry = _reflection . GeneratedProtocolMessageType ( 'InputMapEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELENSEMBLING_STEP_INPUTMAPENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step.InputMapEntry) )) , OutputMapEntry = _reflection . GeneratedProtocolMessageType ( 'OutputMapEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step.OutputMapEntry) )) , DESCRIPTOR = _MODELENSEMBLING_STEP , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step) )) , DESCRIPTOR = _MODELENSEMBLING , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling) )) _sym_db . RegisterMessage ( ModelEnsembling ) _sym_db . RegisterMessage ( ModelEnsembling . Step ) _sym_db . RegisterMessage ( ModelEnsembling . Step . InputMapEntry ) _sym_db . RegisterMessage ( ModelEnsembling . Step . OutputMapEntry ) ModelParameter = _reflection . GeneratedProtocolMessageType ( 'ModelParameter' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELPARAMETER , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelParameter) )) _sym_db . RegisterMessage ( ModelParameter ) ModelWarmup = _reflection . GeneratedProtocolMessageType ( 'ModelWarmup' , ( _message . Message ,), dict ( Input = _reflection . GeneratedProtocolMessageType ( 'Input' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELWARMUP_INPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelWarmup.Input) )) , InputsEntry = _reflection . GeneratedProtocolMessageType ( 'InputsEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELWARMUP_INPUTSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelWarmup.InputsEntry) )) , DESCRIPTOR = _MODELWARMUP , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelWarmup) )) _sym_db . RegisterMessage ( ModelWarmup ) _sym_db . RegisterMessage ( ModelWarmup . Input ) _sym_db . RegisterMessage ( ModelWarmup . InputsEntry ) ModelOperations = _reflection . GeneratedProtocolMessageType ( 'ModelOperations' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPERATIONS , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOperations) )) _sym_db . RegisterMessage ( ModelOperations ) ModelTransactionPolicy = _reflection . GeneratedProtocolMessageType ( 'ModelTransactionPolicy' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELTRANSACTIONPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelTransactionPolicy) )) _sym_db . RegisterMessage ( ModelTransactionPolicy ) ModelRepositoryAgents = _reflection . GeneratedProtocolMessageType ( 'ModelRepositoryAgents' , ( _message . Message ,), dict ( Agent = _reflection . GeneratedProtocolMessageType ( 'Agent' , ( _message . Message ,), dict ( ParametersEntry = _reflection . GeneratedProtocolMessageType ( 'ParametersEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents.Agent.ParametersEntry) )) , DESCRIPTOR = _MODELREPOSITORYAGENTS_AGENT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents.Agent) )) , DESCRIPTOR = _MODELREPOSITORYAGENTS , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents) )) _sym_db . RegisterMessage ( ModelRepositoryAgents ) _sym_db . RegisterMessage ( ModelRepositoryAgents . Agent ) _sym_db . RegisterMessage ( ModelRepositoryAgents . Agent . ParametersEntry ) ModelResponseCache = _reflection . GeneratedProtocolMessageType ( 'ModelResponseCache' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELRESPONSECACHE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelResponseCache) )) _sym_db . RegisterMessage ( ModelResponseCache ) ModelConfig = _reflection . GeneratedProtocolMessageType ( 'ModelConfig' , ( _message . Message ,), dict ( CcModelFilenamesEntry = _reflection . GeneratedProtocolMessageType ( 'CcModelFilenamesEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELCONFIG_CCMODELFILENAMESENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig.CcModelFilenamesEntry) )) , MetricTagsEntry = _reflection . GeneratedProtocolMessageType ( 'MetricTagsEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELCONFIG_METRICTAGSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig.MetricTagsEntry) )) , ParametersEntry = _reflection . GeneratedProtocolMessageType ( 'ParametersEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELCONFIG_PARAMETERSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig.ParametersEntry) )) , DESCRIPTOR = _MODELCONFIG , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig) )) _sym_db . RegisterMessage ( ModelConfig ) _sym_db . RegisterMessage ( ModelConfig . CcModelFilenamesEntry ) _sym_db . RegisterMessage ( ModelConfig . MetricTagsEntry ) _sym_db . RegisterMessage ( ModelConfig . ParametersEntry ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY . _options = None _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY . _options = None _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY . _options = None _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY . _options = None _MODELENSEMBLING_STEP_INPUTMAPENTRY . _options = None _MODELENSEMBLING_STEP_OUTPUTMAPENTRY . _options = None _MODELWARMUP_INPUTSENTRY . _options = None _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY . _options = None _MODELCONFIG_CCMODELFILENAMESENTRY . _options = None _MODELCONFIG_METRICTAGSENTRY . _options = None _MODELCONFIG_PARAMETERSENTRY . _options = None # @@protoc_insertion_point(module_scope) Variables DESCRIPTOR DataType TYPE_BF16 TYPE_BOOL TYPE_FP16 TYPE_FP32 TYPE_FP64 TYPE_INT16 TYPE_INT32 TYPE_INT64 TYPE_INT8 TYPE_INVALID TYPE_STRING TYPE_UINT16 TYPE_UINT32 TYPE_UINT64 TYPE_UINT8 Classes BatchInput A ProtocolMessage class BatchInput ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables BATCH_ACCUMULATED_ELEMENT_COUNT BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO BATCH_ELEMENT_COUNT BATCH_ITEM_SHAPE BATCH_ITEM_SHAPE_FLATTEN BATCH_MAX_ELEMENT_COUNT_AS_SHAPE DESCRIPTOR Extensions Kind data_type kind source_input target_name Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. BatchOutput A ProtocolMessage class BatchOutput ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables BATCH_SCATTER_WITH_INPUT_SHAPE DESCRIPTOR Extensions Kind kind source_input target_name Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelConfig A ProtocolMessage class ModelConfig ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables CcModelFilenamesEntry DESCRIPTOR Extensions MetricTagsEntry ParametersEntry backend batch_input batch_output cc_model_filenames default_model_filename dynamic_batching ensemble_scheduling input instance_group max_batch_size metric_tags model_operations model_repository_agents model_transaction_policy model_warmup name optimization output parameters platform response_cache sequence_batching version_policy Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelDynamicBatching A ProtocolMessage class ModelDynamicBatching ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions PriorityQueuePolicyEntry default_priority_level default_queue_policy max_queue_delay_microseconds preferred_batch_size preserve_ordering priority_levels priority_queue_policy Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelEnsembling A ProtocolMessage class ModelEnsembling ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions Step step Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelInput A ProtocolMessage class ModelInput ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions FORMAT_NCHW FORMAT_NHWC FORMAT_NONE Format allow_ragged_batch data_type dims format is_shape_tensor name optional reshape Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelInstanceGroup A ProtocolMessage class ModelInstanceGroup ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions KIND_AUTO KIND_CPU KIND_GPU KIND_MODEL Kind SecondaryDevice count gpus host_policy kind name passive profile rate_limiter secondary_devices Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelOperations A ProtocolMessage class ModelOperations ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions op_library_filename Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelOptimizationPolicy A ProtocolMessage class ModelOptimizationPolicy ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables Cuda DESCRIPTOR ExecutionAccelerators Extensions Graph ModelPriority PRIORITY_DEFAULT PRIORITY_MAX PRIORITY_MIN PinnedMemoryBuffer cuda eager_batching execution_accelerators gather_kernel_buffer_threshold graph input_pinned_memory output_pinned_memory priority Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelOutput A ProtocolMessage class ModelOutput ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions data_type dims is_shape_tensor label_filename name reshape Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelParameter A ProtocolMessage class ModelParameter ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions string_value Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelQueuePolicy A ProtocolMessage class ModelQueuePolicy ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DELAY DESCRIPTOR Extensions REJECT TimeoutAction allow_timeout_override default_timeout_microseconds max_queue_size timeout_action Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelRateLimiter A ProtocolMessage class ModelRateLimiter ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions Resource priority resources Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelRepositoryAgents A ProtocolMessage class ModelRepositoryAgents ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables Agent DESCRIPTOR Extensions agents Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelResponseCache A ProtocolMessage class ModelResponseCache ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions enable Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelSequenceBatching A ProtocolMessage class ModelSequenceBatching ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables Control ControlInput DESCRIPTOR Extensions InitialState State StrategyDirect StrategyOldest control_input direct max_sequence_idle_microseconds oldest state Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelTensorReshape A ProtocolMessage class ModelTensorReshape ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions shape Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelTransactionPolicy A ProtocolMessage class ModelTransactionPolicy ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions decoupled Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelVersionPolicy A ProtocolMessage class ModelVersionPolicy ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables All DESCRIPTOR Extensions Latest Specific all latest specific Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set. ModelWarmup A ProtocolMessage class ModelWarmup ( / , * args , ** kwargs ) Ancestors (in MRO) google.protobuf.pyext._message.CMessage google.protobuf.message.Message Class variables DESCRIPTOR Extensions Input InputsEntry batch_size count inputs name Methods ByteSize def ByteSize ( ... ) Returns the size of the message in bytes. Clear def Clear ( ... ) Clears the message. ClearExtension def ClearExtension ( ... ) Clears a message field. ClearField def ClearField ( ... ) Clears a message field. CopyFrom def CopyFrom ( ... ) Copies a protocol message into the current message. DiscardUnknownFields def DiscardUnknownFields ( ... ) Discards the unknown fields. FindInitializationErrors def FindInitializationErrors ( ... ) Finds unset required fields. FromString def FromString ( ... ) Creates new method instance from given serialized data. HasExtension def HasExtension ( ... ) Checks if a message field is set. HasField def HasField ( ... ) Checks if a message field is set. IsInitialized def IsInitialized ( ... ) Checks if all required fields of a protocol message are set. ListFields def ListFields ( ... ) Lists all set fields of a message. MergeFrom def MergeFrom ( ... ) Merges a protocol message into the current message. MergeFromString def MergeFromString ( ... ) Merges a serialized message into the current message. ParseFromString def ParseFromString ( ... ) Parses a serialized message into the current message. RegisterExtension def RegisterExtension ( ... ) Registers an extension with the current message. SerializePartialToString def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized. SerializeToString def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages. SetInParent def SetInParent ( ... ) Sets the has bit of the given field in its parent message. UnknownFields def UnknownFields ( ... ) Parse unknown field set WhichOneof def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"Model Config Pb2"},{"location":"reference/simaticai/model_config_pb2.html#module-simaticaimodel_config_pb2","text":"None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. # Generated by the protocol buffer compiler. DO NOT EDIT! # source: model_config.proto import sys _b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( 'latin1' )) from google.protobuf.internal import enum_type_wrapper from google.protobuf import descriptor as _descriptor from google.protobuf import message as _message from google.protobuf import reflection as _reflection from google.protobuf import symbol_database as _symbol_database # @@protoc_insertion_point(imports) _sym_db = _symbol_database . Default () DESCRIPTOR = _descriptor . FileDescriptor ( name = 'model_config.proto' , package = 'inference' , syntax = 'proto3' , serialized_options = None , serialized_pb = _b ( ' \\n\\x12 model_config.proto \\x12\\t inference \\\"\\x96\\x01\\n\\x10 ModelRateLimiter \\x12\\x37\\n\\t resources \\x18\\x01 \\x03 ( \\x0b\\x32 $.inference.ModelRateLimiter.Resource \\x12\\x10\\n\\x08 priority \\x18\\x02 \\x01 ( \\r\\x1a\\x37\\n\\x08 Resource \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x0e\\n\\x06 global \\x18\\x02 \\x01 ( \\x08\\x12\\r\\n\\x05\\x63 ount \\x18\\x03 \\x01 ( \\r\\\"\\x87\\x04\\n\\x12 ModelInstanceGroup \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x30\\n\\x04 kind \\x18\\x04 \\x01 ( \\x0e\\x32\\\" .inference.ModelInstanceGroup.Kind \\x12\\r\\n\\x05\\x63 ount \\x18\\x02 \\x01 ( \\x05\\x12\\x31\\n\\x0c rate_limiter \\x18\\x06 \\x01 ( \\x0b\\x32\\x1b .inference.ModelRateLimiter \\x12\\x0c\\n\\x04 gpus \\x18\\x03 \\x03 ( \\x05\\x12 H \\n\\x11 secondary_devices \\x18\\x08 \\x03 ( \\x0b\\x32 -.inference.ModelInstanceGroup.SecondaryDevice \\x12\\x0f\\n\\x07 profile \\x18\\x05 \\x03 ( \\t\\x12\\x0f\\n\\x07 passive \\x18\\x07 \\x01 ( \\x08\\x12\\x13\\n\\x0b host_policy \\x18\\t \\x01 ( \\t\\x1a\\x9c\\x01\\n\\x0f SecondaryDevice \\x12 O \\n\\x04 kind \\x18\\x01 \\x01 ( \\x0e\\x32\\x41 .inference.ModelInstanceGroup.SecondaryDevice.SecondaryDeviceKind \\x12\\x11\\n\\t device_id \\x18\\x02 \\x01 ( \\x03\\\" % \\n\\x13 SecondaryDeviceKind \\x12\\x0e\\n\\n KIND_NVDLA \\x10\\x00\\\" A \\n\\x04 Kind \\x12\\r\\n\\t KIND_AUTO \\x10\\x00\\x12\\x0c\\n\\x08 KIND_GPU \\x10\\x01\\x12\\x0c\\n\\x08 KIND_CPU \\x10\\x02\\x12\\x0e\\n\\n KIND_MODEL \\x10\\x03\\\" # \\n\\x12 ModelTensorReshape \\x12\\r\\n\\x05 shape \\x18\\x01 \\x03 ( \\x03\\\"\\xb2\\x02\\n\\n ModelInput \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 & \\n\\t data_type \\x18\\x02 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12 , \\n\\x06\\x66 ormat \\x18\\x03 \\x01 ( \\x0e\\x32\\x1c .inference.ModelInput.Format \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x04 \\x03 ( \\x03\\x12 . \\n\\x07 reshape \\x18\\x05 \\x01 ( \\x0b\\x32\\x1d .inference.ModelTensorReshape \\x12\\x17\\n\\x0f is_shape_tensor \\x18\\x06 \\x01 ( \\x08\\x12\\x1a\\n\\x12\\x61 llow_ragged_batch \\x18\\x07 \\x01 ( \\x08\\x12\\x10\\n\\x08 optional \\x18\\x08 \\x01 ( \\x08\\\" ; \\n\\x06\\x46 ormat \\x12\\x0f\\n\\x0b\\x46 ORMAT_NONE \\x10\\x00\\x12\\x0f\\n\\x0b\\x46 ORMAT_NHWC \\x10\\x01\\x12\\x0f\\n\\x0b\\x46 ORMAT_NCHW \\x10\\x02\\\"\\xb2\\x01\\n\\x0b ModelOutput \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 & \\n\\t data_type \\x18\\x02 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x03 \\x03 ( \\x03\\x12 . \\n\\x07 reshape \\x18\\x05 \\x01 ( \\x0b\\x32\\x1d .inference.ModelTensorReshape \\x12\\x16\\n\\x0e label_filename \\x18\\x04 \\x01 ( \\t\\x12\\x17\\n\\x0f is_shape_tensor \\x18\\x06 \\x01 ( \\x08\\\"\\xd9\\x02\\n\\n BatchInput \\x12 ( \\n\\x04 kind \\x18\\x01 \\x01 ( \\x0e\\x32\\x1a .inference.BatchInput.Kind \\x12\\x13\\n\\x0b target_name \\x18\\x02 \\x03 ( \\t\\x12 & \\n\\t data_type \\x18\\x03 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x14\\n\\x0c source_input \\x18\\x04 \\x03 ( \\t\\\"\\xcd\\x01\\n\\x04 Kind \\x12\\x17\\n\\x13\\x42\\x41 TCH_ELEMENT_COUNT \\x10\\x00\\x12 # \\n\\x1f\\x42\\x41 TCH_ACCUMULATED_ELEMENT_COUNT \\x10\\x01\\x12 - \\n )BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO \\x10\\x02\\x12 $ \\n BATCH_MAX_ELEMENT_COUNT_AS_SHAPE \\x10\\x03\\x12\\x14\\n\\x10\\x42\\x41 TCH_ITEM_SHAPE \\x10\\x04\\x12\\x1c\\n\\x18\\x42\\x41 TCH_ITEM_SHAPE_FLATTEN \\x10\\x05\\\"\\x8f\\x01\\n\\x0b\\x42\\x61 tchOutput \\x12\\x13\\n\\x0b target_name \\x18\\x01 \\x03 ( \\t\\x12 ) \\n\\x04 kind \\x18\\x02 \\x01 ( \\x0e\\x32\\x1b .inference.BatchOutput.Kind \\x12\\x14\\n\\x0c source_input \\x18\\x03 \\x03 ( \\t\\\" * \\n\\x04 Kind \\x12\\\"\\n\\x1e\\x42\\x41 TCH_SCATTER_WITH_INPUT_SHAPE \\x10\\x00\\\"\\x90\\x02\\n\\x12 ModelVersionPolicy \\x12\\x36\\n\\x06 latest \\x18\\x01 \\x01 ( \\x0b\\x32 $.inference.ModelVersionPolicy.LatestH \\x00\\x12\\x30\\n\\x03\\x61 ll \\x18\\x02 \\x01 ( \\x0b\\x32 !.inference.ModelVersionPolicy.AllH \\x00\\x12 : \\n\\x08 specific \\x18\\x03 \\x01 ( \\x0b\\x32 &.inference.ModelVersionPolicy.SpecificH \\x00\\x1a\\x1e\\n\\x06 Latest \\x12\\x14\\n\\x0c num_versions \\x18\\x01 \\x01 ( \\r\\x1a\\x05\\n\\x03\\x41 ll \\x1a\\x1c\\n\\x08 Specific \\x12\\x10\\n\\x08 versions \\x18\\x01 \\x03 ( \\x03\\x42\\x0f\\n\\r policy_choice \\\"\\xfd\\r\\n\\x17 ModelOptimizationPolicy \\x12\\x37\\n\\x05 graph \\x18\\x01 \\x01 ( \\x0b\\x32 (.inference.ModelOptimizationPolicy.Graph \\x12\\x42\\n\\x08 priority \\x18\\x02 \\x01 ( \\x0e\\x32\\x30 .inference.ModelOptimizationPolicy.ModelPriority \\x12\\x35\\n\\x04\\x63 uda \\x18\\x03 \\x01 ( \\x0b\\x32\\' .inference.ModelOptimizationPolicy.Cuda \\x12 X \\n\\x16\\x65 xecution_accelerators \\x18\\x04 \\x01 ( \\x0b\\x32\\x38 .inference.ModelOptimizationPolicy.ExecutionAccelerators \\x12 R \\n\\x13 input_pinned_memory \\x18\\x05 \\x01 ( \\x0b\\x32\\x35 .inference.ModelOptimizationPolicy.PinnedMemoryBuffer \\x12 S \\n\\x14 output_pinned_memory \\x18\\x06 \\x01 ( \\x0b\\x32\\x35 .inference.ModelOptimizationPolicy.PinnedMemoryBuffer \\x12 & \\n\\x1e gather_kernel_buffer_threshold \\x18\\x07 \\x01 ( \\r\\x12\\x16\\n\\x0e\\x65\\x61 ger_batching \\x18\\x08 \\x01 ( \\x08\\x1a\\x16\\n\\x05 Graph \\x12\\r\\n\\x05 level \\x18\\x01 \\x01 ( \\x05\\x1a\\xba\\x05\\n\\x04\\x43 uda \\x12\\x0e\\n\\x06 graphs \\x18\\x01 \\x01 ( \\x08\\x12\\x18\\n\\x10\\x62 usy_wait_events \\x18\\x02 \\x01 ( \\x08\\x12\\x45\\n\\n graph_spec \\x18\\x03 \\x03 ( \\x0b\\x32\\x31 .inference.ModelOptimizationPolicy.Cuda.GraphSpec \\x12\\x1a\\n\\x12 output_copy_stream \\x18\\x04 \\x01 ( \\x08\\x1a\\xa4\\x04\\n\\t GraphSpec \\x12\\x12\\n\\n batch_size \\x18\\x01 \\x01 ( \\x05\\x12 K \\n\\x05 input \\x18\\x02 \\x03 ( \\x0b\\x32 <.inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry \\x12 W \\n\\x11 graph_lower_bound \\x18\\x03 \\x01 ( \\x0b\\x32 <.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound \\x1a\\x14\\n\\x05 Shape \\x12\\x0b\\n\\x03\\x64 im \\x18\\x01 \\x03 ( \\x03\\x1a\\xdf\\x01\\n\\n LowerBound \\x12\\x12\\n\\n batch_size \\x18\\x01 \\x01 ( \\x05\\x12 V \\n\\x05 input \\x18\\x02 \\x03 ( \\x0b\\x32 G.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry \\x1a\\x65\\n\\n InputEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\x46\\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x37 .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape: \\x02\\x38\\x01\\x1a\\x65\\n\\n InputEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\x46\\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x37 .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape: \\x02\\x38\\x01\\x1a\\xa4\\x03\\n\\x15\\x45 xecutionAccelerators \\x12 g \\n\\x19 gpu_execution_accelerator \\x18\\x01 \\x03 ( \\x0b\\x32\\x44 .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator \\x12 g \\n\\x19\\x63 pu_execution_accelerator \\x18\\x02 \\x03 ( \\x0b\\x32\\x44 .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator \\x1a\\xb8\\x01\\n\\x0b\\x41\\x63\\x63\\x65 lerator \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 h \\n\\n parameters \\x18\\x02 \\x03 ( \\x0b\\x32 T.inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry \\x1a\\x31\\n\\x0f ParametersEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a $ \\n\\x12 PinnedMemoryBuffer \\x12\\x0e\\n\\x06\\x65 nable \\x18\\x01 \\x01 ( \\x08\\\" I \\n\\r ModelPriority \\x12\\x14\\n\\x10 PRIORITY_DEFAULT \\x10\\x00\\x12\\x10\\n\\x0c PRIORITY_MAX \\x10\\x01\\x12\\x10\\n\\x0c PRIORITY_MIN \\x10\\x02\\\"\\xdb\\x01\\n\\x10 ModelQueuePolicy \\x12\\x41\\n\\x0e timeout_action \\x18\\x01 \\x01 ( \\x0e\\x32 ).inference.ModelQueuePolicy.TimeoutAction \\x12 $ \\n\\x1c\\x64\\x65\\x66\\x61 ult_timeout_microseconds \\x18\\x02 \\x01 ( \\x04\\x12\\x1e\\n\\x16\\x61 llow_timeout_override \\x18\\x03 \\x01 ( \\x08\\x12\\x16\\n\\x0e max_queue_size \\x18\\x04 \\x01 ( \\r\\\" & \\n\\r TimeoutAction \\x12\\n\\n\\x06 REJECT \\x10\\x00\\x12\\t\\n\\x05\\x44\\x45 LAY \\x10\\x01\\\"\\x9b\\x03\\n\\x14 ModelDynamicBatching \\x12\\x1c\\n\\x14 preferred_batch_size \\x18\\x01 \\x03 ( \\x05\\x12 $ \\n\\x1c max_queue_delay_microseconds \\x18\\x02 \\x01 ( \\x04\\x12\\x19\\n\\x11 preserve_ordering \\x18\\x03 \\x01 ( \\x08\\x12\\x17\\n\\x0f priority_levels \\x18\\x04 \\x01 ( \\x04\\x12\\x1e\\n\\x16\\x64\\x65\\x66\\x61 ult_priority_level \\x18\\x05 \\x01 ( \\x04\\x12\\x39\\n\\x14\\x64\\x65\\x66\\x61 ult_queue_policy \\x18\\x06 \\x01 ( \\x0b\\x32\\x1b .inference.ModelQueuePolicy \\x12 W \\n\\x15 priority_queue_policy \\x18\\x07 \\x03 ( \\x0b\\x32\\x38 .inference.ModelDynamicBatching.PriorityQueuePolicyEntry \\x1a W \\n\\x18 PriorityQueuePolicyEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\x04\\x12 * \\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x1b .inference.ModelQueuePolicy: \\x02\\x38\\x01\\\"\\x8b\\n\\n\\x15 ModelSequenceBatching \\x12\\x41\\n\\x06\\x64 irect \\x18\\x03 \\x01 ( \\x0b\\x32 /.inference.ModelSequenceBatching.StrategyDirectH \\x00\\x12\\x41\\n\\x06 oldest \\x18\\x04 \\x01 ( \\x0b\\x32 /.inference.ModelSequenceBatching.StrategyOldestH \\x00\\x12 & \\n\\x1e max_sequence_idle_microseconds \\x18\\x01 \\x01 ( \\x04\\x12\\x44\\n\\r control_input \\x18\\x02 \\x03 ( \\x0b\\x32 -.inference.ModelSequenceBatching.ControlInput \\x12\\x35\\n\\x05 state \\x18\\x05 \\x03 ( \\x0b\\x32 &.inference.ModelSequenceBatching.State \\x1a\\xb1\\x02\\n\\x07\\x43 ontrol \\x12 ; \\n\\x04 kind \\x18\\x01 \\x01 ( \\x0e\\x32 -.inference.ModelSequenceBatching.Control.Kind \\x12\\x18\\n\\x10 int32_false_true \\x18\\x02 \\x03 ( \\x05\\x12\\x17\\n\\x0f\\x66 p32_false_true \\x18\\x03 \\x03 ( \\x02\\x12\\x17\\n\\x0f\\x62 ool_false_true \\x18\\x05 \\x03 ( \\x08\\x12 & \\n\\t data_type \\x18\\x04 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\\" u \\n\\x04 Kind \\x12\\x1a\\n\\x16\\x43 ONTROL_SEQUENCE_START \\x10\\x00\\x12\\x1a\\n\\x16\\x43 ONTROL_SEQUENCE_READY \\x10\\x01\\x12\\x18\\n\\x14\\x43 ONTROL_SEQUENCE_END \\x10\\x02\\x12\\x1b\\n\\x17\\x43 ONTROL_SEQUENCE_CORRID \\x10\\x03\\x1a W \\n\\x0c\\x43 ontrolInput \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x39\\n\\x07\\x63 ontrol \\x18\\x02 \\x03 ( \\x0b\\x32 (.inference.ModelSequenceBatching.Control \\x1a\\x8a\\x01\\n\\x0c InitialState \\x12 & \\n\\t data_type \\x18\\x01 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x02 \\x03 ( \\x03\\x12\\x13\\n\\t zero_data \\x18\\x03 \\x01 ( \\x08 H \\x00\\x12\\x13\\n\\t data_file \\x18\\x04 \\x01 ( \\t H \\x00\\x12\\x0c\\n\\x04 name \\x18\\x05 \\x01 ( \\t B \\x0c\\n\\n state_data \\x1a\\xac\\x01\\n\\x05 State \\x12\\x12\\n\\n input_name \\x18\\x01 \\x01 ( \\t\\x12\\x13\\n\\x0b output_name \\x18\\x02 \\x01 ( \\t\\x12 & \\n\\t data_type \\x18\\x03 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x04 \\x03 ( \\x03\\x12\\x44\\n\\r initial_state \\x18\\x05 \\x03 ( \\x0b\\x32 -.inference.ModelSequenceBatching.InitialState \\x1a X \\n\\x0e StrategyDirect \\x12 $ \\n\\x1c max_queue_delay_microseconds \\x18\\x01 \\x01 ( \\x04\\x12 \\n\\x18 minimum_slot_utilization \\x18\\x02 \\x01 ( \\x02\\x1a\\x90\\x01\\n\\x0e StrategyOldest \\x12\\x1f\\n\\x17 max_candidate_sequences \\x18\\x01 \\x01 ( \\x05\\x12\\x1c\\n\\x14 preferred_batch_size \\x18\\x02 \\x03 ( \\x05\\x12 $ \\n\\x1c max_queue_delay_microseconds \\x18\\x03 \\x01 ( \\x04\\x12\\x19\\n\\x11 preserve_ordering \\x18\\x04 \\x01 ( \\x08\\x42\\x11\\n\\x0f strategy_choice \\\"\\xf6\\x02\\n\\x0f ModelEnsembling \\x12 - \\n\\x04 step \\x18\\x01 \\x03 ( \\x0b\\x32\\x1f .inference.ModelEnsembling.Step \\x1a\\xb3\\x02\\n\\x04 Step \\x12\\x12\\n\\n model_name \\x18\\x01 \\x01 ( \\t\\x12\\x15\\n\\r model_version \\x18\\x02 \\x01 ( \\x03\\x12 @ \\n\\t input_map \\x18\\x03 \\x03 ( \\x0b\\x32 -.inference.ModelEnsembling.Step.InputMapEntry \\x12\\x42\\n\\n output_map \\x18\\x04 \\x03 ( \\x0b\\x32 ..inference.ModelEnsembling.Step.OutputMapEntry \\x12\\x17\\n\\x0f model_namespace \\x18\\x05 \\x01 ( \\t\\x1a / \\n\\r InputMapEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a\\x30\\n\\x0e OutputMapEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\\" & \\n\\x0e ModelParameter \\x12\\x14\\n\\x0c string_value \\x18\\x01 \\x01 ( \\t\\\"\\xd9\\x02\\n\\x0b ModelWarmup \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x12\\n\\n batch_size \\x18\\x02 \\x01 ( \\r\\x12\\x32\\n\\x06 inputs \\x18\\x03 \\x03 ( \\x0b\\x32\\\" .inference.ModelWarmup.InputsEntry \\x12\\r\\n\\x05\\x63 ount \\x18\\x04 \\x01 ( \\r\\x1a\\x97\\x01\\n\\x05 Input \\x12 & \\n\\t data_type \\x18\\x01 \\x01 ( \\x0e\\x32\\x13 .inference.DataType \\x12\\x0c\\n\\x04\\x64 ims \\x18\\x02 \\x03 ( \\x03\\x12\\x13\\n\\t zero_data \\x18\\x03 \\x01 ( \\x08 H \\x00\\x12\\x15\\n\\x0b random_data \\x18\\x04 \\x01 ( \\x08 H \\x00\\x12\\x19\\n\\x0f input_data_file \\x18\\x05 \\x01 ( \\t H \\x00\\x42\\x11\\n\\x0f input_data_type \\x1a K \\n\\x0b InputsEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12 + \\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x1c .inference.ModelWarmup.Input: \\x02\\x38\\x01\\\" . \\n\\x0f ModelOperations \\x12\\x1b\\n\\x13 op_library_filename \\x18\\x01 \\x03 ( \\t\\\" + \\n\\x16 ModelTransactionPolicy \\x12\\x11\\n\\t decoupled \\x18\\x01 \\x01 ( \\x08\\\"\\xe6\\x01\\n\\x15 ModelRepositoryAgents \\x12\\x36\\n\\x06\\x61 gents \\x18\\x01 \\x03 ( \\x0b\\x32 &.inference.ModelRepositoryAgents.Agent \\x1a\\x94\\x01\\n\\x05\\x41 gent \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12 J \\n\\n parameters \\x18\\x02 \\x03 ( \\x0b\\x32\\x36 .inference.ModelRepositoryAgents.Agent.ParametersEntry \\x1a\\x31\\n\\x0f ParametersEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\\" $ \\n\\x12 ModelResponseCache \\x12\\x0e\\n\\x06\\x65 nable \\x18\\x01 \\x01 ( \\x08\\\"\\xb2\\n\\n\\x0b ModelConfig \\x12\\x0c\\n\\x04 name \\x18\\x01 \\x01 ( \\t\\x12\\x10\\n\\x08 platform \\x18\\x02 \\x01 ( \\t\\x12\\x0f\\n\\x07\\x62\\x61\\x63 kend \\x18\\x11 \\x01 ( \\t\\x12\\x35\\n\\x0e version_policy \\x18\\x03 \\x01 ( \\x0b\\x32\\x1d .inference.ModelVersionPolicy \\x12\\x16\\n\\x0e max_batch_size \\x18\\x04 \\x01 ( \\x05\\x12 $ \\n\\x05 input \\x18\\x05 \\x03 ( \\x0b\\x32\\x15 .inference.ModelInput \\x12 & \\n\\x06 output \\x18\\x06 \\x03 ( \\x0b\\x32\\x16 .inference.ModelOutput \\x12 * \\n\\x0b\\x62\\x61 tch_input \\x18\\x14 \\x03 ( \\x0b\\x32\\x15 .inference.BatchInput \\x12 , \\n\\x0c\\x62\\x61 tch_output \\x18\\x15 \\x03 ( \\x0b\\x32\\x16 .inference.BatchOutput \\x12\\x38\\n\\x0c optimization \\x18\\x0c \\x01 ( \\x0b\\x32\\\" .inference.ModelOptimizationPolicy \\x12 ; \\n\\x10\\x64 ynamic_batching \\x18\\x0b \\x01 ( \\x0b\\x32\\x1f .inference.ModelDynamicBatchingH \\x00\\x12 = \\n\\x11 sequence_batching \\x18\\r \\x01 ( \\x0b\\x32 .inference.ModelSequenceBatchingH \\x00\\x12\\x39\\n\\x13\\x65 nsemble_scheduling \\x18\\x0f \\x01 ( \\x0b\\x32\\x1a .inference.ModelEnsemblingH \\x00\\x12\\x35\\n\\x0e instance_group \\x18\\x07 \\x03 ( \\x0b\\x32\\x1d .inference.ModelInstanceGroup \\x12\\x1e\\n\\x16\\x64\\x65\\x66\\x61 ult_model_filename \\x18\\x08 \\x01 ( \\t\\x12 H \\n\\x12\\x63\\x63 _model_filenames \\x18\\t \\x03 ( \\x0b\\x32 ,.inference.ModelConfig.CcModelFilenamesEntry \\x12 ; \\n\\x0b metric_tags \\x18\\n \\x03 ( \\x0b\\x32 &.inference.ModelConfig.MetricTagsEntry \\x12 : \\n\\n parameters \\x18\\x0e \\x03 ( \\x0b\\x32 &.inference.ModelConfig.ParametersEntry \\x12 , \\n\\x0c model_warmup \\x18\\x10 \\x03 ( \\x0b\\x32\\x16 .inference.ModelWarmup \\x12\\x34\\n\\x10 model_operations \\x18\\x12 \\x01 ( \\x0b\\x32\\x1a .inference.ModelOperations \\x12\\x43\\n\\x18 model_transaction_policy \\x18\\x13 \\x01 ( \\x0b\\x32 !.inference.ModelTransactionPolicy \\x12\\x41\\n\\x17 model_repository_agents \\x18\\x17 \\x01 ( \\x0b\\x32 .inference.ModelRepositoryAgents \\x12\\x35\\n\\x0e response_cache \\x18\\x18 \\x01 ( \\x0b\\x32\\x1d .inference.ModelResponseCache \\x1a\\x37\\n\\x15\\x43\\x63 ModelFilenamesEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a\\x31\\n\\x0f MetricTagsEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12\\r\\n\\x05 value \\x18\\x02 \\x01 ( \\t : \\x02\\x38\\x01\\x1a L \\n\\x0f ParametersEntry \\x12\\x0b\\n\\x03 key \\x18\\x01 \\x01 ( \\t\\x12 ( \\n\\x05 value \\x18\\x02 \\x01 ( \\x0b\\x32\\x19 .inference.ModelParameter: \\x02\\x38\\x01\\x42\\x13\\n\\x11 scheduling_choice* \\xfa\\x01\\n\\x08\\x44\\x61 taType \\x12\\x10\\n\\x0c TYPE_INVALID \\x10\\x00\\x12\\r\\n\\t TYPE_BOOL \\x10\\x01\\x12\\x0e\\n\\n TYPE_UINT8 \\x10\\x02\\x12\\x0f\\n\\x0b TYPE_UINT16 \\x10\\x03\\x12\\x0f\\n\\x0b TYPE_UINT32 \\x10\\x04\\x12\\x0f\\n\\x0b TYPE_UINT64 \\x10\\x05\\x12\\r\\n\\t TYPE_INT8 \\x10\\x06\\x12\\x0e\\n\\n TYPE_INT16 \\x10\\x07\\x12\\x0e\\n\\n TYPE_INT32 \\x10\\x08\\x12\\x0e\\n\\n TYPE_INT64 \\x10\\t\\x12\\r\\n\\t TYPE_FP16 \\x10\\n\\x12\\r\\n\\t TYPE_FP32 \\x10\\x0b\\x12\\r\\n\\t TYPE_FP64 \\x10\\x0c\\x12\\x0f\\n\\x0b TYPE_STRING \\x10\\r\\x12\\r\\n\\t TYPE_BF16 \\x10\\x0e\\x62\\x06 proto3' ) ) _DATATYPE = _descriptor . EnumDescriptor ( name = 'DataType' , full_name = 'inference.DataType' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'TYPE_INVALID' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_BOOL' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT8' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT16' , index = 3 , number = 3 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT32' , index = 4 , number = 4 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_UINT64' , index = 5 , number = 5 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT8' , index = 6 , number = 6 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT16' , index = 7 , number = 7 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT32' , index = 8 , number = 8 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_INT64' , index = 9 , number = 9 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_FP16' , index = 10 , number = 10 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_FP32' , index = 11 , number = 11 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_FP64' , index = 12 , number = 12 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_STRING' , index = 13 , number = 13 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'TYPE_BF16' , index = 14 , number = 14 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 8189 , serialized_end = 8439 , ) _sym_db . RegisterEnumDescriptor ( _DATATYPE ) DataType = enum_type_wrapper . EnumTypeWrapper ( _DATATYPE ) TYPE_INVALID = 0 TYPE_BOOL = 1 TYPE_UINT8 = 2 TYPE_UINT16 = 3 TYPE_UINT32 = 4 TYPE_UINT64 = 5 TYPE_INT8 = 6 TYPE_INT16 = 7 TYPE_INT32 = 8 TYPE_INT64 = 9 TYPE_FP16 = 10 TYPE_FP32 = 11 TYPE_FP64 = 12 TYPE_STRING = 13 TYPE_BF16 = 14 _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND = _descriptor . EnumDescriptor ( name = 'SecondaryDeviceKind' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice.SecondaryDeviceKind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'KIND_NVDLA' , index = 0 , number = 0 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 602 , serialized_end = 639 , ) _sym_db . RegisterEnumDescriptor ( _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND ) _MODELINSTANCEGROUP_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.ModelInstanceGroup.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'KIND_AUTO' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'KIND_GPU' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'KIND_CPU' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'KIND_MODEL' , index = 3 , number = 3 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 641 , serialized_end = 706 , ) _sym_db . RegisterEnumDescriptor ( _MODELINSTANCEGROUP_KIND ) _MODELINPUT_FORMAT = _descriptor . EnumDescriptor ( name = 'Format' , full_name = 'inference.ModelInput.Format' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'FORMAT_NONE' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'FORMAT_NHWC' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'FORMAT_NCHW' , index = 2 , number = 2 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 993 , serialized_end = 1052 , ) _sym_db . RegisterEnumDescriptor ( _MODELINPUT_FORMAT ) _BATCHINPUT_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.BatchInput.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'BATCH_ELEMENT_COUNT' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ACCUMULATED_ELEMENT_COUNT' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_MAX_ELEMENT_COUNT_AS_SHAPE' , index = 3 , number = 3 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ITEM_SHAPE' , index = 4 , number = 4 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'BATCH_ITEM_SHAPE_FLATTEN' , index = 5 , number = 5 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 1376 , serialized_end = 1581 , ) _sym_db . RegisterEnumDescriptor ( _BATCHINPUT_KIND ) _BATCHOUTPUT_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.BatchOutput.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'BATCH_SCATTER_WITH_INPUT_SHAPE' , index = 0 , number = 0 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 1685 , serialized_end = 1727 , ) _sym_db . RegisterEnumDescriptor ( _BATCHOUTPUT_KIND ) _MODELOPTIMIZATIONPOLICY_MODELPRIORITY = _descriptor . EnumDescriptor ( name = 'ModelPriority' , full_name = 'inference.ModelOptimizationPolicy.ModelPriority' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'PRIORITY_DEFAULT' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'PRIORITY_MAX' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'PRIORITY_MIN' , index = 2 , number = 2 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 3721 , serialized_end = 3794 , ) _sym_db . RegisterEnumDescriptor ( _MODELOPTIMIZATIONPOLICY_MODELPRIORITY ) _MODELQUEUEPOLICY_TIMEOUTACTION = _descriptor . EnumDescriptor ( name = 'TimeoutAction' , full_name = 'inference.ModelQueuePolicy.TimeoutAction' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'REJECT' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'DELAY' , index = 1 , number = 1 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 3978 , serialized_end = 4016 , ) _sym_db . RegisterEnumDescriptor ( _MODELQUEUEPOLICY_TIMEOUTACTION ) _MODELSEQUENCEBATCHING_CONTROL_KIND = _descriptor . EnumDescriptor ( name = 'Kind' , full_name = 'inference.ModelSequenceBatching.Control.Kind' , filename = None , file = DESCRIPTOR , values = [ _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_START' , index = 0 , number = 0 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_READY' , index = 1 , number = 1 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_END' , index = 2 , number = 2 , serialized_options = None , type = None ), _descriptor . EnumValueDescriptor ( name = 'CONTROL_SEQUENCE_CORRID' , index = 3 , number = 3 , serialized_options = None , type = None ), ], containing_type = None , serialized_options = None , serialized_start = 4946 , serialized_end = 5063 , ) _sym_db . RegisterEnumDescriptor ( _MODELSEQUENCEBATCHING_CONTROL_KIND ) _MODELRATELIMITER_RESOURCE = _descriptor . Descriptor ( name = 'Resource' , full_name = 'inference.ModelRateLimiter.Resource' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelRateLimiter.Resource.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'global' , full_name = 'inference.ModelRateLimiter.Resource.global' , index = 1 , number = 2 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'count' , full_name = 'inference.ModelRateLimiter.Resource.count' , index = 2 , number = 3 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 129 , serialized_end = 184 , ) _MODELRATELIMITER = _descriptor . Descriptor ( name = 'ModelRateLimiter' , full_name = 'inference.ModelRateLimiter' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'resources' , full_name = 'inference.ModelRateLimiter.resources' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority' , full_name = 'inference.ModelRateLimiter.priority' , index = 1 , number = 2 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELRATELIMITER_RESOURCE , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 34 , serialized_end = 184 , ) _MODELINSTANCEGROUP_SECONDARYDEVICE = _descriptor . Descriptor ( name = 'SecondaryDevice' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice.kind' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'device_id' , full_name = 'inference.ModelInstanceGroup.SecondaryDevice.device_id' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 483 , serialized_end = 639 , ) _MODELINSTANCEGROUP = _descriptor . Descriptor ( name = 'ModelInstanceGroup' , full_name = 'inference.ModelInstanceGroup' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelInstanceGroup.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.ModelInstanceGroup.kind' , index = 1 , number = 4 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'count' , full_name = 'inference.ModelInstanceGroup.count' , index = 2 , number = 2 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'rate_limiter' , full_name = 'inference.ModelInstanceGroup.rate_limiter' , index = 3 , number = 6 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'gpus' , full_name = 'inference.ModelInstanceGroup.gpus' , index = 4 , number = 3 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'secondary_devices' , full_name = 'inference.ModelInstanceGroup.secondary_devices' , index = 5 , number = 8 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'profile' , full_name = 'inference.ModelInstanceGroup.profile' , index = 6 , number = 5 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'passive' , full_name = 'inference.ModelInstanceGroup.passive' , index = 7 , number = 7 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'host_policy' , full_name = 'inference.ModelInstanceGroup.host_policy' , index = 8 , number = 9 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELINSTANCEGROUP_SECONDARYDEVICE , ], enum_types = [ _MODELINSTANCEGROUP_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 187 , serialized_end = 706 , ) _MODELTENSORRESHAPE = _descriptor . Descriptor ( name = 'ModelTensorReshape' , full_name = 'inference.ModelTensorReshape' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'shape' , full_name = 'inference.ModelTensorReshape.shape' , index = 0 , number = 1 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 708 , serialized_end = 743 , ) _MODELINPUT = _descriptor . Descriptor ( name = 'ModelInput' , full_name = 'inference.ModelInput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelInput.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelInput.data_type' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'format' , full_name = 'inference.ModelInput.format' , index = 2 , number = 3 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelInput.dims' , index = 3 , number = 4 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'reshape' , full_name = 'inference.ModelInput.reshape' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'is_shape_tensor' , full_name = 'inference.ModelInput.is_shape_tensor' , index = 5 , number = 6 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'allow_ragged_batch' , full_name = 'inference.ModelInput.allow_ragged_batch' , index = 6 , number = 7 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'optional' , full_name = 'inference.ModelInput.optional' , index = 7 , number = 8 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELINPUT_FORMAT , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 746 , serialized_end = 1052 , ) _MODELOUTPUT = _descriptor . Descriptor ( name = 'ModelOutput' , full_name = 'inference.ModelOutput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelOutput.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelOutput.data_type' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelOutput.dims' , index = 2 , number = 3 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'reshape' , full_name = 'inference.ModelOutput.reshape' , index = 3 , number = 5 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'label_filename' , full_name = 'inference.ModelOutput.label_filename' , index = 4 , number = 4 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'is_shape_tensor' , full_name = 'inference.ModelOutput.is_shape_tensor' , index = 5 , number = 6 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1055 , serialized_end = 1233 , ) _BATCHINPUT = _descriptor . Descriptor ( name = 'BatchInput' , full_name = 'inference.BatchInput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.BatchInput.kind' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'target_name' , full_name = 'inference.BatchInput.target_name' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.BatchInput.data_type' , index = 2 , number = 3 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'source_input' , full_name = 'inference.BatchInput.source_input' , index = 3 , number = 4 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _BATCHINPUT_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1236 , serialized_end = 1581 , ) _BATCHOUTPUT = _descriptor . Descriptor ( name = 'BatchOutput' , full_name = 'inference.BatchOutput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'target_name' , full_name = 'inference.BatchOutput.target_name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.BatchOutput.kind' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'source_input' , full_name = 'inference.BatchOutput.source_input' , index = 2 , number = 3 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _BATCHOUTPUT_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1584 , serialized_end = 1727 , ) _MODELVERSIONPOLICY_LATEST = _descriptor . Descriptor ( name = 'Latest' , full_name = 'inference.ModelVersionPolicy.Latest' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'num_versions' , full_name = 'inference.ModelVersionPolicy.Latest.num_versions' , index = 0 , number = 1 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1918 , serialized_end = 1948 , ) _MODELVERSIONPOLICY_ALL = _descriptor . Descriptor ( name = 'All' , full_name = 'inference.ModelVersionPolicy.All' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1950 , serialized_end = 1955 , ) _MODELVERSIONPOLICY_SPECIFIC = _descriptor . Descriptor ( name = 'Specific' , full_name = 'inference.ModelVersionPolicy.Specific' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'versions' , full_name = 'inference.ModelVersionPolicy.Specific.versions' , index = 0 , number = 1 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 1957 , serialized_end = 1985 , ) _MODELVERSIONPOLICY = _descriptor . Descriptor ( name = 'ModelVersionPolicy' , full_name = 'inference.ModelVersionPolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'latest' , full_name = 'inference.ModelVersionPolicy.latest' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'all' , full_name = 'inference.ModelVersionPolicy.all' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'specific' , full_name = 'inference.ModelVersionPolicy.specific' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELVERSIONPOLICY_LATEST , _MODELVERSIONPOLICY_ALL , _MODELVERSIONPOLICY_SPECIFIC , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'policy_choice' , full_name = 'inference.ModelVersionPolicy.policy_choice' , index = 0 , containing_type = None , fields = []), ], serialized_start = 1730 , serialized_end = 2002 , ) _MODELOPTIMIZATIONPOLICY_GRAPH = _descriptor . Descriptor ( name = 'Graph' , full_name = 'inference.ModelOptimizationPolicy.Graph' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'level' , full_name = 'inference.ModelOptimizationPolicy.Graph.level' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2535 , serialized_end = 2557 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE = _descriptor . Descriptor ( name = 'Shape' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'dim' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.dim' , index = 0 , number = 1 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2909 , serialized_end = 2929 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY = _descriptor . Descriptor ( name = 'InputEntry' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3054 , serialized_end = 3155 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND = _descriptor . Descriptor ( name = 'LowerBound' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'batch_size' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.batch_size' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.input' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2932 , serialized_end = 3155 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY = _descriptor . Descriptor ( name = 'InputEntry' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3054 , serialized_end = 3155 , ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC = _descriptor . Descriptor ( name = 'GraphSpec' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'batch_size' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.batch_size' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.input' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'graph_lower_bound' , full_name = 'inference.ModelOptimizationPolicy.Cuda.GraphSpec.graph_lower_bound' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE , _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND , _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2710 , serialized_end = 3258 , ) _MODELOPTIMIZATIONPOLICY_CUDA = _descriptor . Descriptor ( name = 'Cuda' , full_name = 'inference.ModelOptimizationPolicy.Cuda' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'graphs' , full_name = 'inference.ModelOptimizationPolicy.Cuda.graphs' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'busy_wait_events' , full_name = 'inference.ModelOptimizationPolicy.Cuda.busy_wait_events' , index = 1 , number = 2 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'graph_spec' , full_name = 'inference.ModelOptimizationPolicy.Cuda.graph_spec' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_copy_stream' , full_name = 'inference.ModelOptimizationPolicy.Cuda.output_copy_stream' , index = 3 , number = 4 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2560 , serialized_end = 3258 , ) _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY = _descriptor . Descriptor ( name = 'ParametersEntry' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3632 , serialized_end = 3681 , ) _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR = _descriptor . Descriptor ( name = 'Accelerator' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'parameters' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parameters' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3497 , serialized_end = 3681 , ) _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS = _descriptor . Descriptor ( name = 'ExecutionAccelerators' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'gpu_execution_accelerator' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'cpu_execution_accelerator' , full_name = 'inference.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3261 , serialized_end = 3681 , ) _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER = _descriptor . Descriptor ( name = 'PinnedMemoryBuffer' , full_name = 'inference.ModelOptimizationPolicy.PinnedMemoryBuffer' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'enable' , full_name = 'inference.ModelOptimizationPolicy.PinnedMemoryBuffer.enable' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3683 , serialized_end = 3719 , ) _MODELOPTIMIZATIONPOLICY = _descriptor . Descriptor ( name = 'ModelOptimizationPolicy' , full_name = 'inference.ModelOptimizationPolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'graph' , full_name = 'inference.ModelOptimizationPolicy.graph' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority' , full_name = 'inference.ModelOptimizationPolicy.priority' , index = 1 , number = 2 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'cuda' , full_name = 'inference.ModelOptimizationPolicy.cuda' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'execution_accelerators' , full_name = 'inference.ModelOptimizationPolicy.execution_accelerators' , index = 3 , number = 4 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input_pinned_memory' , full_name = 'inference.ModelOptimizationPolicy.input_pinned_memory' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_pinned_memory' , full_name = 'inference.ModelOptimizationPolicy.output_pinned_memory' , index = 5 , number = 6 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'gather_kernel_buffer_threshold' , full_name = 'inference.ModelOptimizationPolicy.gather_kernel_buffer_threshold' , index = 6 , number = 7 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'eager_batching' , full_name = 'inference.ModelOptimizationPolicy.eager_batching' , index = 7 , number = 8 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELOPTIMIZATIONPOLICY_GRAPH , _MODELOPTIMIZATIONPOLICY_CUDA , _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS , _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER , ], enum_types = [ _MODELOPTIMIZATIONPOLICY_MODELPRIORITY , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 2005 , serialized_end = 3794 , ) _MODELQUEUEPOLICY = _descriptor . Descriptor ( name = 'ModelQueuePolicy' , full_name = 'inference.ModelQueuePolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'timeout_action' , full_name = 'inference.ModelQueuePolicy.timeout_action' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_timeout_microseconds' , full_name = 'inference.ModelQueuePolicy.default_timeout_microseconds' , index = 1 , number = 2 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'allow_timeout_override' , full_name = 'inference.ModelQueuePolicy.allow_timeout_override' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_queue_size' , full_name = 'inference.ModelQueuePolicy.max_queue_size' , index = 3 , number = 4 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELQUEUEPOLICY_TIMEOUTACTION , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3797 , serialized_end = 4016 , ) _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY = _descriptor . Descriptor ( name = 'PriorityQueuePolicyEntry' , full_name = 'inference.ModelDynamicBatching.PriorityQueuePolicyEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelDynamicBatching.PriorityQueuePolicyEntry.key' , index = 0 , number = 1 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelDynamicBatching.PriorityQueuePolicyEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 4343 , serialized_end = 4430 , ) _MODELDYNAMICBATCHING = _descriptor . Descriptor ( name = 'ModelDynamicBatching' , full_name = 'inference.ModelDynamicBatching' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'preferred_batch_size' , full_name = 'inference.ModelDynamicBatching.preferred_batch_size' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_queue_delay_microseconds' , full_name = 'inference.ModelDynamicBatching.max_queue_delay_microseconds' , index = 1 , number = 2 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'preserve_ordering' , full_name = 'inference.ModelDynamicBatching.preserve_ordering' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority_levels' , full_name = 'inference.ModelDynamicBatching.priority_levels' , index = 3 , number = 4 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_priority_level' , full_name = 'inference.ModelDynamicBatching.default_priority_level' , index = 4 , number = 5 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_queue_policy' , full_name = 'inference.ModelDynamicBatching.default_queue_policy' , index = 5 , number = 6 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'priority_queue_policy' , full_name = 'inference.ModelDynamicBatching.priority_queue_policy' , index = 6 , number = 7 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 4019 , serialized_end = 4430 , ) _MODELSEQUENCEBATCHING_CONTROL = _descriptor . Descriptor ( name = 'Control' , full_name = 'inference.ModelSequenceBatching.Control' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'kind' , full_name = 'inference.ModelSequenceBatching.Control.kind' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'int32_false_true' , full_name = 'inference.ModelSequenceBatching.Control.int32_false_true' , index = 1 , number = 2 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'fp32_false_true' , full_name = 'inference.ModelSequenceBatching.Control.fp32_false_true' , index = 2 , number = 3 , type = 2 , cpp_type = 6 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'bool_false_true' , full_name = 'inference.ModelSequenceBatching.Control.bool_false_true' , index = 3 , number = 5 , type = 8 , cpp_type = 7 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelSequenceBatching.Control.data_type' , index = 4 , number = 4 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ _MODELSEQUENCEBATCHING_CONTROL_KIND , ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 4758 , serialized_end = 5063 , ) _MODELSEQUENCEBATCHING_CONTROLINPUT = _descriptor . Descriptor ( name = 'ControlInput' , full_name = 'inference.ModelSequenceBatching.ControlInput' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelSequenceBatching.ControlInput.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'control' , full_name = 'inference.ModelSequenceBatching.ControlInput.control' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5065 , serialized_end = 5152 , ) _MODELSEQUENCEBATCHING_INITIALSTATE = _descriptor . Descriptor ( name = 'InitialState' , full_name = 'inference.ModelSequenceBatching.InitialState' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelSequenceBatching.InitialState.data_type' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelSequenceBatching.InitialState.dims' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'zero_data' , full_name = 'inference.ModelSequenceBatching.InitialState.zero_data' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_file' , full_name = 'inference.ModelSequenceBatching.InitialState.data_file' , index = 3 , number = 4 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelSequenceBatching.InitialState.name' , index = 4 , number = 5 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'state_data' , full_name = 'inference.ModelSequenceBatching.InitialState.state_data' , index = 0 , containing_type = None , fields = []), ], serialized_start = 5155 , serialized_end = 5293 , ) _MODELSEQUENCEBATCHING_STATE = _descriptor . Descriptor ( name = 'State' , full_name = 'inference.ModelSequenceBatching.State' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'input_name' , full_name = 'inference.ModelSequenceBatching.State.input_name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_name' , full_name = 'inference.ModelSequenceBatching.State.output_name' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelSequenceBatching.State.data_type' , index = 2 , number = 3 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelSequenceBatching.State.dims' , index = 3 , number = 4 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'initial_state' , full_name = 'inference.ModelSequenceBatching.State.initial_state' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5296 , serialized_end = 5468 , ) _MODELSEQUENCEBATCHING_STRATEGYDIRECT = _descriptor . Descriptor ( name = 'StrategyDirect' , full_name = 'inference.ModelSequenceBatching.StrategyDirect' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'max_queue_delay_microseconds' , full_name = 'inference.ModelSequenceBatching.StrategyDirect.max_queue_delay_microseconds' , index = 0 , number = 1 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'minimum_slot_utilization' , full_name = 'inference.ModelSequenceBatching.StrategyDirect.minimum_slot_utilization' , index = 1 , number = 2 , type = 2 , cpp_type = 6 , label = 1 , has_default_value = False , default_value = float ( 0 ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5470 , serialized_end = 5558 , ) _MODELSEQUENCEBATCHING_STRATEGYOLDEST = _descriptor . Descriptor ( name = 'StrategyOldest' , full_name = 'inference.ModelSequenceBatching.StrategyOldest' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'max_candidate_sequences' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.max_candidate_sequences' , index = 0 , number = 1 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'preferred_batch_size' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.preferred_batch_size' , index = 1 , number = 2 , type = 5 , cpp_type = 1 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_queue_delay_microseconds' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.max_queue_delay_microseconds' , index = 2 , number = 3 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'preserve_ordering' , full_name = 'inference.ModelSequenceBatching.StrategyOldest.preserve_ordering' , index = 3 , number = 4 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5561 , serialized_end = 5705 , ) _MODELSEQUENCEBATCHING = _descriptor . Descriptor ( name = 'ModelSequenceBatching' , full_name = 'inference.ModelSequenceBatching' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'direct' , full_name = 'inference.ModelSequenceBatching.direct' , index = 0 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'oldest' , full_name = 'inference.ModelSequenceBatching.oldest' , index = 1 , number = 4 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_sequence_idle_microseconds' , full_name = 'inference.ModelSequenceBatching.max_sequence_idle_microseconds' , index = 2 , number = 1 , type = 4 , cpp_type = 4 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'control_input' , full_name = 'inference.ModelSequenceBatching.control_input' , index = 3 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'state' , full_name = 'inference.ModelSequenceBatching.state' , index = 4 , number = 5 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELSEQUENCEBATCHING_CONTROL , _MODELSEQUENCEBATCHING_CONTROLINPUT , _MODELSEQUENCEBATCHING_INITIALSTATE , _MODELSEQUENCEBATCHING_STATE , _MODELSEQUENCEBATCHING_STRATEGYDIRECT , _MODELSEQUENCEBATCHING_STRATEGYOLDEST , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'strategy_choice' , full_name = 'inference.ModelSequenceBatching.strategy_choice' , index = 0 , containing_type = None , fields = []), ], serialized_start = 4433 , serialized_end = 5724 , ) _MODELENSEMBLING_STEP_INPUTMAPENTRY = _descriptor . Descriptor ( name = 'InputMapEntry' , full_name = 'inference.ModelEnsembling.Step.InputMapEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelEnsembling.Step.InputMapEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelEnsembling.Step.InputMapEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6004 , serialized_end = 6051 , ) _MODELENSEMBLING_STEP_OUTPUTMAPENTRY = _descriptor . Descriptor ( name = 'OutputMapEntry' , full_name = 'inference.ModelEnsembling.Step.OutputMapEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelEnsembling.Step.OutputMapEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelEnsembling.Step.OutputMapEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6053 , serialized_end = 6101 , ) _MODELENSEMBLING_STEP = _descriptor . Descriptor ( name = 'Step' , full_name = 'inference.ModelEnsembling.Step' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'model_name' , full_name = 'inference.ModelEnsembling.Step.model_name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_version' , full_name = 'inference.ModelEnsembling.Step.model_version' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input_map' , full_name = 'inference.ModelEnsembling.Step.input_map' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output_map' , full_name = 'inference.ModelEnsembling.Step.output_map' , index = 3 , number = 4 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_namespace' , full_name = 'inference.ModelEnsembling.Step.model_namespace' , index = 4 , number = 5 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELENSEMBLING_STEP_INPUTMAPENTRY , _MODELENSEMBLING_STEP_OUTPUTMAPENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5794 , serialized_end = 6101 , ) _MODELENSEMBLING = _descriptor . Descriptor ( name = 'ModelEnsembling' , full_name = 'inference.ModelEnsembling' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'step' , full_name = 'inference.ModelEnsembling.step' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELENSEMBLING_STEP , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 5727 , serialized_end = 6101 , ) _MODELPARAMETER = _descriptor . Descriptor ( name = 'ModelParameter' , full_name = 'inference.ModelParameter' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'string_value' , full_name = 'inference.ModelParameter.string_value' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6103 , serialized_end = 6141 , ) _MODELWARMUP_INPUT = _descriptor . Descriptor ( name = 'Input' , full_name = 'inference.ModelWarmup.Input' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'data_type' , full_name = 'inference.ModelWarmup.Input.data_type' , index = 0 , number = 1 , type = 14 , cpp_type = 8 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dims' , full_name = 'inference.ModelWarmup.Input.dims' , index = 1 , number = 2 , type = 3 , cpp_type = 2 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'zero_data' , full_name = 'inference.ModelWarmup.Input.zero_data' , index = 2 , number = 3 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'random_data' , full_name = 'inference.ModelWarmup.Input.random_data' , index = 3 , number = 4 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input_data_file' , full_name = 'inference.ModelWarmup.Input.input_data_file' , index = 4 , number = 5 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'input_data_type' , full_name = 'inference.ModelWarmup.Input.input_data_type' , index = 0 , containing_type = None , fields = []), ], serialized_start = 6261 , serialized_end = 6412 , ) _MODELWARMUP_INPUTSENTRY = _descriptor . Descriptor ( name = 'InputsEntry' , full_name = 'inference.ModelWarmup.InputsEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelWarmup.InputsEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelWarmup.InputsEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6414 , serialized_end = 6489 , ) _MODELWARMUP = _descriptor . Descriptor ( name = 'ModelWarmup' , full_name = 'inference.ModelWarmup' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelWarmup.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'batch_size' , full_name = 'inference.ModelWarmup.batch_size' , index = 1 , number = 2 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'inputs' , full_name = 'inference.ModelWarmup.inputs' , index = 2 , number = 3 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'count' , full_name = 'inference.ModelWarmup.count' , index = 3 , number = 4 , type = 13 , cpp_type = 3 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELWARMUP_INPUT , _MODELWARMUP_INPUTSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6144 , serialized_end = 6489 , ) _MODELOPERATIONS = _descriptor . Descriptor ( name = 'ModelOperations' , full_name = 'inference.ModelOperations' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'op_library_filename' , full_name = 'inference.ModelOperations.op_library_filename' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6491 , serialized_end = 6537 , ) _MODELTRANSACTIONPOLICY = _descriptor . Descriptor ( name = 'ModelTransactionPolicy' , full_name = 'inference.ModelTransactionPolicy' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'decoupled' , full_name = 'inference.ModelTransactionPolicy.decoupled' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6539 , serialized_end = 6582 , ) _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY = _descriptor . Descriptor ( name = 'ParametersEntry' , full_name = 'inference.ModelRepositoryAgents.Agent.ParametersEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelRepositoryAgents.Agent.ParametersEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelRepositoryAgents.Agent.ParametersEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 3632 , serialized_end = 3681 , ) _MODELREPOSITORYAGENTS_AGENT = _descriptor . Descriptor ( name = 'Agent' , full_name = 'inference.ModelRepositoryAgents.Agent' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelRepositoryAgents.Agent.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'parameters' , full_name = 'inference.ModelRepositoryAgents.Agent.parameters' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6667 , serialized_end = 6815 , ) _MODELREPOSITORYAGENTS = _descriptor . Descriptor ( name = 'ModelRepositoryAgents' , full_name = 'inference.ModelRepositoryAgents' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'agents' , full_name = 'inference.ModelRepositoryAgents.agents' , index = 0 , number = 1 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELREPOSITORYAGENTS_AGENT , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6585 , serialized_end = 6815 , ) _MODELRESPONSECACHE = _descriptor . Descriptor ( name = 'ModelResponseCache' , full_name = 'inference.ModelResponseCache' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'enable' , full_name = 'inference.ModelResponseCache.enable' , index = 0 , number = 1 , type = 8 , cpp_type = 7 , label = 1 , has_default_value = False , default_value = False , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 6817 , serialized_end = 6853 , ) _MODELCONFIG_CCMODELFILENAMESENTRY = _descriptor . Descriptor ( name = 'CcModelFilenamesEntry' , full_name = 'inference.ModelConfig.CcModelFilenamesEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelConfig.CcModelFilenamesEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelConfig.CcModelFilenamesEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 7981 , serialized_end = 8036 , ) _MODELCONFIG_METRICTAGSENTRY = _descriptor . Descriptor ( name = 'MetricTagsEntry' , full_name = 'inference.ModelConfig.MetricTagsEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelConfig.MetricTagsEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelConfig.MetricTagsEntry.value' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 8038 , serialized_end = 8087 , ) _MODELCONFIG_PARAMETERSENTRY = _descriptor . Descriptor ( name = 'ParametersEntry' , full_name = 'inference.ModelConfig.ParametersEntry' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'key' , full_name = 'inference.ModelConfig.ParametersEntry.key' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'value' , full_name = 'inference.ModelConfig.ParametersEntry.value' , index = 1 , number = 2 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [], enum_types = [ ], serialized_options = _b ( '8 \\001 ' ), is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ ], serialized_start = 8089 , serialized_end = 8165 , ) _MODELCONFIG = _descriptor . Descriptor ( name = 'ModelConfig' , full_name = 'inference.ModelConfig' , filename = None , file = DESCRIPTOR , containing_type = None , fields = [ _descriptor . FieldDescriptor ( name = 'name' , full_name = 'inference.ModelConfig.name' , index = 0 , number = 1 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'platform' , full_name = 'inference.ModelConfig.platform' , index = 1 , number = 2 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'backend' , full_name = 'inference.ModelConfig.backend' , index = 2 , number = 17 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'version_policy' , full_name = 'inference.ModelConfig.version_policy' , index = 3 , number = 3 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'max_batch_size' , full_name = 'inference.ModelConfig.max_batch_size' , index = 4 , number = 4 , type = 5 , cpp_type = 1 , label = 1 , has_default_value = False , default_value = 0 , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'input' , full_name = 'inference.ModelConfig.input' , index = 5 , number = 5 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'output' , full_name = 'inference.ModelConfig.output' , index = 6 , number = 6 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'batch_input' , full_name = 'inference.ModelConfig.batch_input' , index = 7 , number = 20 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'batch_output' , full_name = 'inference.ModelConfig.batch_output' , index = 8 , number = 21 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'optimization' , full_name = 'inference.ModelConfig.optimization' , index = 9 , number = 12 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'dynamic_batching' , full_name = 'inference.ModelConfig.dynamic_batching' , index = 10 , number = 11 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'sequence_batching' , full_name = 'inference.ModelConfig.sequence_batching' , index = 11 , number = 13 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'ensemble_scheduling' , full_name = 'inference.ModelConfig.ensemble_scheduling' , index = 12 , number = 15 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'instance_group' , full_name = 'inference.ModelConfig.instance_group' , index = 13 , number = 7 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'default_model_filename' , full_name = 'inference.ModelConfig.default_model_filename' , index = 14 , number = 8 , type = 9 , cpp_type = 9 , label = 1 , has_default_value = False , default_value = _b ( \"\" ) . decode ( 'utf-8' ), message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'cc_model_filenames' , full_name = 'inference.ModelConfig.cc_model_filenames' , index = 15 , number = 9 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'metric_tags' , full_name = 'inference.ModelConfig.metric_tags' , index = 16 , number = 10 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'parameters' , full_name = 'inference.ModelConfig.parameters' , index = 17 , number = 14 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_warmup' , full_name = 'inference.ModelConfig.model_warmup' , index = 18 , number = 16 , type = 11 , cpp_type = 10 , label = 3 , has_default_value = False , default_value = [], message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_operations' , full_name = 'inference.ModelConfig.model_operations' , index = 19 , number = 18 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_transaction_policy' , full_name = 'inference.ModelConfig.model_transaction_policy' , index = 20 , number = 19 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'model_repository_agents' , full_name = 'inference.ModelConfig.model_repository_agents' , index = 21 , number = 23 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), _descriptor . FieldDescriptor ( name = 'response_cache' , full_name = 'inference.ModelConfig.response_cache' , index = 22 , number = 24 , type = 11 , cpp_type = 10 , label = 1 , has_default_value = False , default_value = None , message_type = None , enum_type = None , containing_type = None , is_extension = False , extension_scope = None , serialized_options = None , file = DESCRIPTOR ), ], extensions = [ ], nested_types = [ _MODELCONFIG_CCMODELFILENAMESENTRY , _MODELCONFIG_METRICTAGSENTRY , _MODELCONFIG_PARAMETERSENTRY , ], enum_types = [ ], serialized_options = None , is_extendable = False , syntax = 'proto3' , extension_ranges = [], oneofs = [ _descriptor . OneofDescriptor ( name = 'scheduling_choice' , full_name = 'inference.ModelConfig.scheduling_choice' , index = 0 , containing_type = None , fields = []), ], serialized_start = 6856 , serialized_end = 8186 , ) _MODELRATELIMITER_RESOURCE . containing_type = _MODELRATELIMITER _MODELRATELIMITER . fields_by_name [ 'resources' ] . message_type = _MODELRATELIMITER_RESOURCE _MODELINSTANCEGROUP_SECONDARYDEVICE . fields_by_name [ 'kind' ] . enum_type = _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND _MODELINSTANCEGROUP_SECONDARYDEVICE . containing_type = _MODELINSTANCEGROUP _MODELINSTANCEGROUP_SECONDARYDEVICE_SECONDARYDEVICEKIND . containing_type = _MODELINSTANCEGROUP_SECONDARYDEVICE _MODELINSTANCEGROUP . fields_by_name [ 'kind' ] . enum_type = _MODELINSTANCEGROUP_KIND _MODELINSTANCEGROUP . fields_by_name [ 'rate_limiter' ] . message_type = _MODELRATELIMITER _MODELINSTANCEGROUP . fields_by_name [ 'secondary_devices' ] . message_type = _MODELINSTANCEGROUP_SECONDARYDEVICE _MODELINSTANCEGROUP_KIND . containing_type = _MODELINSTANCEGROUP _MODELINPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELINPUT . fields_by_name [ 'format' ] . enum_type = _MODELINPUT_FORMAT _MODELINPUT . fields_by_name [ 'reshape' ] . message_type = _MODELTENSORRESHAPE _MODELINPUT_FORMAT . containing_type = _MODELINPUT _MODELOUTPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELOUTPUT . fields_by_name [ 'reshape' ] . message_type = _MODELTENSORRESHAPE _BATCHINPUT . fields_by_name [ 'kind' ] . enum_type = _BATCHINPUT_KIND _BATCHINPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _BATCHINPUT_KIND . containing_type = _BATCHINPUT _BATCHOUTPUT . fields_by_name [ 'kind' ] . enum_type = _BATCHOUTPUT_KIND _BATCHOUTPUT_KIND . containing_type = _BATCHOUTPUT _MODELVERSIONPOLICY_LATEST . containing_type = _MODELVERSIONPOLICY _MODELVERSIONPOLICY_ALL . containing_type = _MODELVERSIONPOLICY _MODELVERSIONPOLICY_SPECIFIC . containing_type = _MODELVERSIONPOLICY _MODELVERSIONPOLICY . fields_by_name [ 'latest' ] . message_type = _MODELVERSIONPOLICY_LATEST _MODELVERSIONPOLICY . fields_by_name [ 'all' ] . message_type = _MODELVERSIONPOLICY_ALL _MODELVERSIONPOLICY . fields_by_name [ 'specific' ] . message_type = _MODELVERSIONPOLICY_SPECIFIC _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] . fields . append ( _MODELVERSIONPOLICY . fields_by_name [ 'latest' ]) _MODELVERSIONPOLICY . fields_by_name [ 'latest' ] . containing_oneof = _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] . fields . append ( _MODELVERSIONPOLICY . fields_by_name [ 'all' ]) _MODELVERSIONPOLICY . fields_by_name [ 'all' ] . containing_oneof = _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] . fields . append ( _MODELVERSIONPOLICY . fields_by_name [ 'specific' ]) _MODELVERSIONPOLICY . fields_by_name [ 'specific' ] . containing_oneof = _MODELVERSIONPOLICY . oneofs_by_name [ 'policy_choice' ] _MODELOPTIMIZATIONPOLICY_GRAPH . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY . fields_by_name [ 'value' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND . fields_by_name [ 'input' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY . fields_by_name [ 'value' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC . fields_by_name [ 'input' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC . fields_by_name [ 'graph_lower_bound' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC . containing_type = _MODELOPTIMIZATIONPOLICY_CUDA _MODELOPTIMIZATIONPOLICY_CUDA . fields_by_name [ 'graph_spec' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC _MODELOPTIMIZATIONPOLICY_CUDA . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY . containing_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR . fields_by_name [ 'parameters' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR . containing_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS . fields_by_name [ 'gpu_execution_accelerator' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS . fields_by_name [ 'cpu_execution_accelerator' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER . containing_type = _MODELOPTIMIZATIONPOLICY _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'graph' ] . message_type = _MODELOPTIMIZATIONPOLICY_GRAPH _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'priority' ] . enum_type = _MODELOPTIMIZATIONPOLICY_MODELPRIORITY _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'cuda' ] . message_type = _MODELOPTIMIZATIONPOLICY_CUDA _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'execution_accelerators' ] . message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'input_pinned_memory' ] . message_type = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER _MODELOPTIMIZATIONPOLICY . fields_by_name [ 'output_pinned_memory' ] . message_type = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER _MODELOPTIMIZATIONPOLICY_MODELPRIORITY . containing_type = _MODELOPTIMIZATIONPOLICY _MODELQUEUEPOLICY . fields_by_name [ 'timeout_action' ] . enum_type = _MODELQUEUEPOLICY_TIMEOUTACTION _MODELQUEUEPOLICY_TIMEOUTACTION . containing_type = _MODELQUEUEPOLICY _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY . fields_by_name [ 'value' ] . message_type = _MODELQUEUEPOLICY _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY . containing_type = _MODELDYNAMICBATCHING _MODELDYNAMICBATCHING . fields_by_name [ 'default_queue_policy' ] . message_type = _MODELQUEUEPOLICY _MODELDYNAMICBATCHING . fields_by_name [ 'priority_queue_policy' ] . message_type = _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY _MODELSEQUENCEBATCHING_CONTROL . fields_by_name [ 'kind' ] . enum_type = _MODELSEQUENCEBATCHING_CONTROL_KIND _MODELSEQUENCEBATCHING_CONTROL . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELSEQUENCEBATCHING_CONTROL . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_CONTROL_KIND . containing_type = _MODELSEQUENCEBATCHING_CONTROL _MODELSEQUENCEBATCHING_CONTROLINPUT . fields_by_name [ 'control' ] . message_type = _MODELSEQUENCEBATCHING_CONTROL _MODELSEQUENCEBATCHING_CONTROLINPUT . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELSEQUENCEBATCHING_INITIALSTATE . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] . fields . append ( _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'zero_data' ]) _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'zero_data' ] . containing_oneof = _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] . fields . append ( _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'data_file' ]) _MODELSEQUENCEBATCHING_INITIALSTATE . fields_by_name [ 'data_file' ] . containing_oneof = _MODELSEQUENCEBATCHING_INITIALSTATE . oneofs_by_name [ 'state_data' ] _MODELSEQUENCEBATCHING_STATE . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELSEQUENCEBATCHING_STATE . fields_by_name [ 'initial_state' ] . message_type = _MODELSEQUENCEBATCHING_INITIALSTATE _MODELSEQUENCEBATCHING_STATE . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_STRATEGYDIRECT . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING_STRATEGYOLDEST . containing_type = _MODELSEQUENCEBATCHING _MODELSEQUENCEBATCHING . fields_by_name [ 'direct' ] . message_type = _MODELSEQUENCEBATCHING_STRATEGYDIRECT _MODELSEQUENCEBATCHING . fields_by_name [ 'oldest' ] . message_type = _MODELSEQUENCEBATCHING_STRATEGYOLDEST _MODELSEQUENCEBATCHING . fields_by_name [ 'control_input' ] . message_type = _MODELSEQUENCEBATCHING_CONTROLINPUT _MODELSEQUENCEBATCHING . fields_by_name [ 'state' ] . message_type = _MODELSEQUENCEBATCHING_STATE _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] . fields . append ( _MODELSEQUENCEBATCHING . fields_by_name [ 'direct' ]) _MODELSEQUENCEBATCHING . fields_by_name [ 'direct' ] . containing_oneof = _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] . fields . append ( _MODELSEQUENCEBATCHING . fields_by_name [ 'oldest' ]) _MODELSEQUENCEBATCHING . fields_by_name [ 'oldest' ] . containing_oneof = _MODELSEQUENCEBATCHING . oneofs_by_name [ 'strategy_choice' ] _MODELENSEMBLING_STEP_INPUTMAPENTRY . containing_type = _MODELENSEMBLING_STEP _MODELENSEMBLING_STEP_OUTPUTMAPENTRY . containing_type = _MODELENSEMBLING_STEP _MODELENSEMBLING_STEP . fields_by_name [ 'input_map' ] . message_type = _MODELENSEMBLING_STEP_INPUTMAPENTRY _MODELENSEMBLING_STEP . fields_by_name [ 'output_map' ] . message_type = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY _MODELENSEMBLING_STEP . containing_type = _MODELENSEMBLING _MODELENSEMBLING . fields_by_name [ 'step' ] . message_type = _MODELENSEMBLING_STEP _MODELWARMUP_INPUT . fields_by_name [ 'data_type' ] . enum_type = _DATATYPE _MODELWARMUP_INPUT . containing_type = _MODELWARMUP _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] . fields . append ( _MODELWARMUP_INPUT . fields_by_name [ 'zero_data' ]) _MODELWARMUP_INPUT . fields_by_name [ 'zero_data' ] . containing_oneof = _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] . fields . append ( _MODELWARMUP_INPUT . fields_by_name [ 'random_data' ]) _MODELWARMUP_INPUT . fields_by_name [ 'random_data' ] . containing_oneof = _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] . fields . append ( _MODELWARMUP_INPUT . fields_by_name [ 'input_data_file' ]) _MODELWARMUP_INPUT . fields_by_name [ 'input_data_file' ] . containing_oneof = _MODELWARMUP_INPUT . oneofs_by_name [ 'input_data_type' ] _MODELWARMUP_INPUTSENTRY . fields_by_name [ 'value' ] . message_type = _MODELWARMUP_INPUT _MODELWARMUP_INPUTSENTRY . containing_type = _MODELWARMUP _MODELWARMUP . fields_by_name [ 'inputs' ] . message_type = _MODELWARMUP_INPUTSENTRY _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY . containing_type = _MODELREPOSITORYAGENTS_AGENT _MODELREPOSITORYAGENTS_AGENT . fields_by_name [ 'parameters' ] . message_type = _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY _MODELREPOSITORYAGENTS_AGENT . containing_type = _MODELREPOSITORYAGENTS _MODELREPOSITORYAGENTS . fields_by_name [ 'agents' ] . message_type = _MODELREPOSITORYAGENTS_AGENT _MODELCONFIG_CCMODELFILENAMESENTRY . containing_type = _MODELCONFIG _MODELCONFIG_METRICTAGSENTRY . containing_type = _MODELCONFIG _MODELCONFIG_PARAMETERSENTRY . fields_by_name [ 'value' ] . message_type = _MODELPARAMETER _MODELCONFIG_PARAMETERSENTRY . containing_type = _MODELCONFIG _MODELCONFIG . fields_by_name [ 'version_policy' ] . message_type = _MODELVERSIONPOLICY _MODELCONFIG . fields_by_name [ 'input' ] . message_type = _MODELINPUT _MODELCONFIG . fields_by_name [ 'output' ] . message_type = _MODELOUTPUT _MODELCONFIG . fields_by_name [ 'batch_input' ] . message_type = _BATCHINPUT _MODELCONFIG . fields_by_name [ 'batch_output' ] . message_type = _BATCHOUTPUT _MODELCONFIG . fields_by_name [ 'optimization' ] . message_type = _MODELOPTIMIZATIONPOLICY _MODELCONFIG . fields_by_name [ 'dynamic_batching' ] . message_type = _MODELDYNAMICBATCHING _MODELCONFIG . fields_by_name [ 'sequence_batching' ] . message_type = _MODELSEQUENCEBATCHING _MODELCONFIG . fields_by_name [ 'ensemble_scheduling' ] . message_type = _MODELENSEMBLING _MODELCONFIG . fields_by_name [ 'instance_group' ] . message_type = _MODELINSTANCEGROUP _MODELCONFIG . fields_by_name [ 'cc_model_filenames' ] . message_type = _MODELCONFIG_CCMODELFILENAMESENTRY _MODELCONFIG . fields_by_name [ 'metric_tags' ] . message_type = _MODELCONFIG_METRICTAGSENTRY _MODELCONFIG . fields_by_name [ 'parameters' ] . message_type = _MODELCONFIG_PARAMETERSENTRY _MODELCONFIG . fields_by_name [ 'model_warmup' ] . message_type = _MODELWARMUP _MODELCONFIG . fields_by_name [ 'model_operations' ] . message_type = _MODELOPERATIONS _MODELCONFIG . fields_by_name [ 'model_transaction_policy' ] . message_type = _MODELTRANSACTIONPOLICY _MODELCONFIG . fields_by_name [ 'model_repository_agents' ] . message_type = _MODELREPOSITORYAGENTS _MODELCONFIG . fields_by_name [ 'response_cache' ] . message_type = _MODELRESPONSECACHE _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] . fields . append ( _MODELCONFIG . fields_by_name [ 'dynamic_batching' ]) _MODELCONFIG . fields_by_name [ 'dynamic_batching' ] . containing_oneof = _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] . fields . append ( _MODELCONFIG . fields_by_name [ 'sequence_batching' ]) _MODELCONFIG . fields_by_name [ 'sequence_batching' ] . containing_oneof = _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] . fields . append ( _MODELCONFIG . fields_by_name [ 'ensemble_scheduling' ]) _MODELCONFIG . fields_by_name [ 'ensemble_scheduling' ] . containing_oneof = _MODELCONFIG . oneofs_by_name [ 'scheduling_choice' ] DESCRIPTOR . message_types_by_name [ 'ModelRateLimiter' ] = _MODELRATELIMITER DESCRIPTOR . message_types_by_name [ 'ModelInstanceGroup' ] = _MODELINSTANCEGROUP DESCRIPTOR . message_types_by_name [ 'ModelTensorReshape' ] = _MODELTENSORRESHAPE DESCRIPTOR . message_types_by_name [ 'ModelInput' ] = _MODELINPUT DESCRIPTOR . message_types_by_name [ 'ModelOutput' ] = _MODELOUTPUT DESCRIPTOR . message_types_by_name [ 'BatchInput' ] = _BATCHINPUT DESCRIPTOR . message_types_by_name [ 'BatchOutput' ] = _BATCHOUTPUT DESCRIPTOR . message_types_by_name [ 'ModelVersionPolicy' ] = _MODELVERSIONPOLICY DESCRIPTOR . message_types_by_name [ 'ModelOptimizationPolicy' ] = _MODELOPTIMIZATIONPOLICY DESCRIPTOR . message_types_by_name [ 'ModelQueuePolicy' ] = _MODELQUEUEPOLICY DESCRIPTOR . message_types_by_name [ 'ModelDynamicBatching' ] = _MODELDYNAMICBATCHING DESCRIPTOR . message_types_by_name [ 'ModelSequenceBatching' ] = _MODELSEQUENCEBATCHING DESCRIPTOR . message_types_by_name [ 'ModelEnsembling' ] = _MODELENSEMBLING DESCRIPTOR . message_types_by_name [ 'ModelParameter' ] = _MODELPARAMETER DESCRIPTOR . message_types_by_name [ 'ModelWarmup' ] = _MODELWARMUP DESCRIPTOR . message_types_by_name [ 'ModelOperations' ] = _MODELOPERATIONS DESCRIPTOR . message_types_by_name [ 'ModelTransactionPolicy' ] = _MODELTRANSACTIONPOLICY DESCRIPTOR . message_types_by_name [ 'ModelRepositoryAgents' ] = _MODELREPOSITORYAGENTS DESCRIPTOR . message_types_by_name [ 'ModelResponseCache' ] = _MODELRESPONSECACHE DESCRIPTOR . message_types_by_name [ 'ModelConfig' ] = _MODELCONFIG DESCRIPTOR . enum_types_by_name [ 'DataType' ] = _DATATYPE _sym_db . RegisterFileDescriptor ( DESCRIPTOR ) ModelRateLimiter = _reflection . GeneratedProtocolMessageType ( 'ModelRateLimiter' , ( _message . Message ,), dict ( Resource = _reflection . GeneratedProtocolMessageType ( 'Resource' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELRATELIMITER_RESOURCE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRateLimiter.Resource) )) , DESCRIPTOR = _MODELRATELIMITER , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRateLimiter) )) _sym_db . RegisterMessage ( ModelRateLimiter ) _sym_db . RegisterMessage ( ModelRateLimiter . Resource ) ModelInstanceGroup = _reflection . GeneratedProtocolMessageType ( 'ModelInstanceGroup' , ( _message . Message ,), dict ( SecondaryDevice = _reflection . GeneratedProtocolMessageType ( 'SecondaryDevice' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELINSTANCEGROUP_SECONDARYDEVICE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelInstanceGroup.SecondaryDevice) )) , DESCRIPTOR = _MODELINSTANCEGROUP , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelInstanceGroup) )) _sym_db . RegisterMessage ( ModelInstanceGroup ) _sym_db . RegisterMessage ( ModelInstanceGroup . SecondaryDevice ) ModelTensorReshape = _reflection . GeneratedProtocolMessageType ( 'ModelTensorReshape' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELTENSORRESHAPE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelTensorReshape) )) _sym_db . RegisterMessage ( ModelTensorReshape ) ModelInput = _reflection . GeneratedProtocolMessageType ( 'ModelInput' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELINPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelInput) )) _sym_db . RegisterMessage ( ModelInput ) ModelOutput = _reflection . GeneratedProtocolMessageType ( 'ModelOutput' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOUTPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOutput) )) _sym_db . RegisterMessage ( ModelOutput ) BatchInput = _reflection . GeneratedProtocolMessageType ( 'BatchInput' , ( _message . Message ,), dict ( DESCRIPTOR = _BATCHINPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.BatchInput) )) _sym_db . RegisterMessage ( BatchInput ) BatchOutput = _reflection . GeneratedProtocolMessageType ( 'BatchOutput' , ( _message . Message ,), dict ( DESCRIPTOR = _BATCHOUTPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.BatchOutput) )) _sym_db . RegisterMessage ( BatchOutput ) ModelVersionPolicy = _reflection . GeneratedProtocolMessageType ( 'ModelVersionPolicy' , ( _message . Message ,), dict ( Latest = _reflection . GeneratedProtocolMessageType ( 'Latest' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELVERSIONPOLICY_LATEST , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Latest) )) , All = _reflection . GeneratedProtocolMessageType ( 'All' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELVERSIONPOLICY_ALL , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.All) )) , Specific = _reflection . GeneratedProtocolMessageType ( 'Specific' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELVERSIONPOLICY_SPECIFIC , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Specific) )) , DESCRIPTOR = _MODELVERSIONPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy) )) _sym_db . RegisterMessage ( ModelVersionPolicy ) _sym_db . RegisterMessage ( ModelVersionPolicy . Latest ) _sym_db . RegisterMessage ( ModelVersionPolicy . All ) _sym_db . RegisterMessage ( ModelVersionPolicy . Specific ) ModelOptimizationPolicy = _reflection . GeneratedProtocolMessageType ( 'ModelOptimizationPolicy' , ( _message . Message ,), dict ( Graph = _reflection . GeneratedProtocolMessageType ( 'Graph' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_GRAPH , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Graph) )) , Cuda = _reflection . GeneratedProtocolMessageType ( 'Cuda' , ( _message . Message ,), dict ( GraphSpec = _reflection . GeneratedProtocolMessageType ( 'GraphSpec' , ( _message . Message ,), dict ( Shape = _reflection . GeneratedProtocolMessageType ( 'Shape' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_SHAPE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape) )) , LowerBound = _reflection . GeneratedProtocolMessageType ( 'LowerBound' , ( _message . Message ,), dict ( InputEntry = _reflection . GeneratedProtocolMessageType ( 'InputEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.InputEntry) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound) )) , InputEntry = _reflection . GeneratedProtocolMessageType ( 'InputEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.InputEntry) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda) )) , ExecutionAccelerators = _reflection . GeneratedProtocolMessageType ( 'ExecutionAccelerators' , ( _message . Message ,), dict ( Accelerator = _reflection . GeneratedProtocolMessageType ( 'Accelerator' , ( _message . Message ,), dict ( ParametersEntry = _reflection . GeneratedProtocolMessageType ( 'ParametersEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators) )) , PinnedMemoryBuffer = _reflection . GeneratedProtocolMessageType ( 'PinnedMemoryBuffer' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.PinnedMemoryBuffer) )) , DESCRIPTOR = _MODELOPTIMIZATIONPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy) )) _sym_db . RegisterMessage ( ModelOptimizationPolicy ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Graph ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . Shape ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . LowerBound ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . LowerBound . InputEntry ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . Cuda . GraphSpec . InputEntry ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . ExecutionAccelerators ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . ExecutionAccelerators . Accelerator ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . ExecutionAccelerators . Accelerator . ParametersEntry ) _sym_db . RegisterMessage ( ModelOptimizationPolicy . PinnedMemoryBuffer ) ModelQueuePolicy = _reflection . GeneratedProtocolMessageType ( 'ModelQueuePolicy' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELQUEUEPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelQueuePolicy) )) _sym_db . RegisterMessage ( ModelQueuePolicy ) ModelDynamicBatching = _reflection . GeneratedProtocolMessageType ( 'ModelDynamicBatching' , ( _message . Message ,), dict ( PriorityQueuePolicyEntry = _reflection . GeneratedProtocolMessageType ( 'PriorityQueuePolicyEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelDynamicBatching.PriorityQueuePolicyEntry) )) , DESCRIPTOR = _MODELDYNAMICBATCHING , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelDynamicBatching) )) _sym_db . RegisterMessage ( ModelDynamicBatching ) _sym_db . RegisterMessage ( ModelDynamicBatching . PriorityQueuePolicyEntry ) ModelSequenceBatching = _reflection . GeneratedProtocolMessageType ( 'ModelSequenceBatching' , ( _message . Message ,), dict ( Control = _reflection . GeneratedProtocolMessageType ( 'Control' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROL , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.Control) )) , ControlInput = _reflection . GeneratedProtocolMessageType ( 'ControlInput' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROLINPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.ControlInput) )) , InitialState = _reflection . GeneratedProtocolMessageType ( 'InitialState' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_INITIALSTATE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.InitialState) )) , State = _reflection . GeneratedProtocolMessageType ( 'State' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_STATE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.State) )) , StrategyDirect = _reflection . GeneratedProtocolMessageType ( 'StrategyDirect' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_STRATEGYDIRECT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyDirect) )) , StrategyOldest = _reflection . GeneratedProtocolMessageType ( 'StrategyOldest' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELSEQUENCEBATCHING_STRATEGYOLDEST , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyOldest) )) , DESCRIPTOR = _MODELSEQUENCEBATCHING , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching) )) _sym_db . RegisterMessage ( ModelSequenceBatching ) _sym_db . RegisterMessage ( ModelSequenceBatching . Control ) _sym_db . RegisterMessage ( ModelSequenceBatching . ControlInput ) _sym_db . RegisterMessage ( ModelSequenceBatching . InitialState ) _sym_db . RegisterMessage ( ModelSequenceBatching . State ) _sym_db . RegisterMessage ( ModelSequenceBatching . StrategyDirect ) _sym_db . RegisterMessage ( ModelSequenceBatching . StrategyOldest ) ModelEnsembling = _reflection . GeneratedProtocolMessageType ( 'ModelEnsembling' , ( _message . Message ,), dict ( Step = _reflection . GeneratedProtocolMessageType ( 'Step' , ( _message . Message ,), dict ( InputMapEntry = _reflection . GeneratedProtocolMessageType ( 'InputMapEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELENSEMBLING_STEP_INPUTMAPENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step.InputMapEntry) )) , OutputMapEntry = _reflection . GeneratedProtocolMessageType ( 'OutputMapEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step.OutputMapEntry) )) , DESCRIPTOR = _MODELENSEMBLING_STEP , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step) )) , DESCRIPTOR = _MODELENSEMBLING , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelEnsembling) )) _sym_db . RegisterMessage ( ModelEnsembling ) _sym_db . RegisterMessage ( ModelEnsembling . Step ) _sym_db . RegisterMessage ( ModelEnsembling . Step . InputMapEntry ) _sym_db . RegisterMessage ( ModelEnsembling . Step . OutputMapEntry ) ModelParameter = _reflection . GeneratedProtocolMessageType ( 'ModelParameter' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELPARAMETER , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelParameter) )) _sym_db . RegisterMessage ( ModelParameter ) ModelWarmup = _reflection . GeneratedProtocolMessageType ( 'ModelWarmup' , ( _message . Message ,), dict ( Input = _reflection . GeneratedProtocolMessageType ( 'Input' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELWARMUP_INPUT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelWarmup.Input) )) , InputsEntry = _reflection . GeneratedProtocolMessageType ( 'InputsEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELWARMUP_INPUTSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelWarmup.InputsEntry) )) , DESCRIPTOR = _MODELWARMUP , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelWarmup) )) _sym_db . RegisterMessage ( ModelWarmup ) _sym_db . RegisterMessage ( ModelWarmup . Input ) _sym_db . RegisterMessage ( ModelWarmup . InputsEntry ) ModelOperations = _reflection . GeneratedProtocolMessageType ( 'ModelOperations' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELOPERATIONS , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelOperations) )) _sym_db . RegisterMessage ( ModelOperations ) ModelTransactionPolicy = _reflection . GeneratedProtocolMessageType ( 'ModelTransactionPolicy' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELTRANSACTIONPOLICY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelTransactionPolicy) )) _sym_db . RegisterMessage ( ModelTransactionPolicy ) ModelRepositoryAgents = _reflection . GeneratedProtocolMessageType ( 'ModelRepositoryAgents' , ( _message . Message ,), dict ( Agent = _reflection . GeneratedProtocolMessageType ( 'Agent' , ( _message . Message ,), dict ( ParametersEntry = _reflection . GeneratedProtocolMessageType ( 'ParametersEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents.Agent.ParametersEntry) )) , DESCRIPTOR = _MODELREPOSITORYAGENTS_AGENT , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents.Agent) )) , DESCRIPTOR = _MODELREPOSITORYAGENTS , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelRepositoryAgents) )) _sym_db . RegisterMessage ( ModelRepositoryAgents ) _sym_db . RegisterMessage ( ModelRepositoryAgents . Agent ) _sym_db . RegisterMessage ( ModelRepositoryAgents . Agent . ParametersEntry ) ModelResponseCache = _reflection . GeneratedProtocolMessageType ( 'ModelResponseCache' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELRESPONSECACHE , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelResponseCache) )) _sym_db . RegisterMessage ( ModelResponseCache ) ModelConfig = _reflection . GeneratedProtocolMessageType ( 'ModelConfig' , ( _message . Message ,), dict ( CcModelFilenamesEntry = _reflection . GeneratedProtocolMessageType ( 'CcModelFilenamesEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELCONFIG_CCMODELFILENAMESENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig.CcModelFilenamesEntry) )) , MetricTagsEntry = _reflection . GeneratedProtocolMessageType ( 'MetricTagsEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELCONFIG_METRICTAGSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig.MetricTagsEntry) )) , ParametersEntry = _reflection . GeneratedProtocolMessageType ( 'ParametersEntry' , ( _message . Message ,), dict ( DESCRIPTOR = _MODELCONFIG_PARAMETERSENTRY , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig.ParametersEntry) )) , DESCRIPTOR = _MODELCONFIG , __module__ = 'model_config_pb2' # @@protoc_insertion_point(class_scope:inference.ModelConfig) )) _sym_db . RegisterMessage ( ModelConfig ) _sym_db . RegisterMessage ( ModelConfig . CcModelFilenamesEntry ) _sym_db . RegisterMessage ( ModelConfig . MetricTagsEntry ) _sym_db . RegisterMessage ( ModelConfig . ParametersEntry ) _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_LOWERBOUND_INPUTENTRY . _options = None _MODELOPTIMIZATIONPOLICY_CUDA_GRAPHSPEC_INPUTENTRY . _options = None _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY . _options = None _MODELDYNAMICBATCHING_PRIORITYQUEUEPOLICYENTRY . _options = None _MODELENSEMBLING_STEP_INPUTMAPENTRY . _options = None _MODELENSEMBLING_STEP_OUTPUTMAPENTRY . _options = None _MODELWARMUP_INPUTSENTRY . _options = None _MODELREPOSITORYAGENTS_AGENT_PARAMETERSENTRY . _options = None _MODELCONFIG_CCMODELFILENAMESENTRY . _options = None _MODELCONFIG_METRICTAGSENTRY . _options = None _MODELCONFIG_PARAMETERSENTRY . _options = None # @@protoc_insertion_point(module_scope)","title":"Module simaticai.model_config_pb2"},{"location":"reference/simaticai/model_config_pb2.html#variables","text":"DESCRIPTOR DataType TYPE_BF16 TYPE_BOOL TYPE_FP16 TYPE_FP32 TYPE_FP64 TYPE_INT16 TYPE_INT32 TYPE_INT64 TYPE_INT8 TYPE_INVALID TYPE_STRING TYPE_UINT16 TYPE_UINT32 TYPE_UINT64 TYPE_UINT8","title":"Variables"},{"location":"reference/simaticai/model_config_pb2.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/model_config_pb2.html#batchinput","text":"A ProtocolMessage class BatchInput ( / , * args , ** kwargs )","title":"BatchInput"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables","text":"BATCH_ACCUMULATED_ELEMENT_COUNT BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO BATCH_ELEMENT_COUNT BATCH_ITEM_SHAPE BATCH_ITEM_SHAPE_FLATTEN BATCH_MAX_ELEMENT_COUNT_AS_SHAPE DESCRIPTOR Extensions Kind data_type kind source_input target_name","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#batchoutput","text":"A ProtocolMessage class BatchOutput ( / , * args , ** kwargs )","title":"BatchOutput"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_1","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_1","text":"BATCH_SCATTER_WITH_INPUT_SHAPE DESCRIPTOR Extensions Kind kind source_input target_name","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_1","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_1","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_1","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_1","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_1","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_1","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_1","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_1","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_1","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_1","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_1","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_1","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_1","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_1","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_1","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_1","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_1","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_1","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_1","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_1","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_1","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_1","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelconfig","text":"A ProtocolMessage class ModelConfig ( / , * args , ** kwargs )","title":"ModelConfig"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_2","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_2","text":"CcModelFilenamesEntry DESCRIPTOR Extensions MetricTagsEntry ParametersEntry backend batch_input batch_output cc_model_filenames default_model_filename dynamic_batching ensemble_scheduling input instance_group max_batch_size metric_tags model_operations model_repository_agents model_transaction_policy model_warmup name optimization output parameters platform response_cache sequence_batching version_policy","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_2","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_2","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_2","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_2","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_2","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_2","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_2","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_2","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_2","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_2","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_2","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_2","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_2","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_2","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_2","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_2","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_2","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_2","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_2","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_2","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_2","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_2","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modeldynamicbatching","text":"A ProtocolMessage class ModelDynamicBatching ( / , * args , ** kwargs )","title":"ModelDynamicBatching"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_3","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_3","text":"DESCRIPTOR Extensions PriorityQueuePolicyEntry default_priority_level default_queue_policy max_queue_delay_microseconds preferred_batch_size preserve_ordering priority_levels priority_queue_policy","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_3","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_3","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_3","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_3","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_3","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_3","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_3","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_3","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_3","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_3","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_3","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_3","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_3","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_3","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_3","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_3","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_3","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_3","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_3","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_3","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_3","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_3","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelensembling","text":"A ProtocolMessage class ModelEnsembling ( / , * args , ** kwargs )","title":"ModelEnsembling"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_4","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_4","text":"DESCRIPTOR Extensions Step step","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_4","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_4","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_4","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_4","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_4","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_4","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_4","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_4","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_4","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_4","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_4","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_4","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_4","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_4","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_4","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_4","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_4","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_4","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_4","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_4","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_4","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_4","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelinput","text":"A ProtocolMessage class ModelInput ( / , * args , ** kwargs )","title":"ModelInput"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_5","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_5","text":"DESCRIPTOR Extensions FORMAT_NCHW FORMAT_NHWC FORMAT_NONE Format allow_ragged_batch data_type dims format is_shape_tensor name optional reshape","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_5","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_5","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_5","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_5","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_5","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_5","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_5","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_5","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_5","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_5","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_5","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_5","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_5","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_5","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_5","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_5","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_5","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_5","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_5","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_5","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_5","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_5","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelinstancegroup","text":"A ProtocolMessage class ModelInstanceGroup ( / , * args , ** kwargs )","title":"ModelInstanceGroup"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_6","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_6","text":"DESCRIPTOR Extensions KIND_AUTO KIND_CPU KIND_GPU KIND_MODEL Kind SecondaryDevice count gpus host_policy kind name passive profile rate_limiter secondary_devices","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_6","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_6","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_6","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_6","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_6","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_6","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_6","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_6","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_6","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_6","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_6","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_6","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_6","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_6","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_6","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_6","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_6","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_6","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_6","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_6","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_6","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_6","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modeloperations","text":"A ProtocolMessage class ModelOperations ( / , * args , ** kwargs )","title":"ModelOperations"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_7","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_7","text":"DESCRIPTOR Extensions op_library_filename","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_7","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_7","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_7","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_7","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_7","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_7","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_7","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_7","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_7","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_7","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_7","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_7","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_7","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_7","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_7","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_7","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_7","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_7","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_7","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_7","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_7","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_7","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modeloptimizationpolicy","text":"A ProtocolMessage class ModelOptimizationPolicy ( / , * args , ** kwargs )","title":"ModelOptimizationPolicy"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_8","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_8","text":"Cuda DESCRIPTOR ExecutionAccelerators Extensions Graph ModelPriority PRIORITY_DEFAULT PRIORITY_MAX PRIORITY_MIN PinnedMemoryBuffer cuda eager_batching execution_accelerators gather_kernel_buffer_threshold graph input_pinned_memory output_pinned_memory priority","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_8","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_8","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_8","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_8","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_8","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_8","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_8","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_8","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_8","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_8","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_8","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_8","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_8","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_8","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_8","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_8","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_8","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_8","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_8","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_8","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_8","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_8","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modeloutput","text":"A ProtocolMessage class ModelOutput ( / , * args , ** kwargs )","title":"ModelOutput"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_9","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_9","text":"DESCRIPTOR Extensions data_type dims is_shape_tensor label_filename name reshape","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_9","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_9","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_9","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_9","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_9","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_9","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_9","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_9","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_9","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_9","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_9","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_9","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_9","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_9","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_9","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_9","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_9","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_9","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_9","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_9","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_9","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_9","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelparameter","text":"A ProtocolMessage class ModelParameter ( / , * args , ** kwargs )","title":"ModelParameter"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_10","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_10","text":"DESCRIPTOR Extensions string_value","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_10","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_10","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_10","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_10","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_10","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_10","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_10","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_10","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_10","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_10","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_10","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_10","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_10","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_10","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_10","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_10","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_10","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_10","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_10","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_10","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_10","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_10","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelqueuepolicy","text":"A ProtocolMessage class ModelQueuePolicy ( / , * args , ** kwargs )","title":"ModelQueuePolicy"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_11","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_11","text":"DELAY DESCRIPTOR Extensions REJECT TimeoutAction allow_timeout_override default_timeout_microseconds max_queue_size timeout_action","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_11","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_11","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_11","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_11","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_11","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_11","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_11","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_11","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_11","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_11","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_11","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_11","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_11","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_11","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_11","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_11","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_11","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_11","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_11","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_11","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_11","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_11","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelratelimiter","text":"A ProtocolMessage class ModelRateLimiter ( / , * args , ** kwargs )","title":"ModelRateLimiter"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_12","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_12","text":"DESCRIPTOR Extensions Resource priority resources","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_12","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_12","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_12","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_12","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_12","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_12","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_12","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_12","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_12","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_12","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_12","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_12","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_12","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_12","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_12","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_12","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_12","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_12","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_12","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_12","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_12","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_12","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelrepositoryagents","text":"A ProtocolMessage class ModelRepositoryAgents ( / , * args , ** kwargs )","title":"ModelRepositoryAgents"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_13","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_13","text":"Agent DESCRIPTOR Extensions agents","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_13","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_13","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_13","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_13","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_13","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_13","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_13","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_13","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_13","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_13","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_13","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_13","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_13","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_13","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_13","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_13","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_13","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_13","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_13","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_13","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_13","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_13","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelresponsecache","text":"A ProtocolMessage class ModelResponseCache ( / , * args , ** kwargs )","title":"ModelResponseCache"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_14","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_14","text":"DESCRIPTOR Extensions enable","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_14","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_14","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_14","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_14","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_14","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_14","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_14","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_14","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_14","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_14","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_14","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_14","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_14","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_14","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_14","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_14","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_14","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_14","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_14","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_14","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_14","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_14","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelsequencebatching","text":"A ProtocolMessage class ModelSequenceBatching ( / , * args , ** kwargs )","title":"ModelSequenceBatching"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_15","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_15","text":"Control ControlInput DESCRIPTOR Extensions InitialState State StrategyDirect StrategyOldest control_input direct max_sequence_idle_microseconds oldest state","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_15","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_15","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_15","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_15","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_15","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_15","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_15","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_15","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_15","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_15","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_15","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_15","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_15","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_15","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_15","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_15","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_15","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_15","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_15","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_15","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_15","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_15","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modeltensorreshape","text":"A ProtocolMessage class ModelTensorReshape ( / , * args , ** kwargs )","title":"ModelTensorReshape"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_16","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_16","text":"DESCRIPTOR Extensions shape","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_16","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_16","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_16","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_16","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_16","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_16","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_16","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_16","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_16","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_16","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_16","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_16","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_16","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_16","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_16","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_16","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_16","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_16","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_16","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_16","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_16","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_16","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modeltransactionpolicy","text":"A ProtocolMessage class ModelTransactionPolicy ( / , * args , ** kwargs )","title":"ModelTransactionPolicy"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_17","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_17","text":"DESCRIPTOR Extensions decoupled","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_17","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_17","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_17","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_17","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_17","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_17","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_17","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_17","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_17","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_17","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_17","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_17","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_17","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_17","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_17","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_17","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_17","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_17","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_17","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_17","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_17","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_17","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelversionpolicy","text":"A ProtocolMessage class ModelVersionPolicy ( / , * args , ** kwargs )","title":"ModelVersionPolicy"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_18","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_18","text":"All DESCRIPTOR Extensions Latest Specific all latest specific","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_18","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_18","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_18","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_18","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_18","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_18","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_18","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_18","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_18","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_18","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_18","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_18","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_18","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_18","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_18","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_18","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_18","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_18","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_18","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_18","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_18","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_18","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/model_config_pb2.html#modelwarmup","text":"A ProtocolMessage class ModelWarmup ( / , * args , ** kwargs )","title":"ModelWarmup"},{"location":"reference/simaticai/model_config_pb2.html#ancestors-in-mro_19","text":"google.protobuf.pyext._message.CMessage google.protobuf.message.Message","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/model_config_pb2.html#class-variables_19","text":"DESCRIPTOR Extensions Input InputsEntry batch_size count inputs name","title":"Class variables"},{"location":"reference/simaticai/model_config_pb2.html#methods_19","text":"","title":"Methods"},{"location":"reference/simaticai/model_config_pb2.html#bytesize_19","text":"def ByteSize ( ... ) Returns the size of the message in bytes.","title":"ByteSize"},{"location":"reference/simaticai/model_config_pb2.html#clear_19","text":"def Clear ( ... ) Clears the message.","title":"Clear"},{"location":"reference/simaticai/model_config_pb2.html#clearextension_19","text":"def ClearExtension ( ... ) Clears a message field.","title":"ClearExtension"},{"location":"reference/simaticai/model_config_pb2.html#clearfield_19","text":"def ClearField ( ... ) Clears a message field.","title":"ClearField"},{"location":"reference/simaticai/model_config_pb2.html#copyfrom_19","text":"def CopyFrom ( ... ) Copies a protocol message into the current message.","title":"CopyFrom"},{"location":"reference/simaticai/model_config_pb2.html#discardunknownfields_19","text":"def DiscardUnknownFields ( ... ) Discards the unknown fields.","title":"DiscardUnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#findinitializationerrors_19","text":"def FindInitializationErrors ( ... ) Finds unset required fields.","title":"FindInitializationErrors"},{"location":"reference/simaticai/model_config_pb2.html#fromstring_19","text":"def FromString ( ... ) Creates new method instance from given serialized data.","title":"FromString"},{"location":"reference/simaticai/model_config_pb2.html#hasextension_19","text":"def HasExtension ( ... ) Checks if a message field is set.","title":"HasExtension"},{"location":"reference/simaticai/model_config_pb2.html#hasfield_19","text":"def HasField ( ... ) Checks if a message field is set.","title":"HasField"},{"location":"reference/simaticai/model_config_pb2.html#isinitialized_19","text":"def IsInitialized ( ... ) Checks if all required fields of a protocol message are set.","title":"IsInitialized"},{"location":"reference/simaticai/model_config_pb2.html#listfields_19","text":"def ListFields ( ... ) Lists all set fields of a message.","title":"ListFields"},{"location":"reference/simaticai/model_config_pb2.html#mergefrom_19","text":"def MergeFrom ( ... ) Merges a protocol message into the current message.","title":"MergeFrom"},{"location":"reference/simaticai/model_config_pb2.html#mergefromstring_19","text":"def MergeFromString ( ... ) Merges a serialized message into the current message.","title":"MergeFromString"},{"location":"reference/simaticai/model_config_pb2.html#parsefromstring_19","text":"def ParseFromString ( ... ) Parses a serialized message into the current message.","title":"ParseFromString"},{"location":"reference/simaticai/model_config_pb2.html#registerextension_19","text":"def RegisterExtension ( ... ) Registers an extension with the current message.","title":"RegisterExtension"},{"location":"reference/simaticai/model_config_pb2.html#serializepartialtostring_19","text":"def SerializePartialToString ( ... ) Serializes the message to a string, even if it isn't initialized.","title":"SerializePartialToString"},{"location":"reference/simaticai/model_config_pb2.html#serializetostring_19","text":"def SerializeToString ( ... ) Serializes the message to a string, only for initialized messages.","title":"SerializeToString"},{"location":"reference/simaticai/model_config_pb2.html#setinparent_19","text":"def SetInParent ( ... ) Sets the has bit of the given field in its parent message.","title":"SetInParent"},{"location":"reference/simaticai/model_config_pb2.html#unknownfields_19","text":"def UnknownFields ( ... ) Parse unknown field set","title":"UnknownFields"},{"location":"reference/simaticai/model_config_pb2.html#whichoneof_19","text":"def WhichOneof ( ... ) Returns the name of the field set inside a oneof, or None if no field is set.","title":"WhichOneof"},{"location":"reference/simaticai/data/index.html","text":"Module simaticai.data This module contains additional files for AI SDK. These files are necessary for some functionalities of the SDK. View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . \"\" \" This module contains additional files for AI SDK. These files are necessary for some functionalities of the SDK. \"\" \" Sub-modules simaticai.data.schemas","title":"Index"},{"location":"reference/simaticai/data/index.html#module-simaticaidata","text":"This module contains additional files for AI SDK. These files are necessary for some functionalities of the SDK. View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . \"\" \" This module contains additional files for AI SDK. These files are necessary for some functionalities of the SDK. \"\" \"","title":"Module simaticai.data"},{"location":"reference/simaticai/data/index.html#sub-modules","text":"simaticai.data.schemas","title":"Sub-modules"},{"location":"reference/simaticai/data/schemas.html","text":"Module simaticai.data.schemas This module contains the schema files used in validation methods. These JSON schema files describe the configuration YAML files' schema for AI Inference Server. View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . \"\" \" This module contains the schema files used in validation methods. These JSON schema files describe the configuration YAML files' schema for AI Inference Server. \"\" \"","title":"Schemas"},{"location":"reference/simaticai/data/schemas.html#module-simaticaidataschemas","text":"This module contains the schema files used in validation methods. These JSON schema files describe the configuration YAML files' schema for AI Inference Server. View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . \"\" \" This module contains the schema files used in validation methods. These JSON schema files describe the configuration YAML files' schema for AI Inference Server. \"\" \"","title":"Module simaticai.data.schemas"},{"location":"reference/simaticai/helpers/index.html","text":"Module simaticai.helpers Helpers. This module contains functionality that does not belong to the main domain of the simaticai module. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Helpers. This module contains functionality that does not belong to the main domain of the `simaticai` module. \"\"\" import hashlib import os from pathlib import Path from typing import Union def calc_sha ( file_path : Union [ str , os . PathLike ]): file_path = Path ( file_path ) data_buffer = memoryview ( bytearray ( 262144 )) sha_generator = hashlib . sha256 () with open ( file_path , 'rb' , buffering = 0 ) as file : for n in iter ( lambda : file . readinto ( data_buffer ), 0 ): sha_generator . update ( data_buffer [: n ]) return sha_generator . hexdigest () Sub-modules simaticai.helpers.model_config simaticai.helpers.pep508 simaticai.helpers.reporter simaticai.helpers.tempfiles simaticai.helpers.yaml_helper Functions calc_sha def calc_sha ( file_path : Union [ str , os . PathLike ] ) View Source def calc_sha(file_path: Union[str, os.PathLike]): file_path = Path(file_path) data_buffer = memoryview(bytearray(262144)) sha_generator = hashlib.sha256() with open(file_path, 'rb', buffering=0) as file: for n in iter(lambda: file.readinto(data_buffer), 0): sha_generator.update(data_buffer[:n]) return sha_generator.hexdigest()","title":"Index"},{"location":"reference/simaticai/helpers/index.html#module-simaticaihelpers","text":"Helpers. This module contains functionality that does not belong to the main domain of the simaticai module. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Helpers. This module contains functionality that does not belong to the main domain of the `simaticai` module. \"\"\" import hashlib import os from pathlib import Path from typing import Union def calc_sha ( file_path : Union [ str , os . PathLike ]): file_path = Path ( file_path ) data_buffer = memoryview ( bytearray ( 262144 )) sha_generator = hashlib . sha256 () with open ( file_path , 'rb' , buffering = 0 ) as file : for n in iter ( lambda : file . readinto ( data_buffer ), 0 ): sha_generator . update ( data_buffer [: n ]) return sha_generator . hexdigest ()","title":"Module simaticai.helpers"},{"location":"reference/simaticai/helpers/index.html#sub-modules","text":"simaticai.helpers.model_config simaticai.helpers.pep508 simaticai.helpers.reporter simaticai.helpers.tempfiles simaticai.helpers.yaml_helper","title":"Sub-modules"},{"location":"reference/simaticai/helpers/index.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/helpers/index.html#calc_sha","text":"def calc_sha ( file_path : Union [ str , os . PathLike ] ) View Source def calc_sha(file_path: Union[str, os.PathLike]): file_path = Path(file_path) data_buffer = memoryview(bytearray(262144)) sha_generator = hashlib.sha256() with open(file_path, 'rb', buffering=0) as file: for n in iter(lambda: file.readinto(data_buffer), 0): sha_generator.update(data_buffer[:n]) return sha_generator.hexdigest()","title":"calc_sha"},{"location":"reference/simaticai/helpers/model_config.html","text":"Module simaticai.helpers.model_config None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os from onnx import load , TensorProto import logging from typing import Optional , Union from enum import Enum _logger = logging . getLogger ( __name__ ) # TODO: clarify which elem_type in onnx is supported and find appropriat aiis type tensor_type_dict = { TensorProto . FLOAT : { \"data_type\" : \"TYPE_FP32\" , \"aiis_type\" : \"Float32Array\" }, TensorProto . UINT8 : { \"data_type\" : \"TYPE_UINT8\" , \"aiis_type\" : \"Uint8Array\" }, TensorProto . INT8 : { \"data_type\" : \"TYPE_INT8\" , \"aiis_type\" : \"Int8Array\" }, TensorProto . UINT16 : { \"data_type\" : \"TYPE_UINT16\" , \"aiis_type\" : \"Uint16Array\" }, TensorProto . INT16 : { \"data_type\" : \"TYPE_INT16\" , \"aiis_type\" : \"Int16Array\" }, TensorProto . INT32 : { \"data_type\" : \"TYPE_INT32\" , \"aiis_type\" : \"Int32Array\" }, TensorProto . INT64 : { \"data_type\" : \"TYPE_INT64\" , \"aiis_type\" : \"Int64Array\" }, TensorProto . BOOL : { \"data_type\" : \"TYPE_BOOL\" , \"aiis_type\" : \"BooleanArray\" }, TensorProto . FLOAT16 : { \"data_type\" : \"TYPE_FP16\" , \"aiis_type\" : \"Float16Array\" }, TensorProto . BFLOAT16 : { }, TensorProto . DOUBLE : { \"data_type\" : \"TYPE_FP64\" , \"aiis_type\" : \"Float64Array\" }, TensorProto . COMPLEX64 : { }, TensorProto . COMPLEX128 : { }, TensorProto . UINT32 : { \"data_type\" : \"TYPE_UINT32\" , \"aiis_type\" : \"Uint32Array\" }, TensorProto . UINT64 : { \"data_type\" : \"TYPE_UINT64\" , \"aiis_type\" : \"Uint64Array\" }, TensorProto . STRING : { \"data_type\" : \"TYPE_STRING\" , \"aiis_type\" : \"StringArray\" }, TensorProto . FLOAT8E4M3FN : { }, TensorProto . FLOAT8E4M3FNUZ : { }, TensorProto . FLOAT8E5M2 : { }, TensorProto . FLOAT8E5M2FNUZ : { }, TensorProto . UNDEFINED : { }, } def get_data_type ( tensor_proto ): data_type = tensor_type_dict . get ( tensor_proto , {}) . get ( \"data_type\" , None ) if data_type is None : raise ValueError ( f \"Unsupported data type: { tensor_proto } \" ) return data_type def get_aiis_type ( tensor_proto ): aiis_type = tensor_type_dict . get ( tensor_proto , {}) . get ( \"aiis_type\" , None ) if aiis_type is None : raise ValueError ( f \"Unsupported data type: { tensor_proto } \" ) return aiis_type TPL_TENSORRT_ACCELERATOR = \"\"\" optimization {{ execution_accelerators {{ gpu_execution_accelerator : [ {{ name : \"tensorrt\" {extra_parameters} }} ] }} }} \"\"\" class TensorRTOptimization : \"\"\" Class representing TensorRT optimization configuration. This class provides methods to configure the optimization parameters for TensorRT. Attributes: allowed_parameters (list): List of allowed parameter names. parameters (dict): Dictionary containing the optimization parameters. Methods: add_extra_parameter(self, key: str, value: str) Adds an extra parameter to the TensorRT optimization. allowed_parameters: \"precision_mode\", \"trt_engine_cache_enable\", \"trt_engine_cache_path\", \"max_cached_engines\", \"minimum_segment_size\", \"max_workspace_size_bytes\" Intended usage: gpu_accelerator = TensorRTOptimization(precision_mode = TensorRTOptimization.PrecisionMode.FP16) .add_extra_parameter(\"minimum_segment_size\", 3) model_config = ModelConfig(model_config, max_batch_size = 1, optimization = gpu_accelerator) \"\"\" class PrecisionMode ( Enum ): \"\"\" Enum class for different precision modes in TensorRT optimization. \"\"\" FP32 = \"FP32\" FP16 = \"FP16\" allowed_parameters = [ \"precision_mode\" , \"max_cached_engines\" , \"minimum_segment_size\" , \"max_workspace_size_bytes\" , \"trt_engine_cache_enable\" , \"trt_engine_cache_path\" ] def __init__ ( self , precision_mode : PrecisionMode = PrecisionMode . FP32 ): \"\"\" Initializes a new instance of the TensorRTOptimization class. Args: precision_mode (PrecisionMode): The precision mode for the TensorRT optimization. \"\"\" self . parameters = { \"precision_mode\" : precision_mode . value , \"trt_engine_cache_enable\" : \"true\" , \"trt_engine_cache_path\" : \"/tmp/triton\" } def add_extra_parameter ( self , key : str , value : str ): \"\"\" Add extra parameter to the TensorRT optimization. Args: key (str): The key of the parameter. value (str): The value of the parameter. \"\"\" assert key in self . allowed_parameters , f \"Parameter ' { key } ' is not allowed\" if key in self . parameters : _logger . warn ( f \"Parameter ' { key } ' already exists with value { self . parameters [ key ] } and will be overwritten with value { value } \" ) self . parameters [ key ] = value return self def __str__ ( self ): \"\"\" Returns a string representation of the TensorRTOptimization object. \"\"\" return TPL_TENSORRT_ACCELERATOR . format ( extra_parameters = self . _parameters_to_string ()) def _parameters_to_string ( self ): \"\"\" Converts the parameters dictionary to a string representation. \"\"\" return \" \\n\\t\\t \" . join ([ f \"parameters {{ key: \\\" { key } \\\" value: \\\" { value } \\\" }} \" for key , value in self . parameters . items ()]) class Warmup ( Enum ): DISABLED = 0 ZERO_DATA = 1 RANDOM_DATA = 2 def _warmup_str ( warmup ): if Warmup . ZERO_DATA == warmup : return \"zero_data: true\" if Warmup . RANDOM_DATA == warmup : return \"random_data: true\" return \"\" def _var_to_string ( var ) -> str : return f \"\"\" {{ name: \" { var [ 'name' ] } \" data_type: { var [ 'data_type' ] } dims: { var [ 'dims' ] } }} \"\"\" def _warmup_var_to_string ( var , warmup ) -> str : return f \"\"\" \\ {{ key: \" { var [ 'name' ] } \" value: {{ data_type: { var [ 'data_type' ] } dims: { var [ 'dims' ] } { _warmup_str ( warmup ) } }} }} \"\"\" class ModelConfig : def __init__ ( self , onnx_path : Union [ str , os . PathLike ], max_batch_size : int = 0 , warmup : Warmup = Warmup . ZERO_DATA , optimization : Optional [ TensorRTOptimization ] = None ): if onnx_path is None or \"\" == onnx_path : raise AssertionError ( \"ONNX model path must not be empty\" ) model = load ( onnx_path ) if not warmup : self . warmup = Warmup . DISABLED else : self . warmup = warmup self . optimization = optimization self . max_batch_size = max_batch_size dims_from = 1 if self . max_batch_size > 1 else 0 self . inputs = [] self . outputs = [] for input in model . graph . input : tensor = input . type . tensor_type self . inputs . append ({ \"name\" : input . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) if self . max_batch_size > 1 and self . max_batch_size > tensor . shape . dim [ 0 ] . dim_value : _logger . warning ( f \"max_batch_size is greater than dim[0] of input ' { input . name } '\" ) for output in model . graph . output : tensor = output . type . tensor_type self . outputs . append ({ \"name\" : output . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) def __str__ ( self ): input = \", \" . join ([ _var_to_string ( input ) for input in self . inputs ]) output = \", \" . join ([ _var_to_string ( output ) for output in self . outputs ]) warmup = \"\" if self . warmup is not Warmup . DISABLED : warmup_inputs = \", \" . join ([ _warmup_var_to_string ( input , self . warmup ) for input in self . inputs ]) warmup = f \"\"\" \\ model_warmup [ {{ batch_size: 1 inputs { warmup_inputs } }} ] \"\"\" mbs = f \"max_batch_size: { self . max_batch_size } \" if 1 < self . max_batch_size else \"\" return f \"\"\" \\ platform: \"onnxruntime_onnx\" { mbs } input [ { input } ] output [ { output } ] { warmup } { \"\" if self . optimization is None else self . optimization . __str__ () } \"\"\" def __repr__ ( self ): return self . __str__ () Variables TPL_TENSORRT_ACCELERATOR tensor_type_dict Functions get_aiis_type def get_aiis_type ( tensor_proto ) View Source def get_aiis_type ( tensor_proto ): aiis_type = tensor_type_dict . get ( tensor_proto , {}). get ( \"aiis_type\" , None ) if aiis_type is None : raise ValueError ( f \"Unsupported data type: {tensor_proto}\" ) return aiis_type get_data_type def get_data_type ( tensor_proto ) View Source def get_data_type ( tensor_proto ): data_type = tensor_type_dict . get ( tensor_proto , {}). get ( \"data_type\" , None ) if data_type is None : raise ValueError ( f \"Unsupported data type: {tensor_proto}\" ) return data_type Classes ModelConfig class ModelConfig ( onnx_path : Union [ str , os . PathLike ], max_batch_size : int = 0 , warmup : simaticai . helpers . model_config . Warmup = < Warmup . ZERO_DATA : 1 > , optimization : Optional [ simaticai . helpers . model_config . TensorRTOptimization ] = None ) View Source class ModelConfig : def __init__ ( self , onnx_path : Union [ str , os . PathLike ], max_batch_size : int = 0 , warmup : Warmup = Warmup . ZERO_DATA , optimization : Optional [ TensorRTOptimization ] = None ): if onnx_path is None or \"\" == onnx_path : raise AssertionError ( \"ONNX model path must not be empty\" ) model = load ( onnx_path ) if not warmup : self . warmup = Warmup . DISABLED else : self . warmup = warmup self . optimization = optimization self . max_batch_size = max_batch_size dims_from = 1 if self . max_batch_size > 1 else 0 self . inputs = [] self . outputs = [] for input in model . graph . input : tensor = input . type . tensor_type self . inputs . append ({ \"name\" : input . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) if self . max_batch_size > 1 and self . max_batch_size > tensor . shape . dim [ 0 ] . dim_value : _logger . warning ( f \"max_batch_size is greater than dim[0] of input '{input.name}'\" ) for output in model . graph . output : tensor = output . type . tensor_type self . outputs . append ({ \"name\" : output . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) def __str__ ( self ): input = \", \" . join ([ _var_to_string ( input ) for input in self . inputs ]) output = \", \" . join ([ _var_to_string ( output ) for output in self . outputs ]) warmup = \"\" if self . warmup is not Warmup . DISABLED : warmup_inputs = \", \" . join ([ _warmup_var_to_string ( input , self . warmup ) for input in self . inputs ]) warmup = f \"\"\" \\ model_warmup [{{ batch_size: 1 inputs {warmup_inputs} }}] \"\"\" mbs = f \"max_batch_size: {self.max_batch_size}\" if 1 < self . max_batch_size else \"\" return f \"\"\" \\ platform: \"onnxruntime_onnx\" {mbs} input [{input}] output [{output}] {warmup} {\"\" if self.optimization is None else self.optimization.__str__()} \"\"\" def __repr__ ( self ): return self . __str__ () TensorRTOptimization Class representing TensorRT optimization configuration. This class provides methods to configure the optimization parameters for TensorRT. class TensorRTOptimization ( precision_mode : simaticai . helpers . model_config . TensorRTOptimization . PrecisionMode = < PrecisionMode . FP32 : 'FP32' > ) Attributes Name Type Description Default allowed_parameters list List of allowed parameter names. None parameters dict Dictionary containing the optimization parameters. None View Source class TensorRTOptimization : \"\"\" Class representing TensorRT optimization configuration. This class provides methods to configure the optimization parameters for TensorRT. Attributes: allowed_parameters (list): List of allowed parameter names. parameters (dict): Dictionary containing the optimization parameters. Methods: add_extra_parameter(self, key: str, value: str) Adds an extra parameter to the TensorRT optimization. allowed_parameters: \" precision_mode \", \" trt_engine_cache_enable \", \" trt_engine_cache_path \", \" max_cached_engines \", \" minimum_segment_size \", \" max_workspace_size_bytes \" Intended usage: gpu_accelerator = TensorRTOptimization(precision_mode = TensorRTOptimization.PrecisionMode.FP16) .add_extra_parameter(\" minimum_segment_size \", 3) model_config = ModelConfig(model_config, max_batch_size = 1, optimization = gpu_accelerator) \"\"\" class PrecisionMode ( Enum ) : \"\"\" Enum class for different precision modes in TensorRT optimization. \"\"\" FP32 = \"FP32\" FP16 = \"FP16\" allowed_parameters = [ \"precision_mode\", \"max_cached_engines\", \"minimum_segment_size\", \"max_workspace_size_bytes\", \"trt_engine_cache_enable\", \"trt_engine_cache_path\" ] def __init__ ( self , precision_mode : PrecisionMode = PrecisionMode . FP32 ) : \"\"\" Initializes a new instance of the TensorRTOptimization class. Args: precision_mode (PrecisionMode): The precision mode for the TensorRT optimization. \"\"\" self . parameters = { \"precision_mode\" : precision_mode . value , \"trt_engine_cache_enable\" : \"true\" , \"trt_engine_cache_path\" : \"/tmp/triton\" } def add_extra_parameter ( self , key : str , value : str ) : \"\"\" Add extra parameter to the TensorRT optimization. Args: key (str): The key of the parameter. value (str): The value of the parameter. \"\"\" assert key in self . allowed_parameters , f \"Parameter '{key}' is not allowed\" if key in self . parameters : _logger . warn ( f \"Parameter '{key}' already exists with value {self.parameters[key]} and will be overwritten with value {value}\" ) self . parameters [ key ] = value return self def __str__ ( self ) : \"\"\" Returns a string representation of the TensorRTOptimization object. \"\"\" return TPL_TENSORRT_ACCELERATOR . format ( extra_parameters = self . _parameters_to_string ()) def _parameters_to_string ( self ) : \"\"\" Converts the parameters dictionary to a string representation. \"\"\" return \"\\n\\t\\t\" . join ( [ f\"parameters {{ key: \\\"{key}\\\" value: \\\"{value}\\\" }}\" for key, value in self.parameters.items() ] ) Class variables PrecisionMode allowed_parameters Methods add_extra_parameter def add_extra_parameter ( self , key : str , value : str ) Add extra parameter to the TensorRT optimization. Parameters: Name Type Description Default key str The key of the parameter. None value str The value of the parameter. None View Source def add_extra_parameter ( self , key : str , value : str ) : \"\"\" Add extra parameter to the TensorRT optimization. Args: key (str): The key of the parameter. value (str): The value of the parameter. \"\"\" assert key in self . allowed_parameters , f \"Parameter '{key}' is not allowed\" if key in self . parameters : _logger . warn ( f \"Parameter '{key}' already exists with value {self.parameters[key]} and will be overwritten with value {value}\" ) self . parameters [ key ] = value return self Warmup An enumeration. class Warmup ( / , * args , ** kwargs ) View Source class Warmup ( Enum ): DISABLED = 0 ZERO_DATA = 1 RANDOM_DATA = 2 Ancestors (in MRO) enum.Enum Class variables DISABLED RANDOM_DATA ZERO_DATA name value","title":"Model Config"},{"location":"reference/simaticai/helpers/model_config.html#module-simaticaihelpersmodel_config","text":"None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os from onnx import load , TensorProto import logging from typing import Optional , Union from enum import Enum _logger = logging . getLogger ( __name__ ) # TODO: clarify which elem_type in onnx is supported and find appropriat aiis type tensor_type_dict = { TensorProto . FLOAT : { \"data_type\" : \"TYPE_FP32\" , \"aiis_type\" : \"Float32Array\" }, TensorProto . UINT8 : { \"data_type\" : \"TYPE_UINT8\" , \"aiis_type\" : \"Uint8Array\" }, TensorProto . INT8 : { \"data_type\" : \"TYPE_INT8\" , \"aiis_type\" : \"Int8Array\" }, TensorProto . UINT16 : { \"data_type\" : \"TYPE_UINT16\" , \"aiis_type\" : \"Uint16Array\" }, TensorProto . INT16 : { \"data_type\" : \"TYPE_INT16\" , \"aiis_type\" : \"Int16Array\" }, TensorProto . INT32 : { \"data_type\" : \"TYPE_INT32\" , \"aiis_type\" : \"Int32Array\" }, TensorProto . INT64 : { \"data_type\" : \"TYPE_INT64\" , \"aiis_type\" : \"Int64Array\" }, TensorProto . BOOL : { \"data_type\" : \"TYPE_BOOL\" , \"aiis_type\" : \"BooleanArray\" }, TensorProto . FLOAT16 : { \"data_type\" : \"TYPE_FP16\" , \"aiis_type\" : \"Float16Array\" }, TensorProto . BFLOAT16 : { }, TensorProto . DOUBLE : { \"data_type\" : \"TYPE_FP64\" , \"aiis_type\" : \"Float64Array\" }, TensorProto . COMPLEX64 : { }, TensorProto . COMPLEX128 : { }, TensorProto . UINT32 : { \"data_type\" : \"TYPE_UINT32\" , \"aiis_type\" : \"Uint32Array\" }, TensorProto . UINT64 : { \"data_type\" : \"TYPE_UINT64\" , \"aiis_type\" : \"Uint64Array\" }, TensorProto . STRING : { \"data_type\" : \"TYPE_STRING\" , \"aiis_type\" : \"StringArray\" }, TensorProto . FLOAT8E4M3FN : { }, TensorProto . FLOAT8E4M3FNUZ : { }, TensorProto . FLOAT8E5M2 : { }, TensorProto . FLOAT8E5M2FNUZ : { }, TensorProto . UNDEFINED : { }, } def get_data_type ( tensor_proto ): data_type = tensor_type_dict . get ( tensor_proto , {}) . get ( \"data_type\" , None ) if data_type is None : raise ValueError ( f \"Unsupported data type: { tensor_proto } \" ) return data_type def get_aiis_type ( tensor_proto ): aiis_type = tensor_type_dict . get ( tensor_proto , {}) . get ( \"aiis_type\" , None ) if aiis_type is None : raise ValueError ( f \"Unsupported data type: { tensor_proto } \" ) return aiis_type TPL_TENSORRT_ACCELERATOR = \"\"\" optimization {{ execution_accelerators {{ gpu_execution_accelerator : [ {{ name : \"tensorrt\" {extra_parameters} }} ] }} }} \"\"\" class TensorRTOptimization : \"\"\" Class representing TensorRT optimization configuration. This class provides methods to configure the optimization parameters for TensorRT. Attributes: allowed_parameters (list): List of allowed parameter names. parameters (dict): Dictionary containing the optimization parameters. Methods: add_extra_parameter(self, key: str, value: str) Adds an extra parameter to the TensorRT optimization. allowed_parameters: \"precision_mode\", \"trt_engine_cache_enable\", \"trt_engine_cache_path\", \"max_cached_engines\", \"minimum_segment_size\", \"max_workspace_size_bytes\" Intended usage: gpu_accelerator = TensorRTOptimization(precision_mode = TensorRTOptimization.PrecisionMode.FP16) .add_extra_parameter(\"minimum_segment_size\", 3) model_config = ModelConfig(model_config, max_batch_size = 1, optimization = gpu_accelerator) \"\"\" class PrecisionMode ( Enum ): \"\"\" Enum class for different precision modes in TensorRT optimization. \"\"\" FP32 = \"FP32\" FP16 = \"FP16\" allowed_parameters = [ \"precision_mode\" , \"max_cached_engines\" , \"minimum_segment_size\" , \"max_workspace_size_bytes\" , \"trt_engine_cache_enable\" , \"trt_engine_cache_path\" ] def __init__ ( self , precision_mode : PrecisionMode = PrecisionMode . FP32 ): \"\"\" Initializes a new instance of the TensorRTOptimization class. Args: precision_mode (PrecisionMode): The precision mode for the TensorRT optimization. \"\"\" self . parameters = { \"precision_mode\" : precision_mode . value , \"trt_engine_cache_enable\" : \"true\" , \"trt_engine_cache_path\" : \"/tmp/triton\" } def add_extra_parameter ( self , key : str , value : str ): \"\"\" Add extra parameter to the TensorRT optimization. Args: key (str): The key of the parameter. value (str): The value of the parameter. \"\"\" assert key in self . allowed_parameters , f \"Parameter ' { key } ' is not allowed\" if key in self . parameters : _logger . warn ( f \"Parameter ' { key } ' already exists with value { self . parameters [ key ] } and will be overwritten with value { value } \" ) self . parameters [ key ] = value return self def __str__ ( self ): \"\"\" Returns a string representation of the TensorRTOptimization object. \"\"\" return TPL_TENSORRT_ACCELERATOR . format ( extra_parameters = self . _parameters_to_string ()) def _parameters_to_string ( self ): \"\"\" Converts the parameters dictionary to a string representation. \"\"\" return \" \\n\\t\\t \" . join ([ f \"parameters {{ key: \\\" { key } \\\" value: \\\" { value } \\\" }} \" for key , value in self . parameters . items ()]) class Warmup ( Enum ): DISABLED = 0 ZERO_DATA = 1 RANDOM_DATA = 2 def _warmup_str ( warmup ): if Warmup . ZERO_DATA == warmup : return \"zero_data: true\" if Warmup . RANDOM_DATA == warmup : return \"random_data: true\" return \"\" def _var_to_string ( var ) -> str : return f \"\"\" {{ name: \" { var [ 'name' ] } \" data_type: { var [ 'data_type' ] } dims: { var [ 'dims' ] } }} \"\"\" def _warmup_var_to_string ( var , warmup ) -> str : return f \"\"\" \\ {{ key: \" { var [ 'name' ] } \" value: {{ data_type: { var [ 'data_type' ] } dims: { var [ 'dims' ] } { _warmup_str ( warmup ) } }} }} \"\"\" class ModelConfig : def __init__ ( self , onnx_path : Union [ str , os . PathLike ], max_batch_size : int = 0 , warmup : Warmup = Warmup . ZERO_DATA , optimization : Optional [ TensorRTOptimization ] = None ): if onnx_path is None or \"\" == onnx_path : raise AssertionError ( \"ONNX model path must not be empty\" ) model = load ( onnx_path ) if not warmup : self . warmup = Warmup . DISABLED else : self . warmup = warmup self . optimization = optimization self . max_batch_size = max_batch_size dims_from = 1 if self . max_batch_size > 1 else 0 self . inputs = [] self . outputs = [] for input in model . graph . input : tensor = input . type . tensor_type self . inputs . append ({ \"name\" : input . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) if self . max_batch_size > 1 and self . max_batch_size > tensor . shape . dim [ 0 ] . dim_value : _logger . warning ( f \"max_batch_size is greater than dim[0] of input ' { input . name } '\" ) for output in model . graph . output : tensor = output . type . tensor_type self . outputs . append ({ \"name\" : output . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) def __str__ ( self ): input = \", \" . join ([ _var_to_string ( input ) for input in self . inputs ]) output = \", \" . join ([ _var_to_string ( output ) for output in self . outputs ]) warmup = \"\" if self . warmup is not Warmup . DISABLED : warmup_inputs = \", \" . join ([ _warmup_var_to_string ( input , self . warmup ) for input in self . inputs ]) warmup = f \"\"\" \\ model_warmup [ {{ batch_size: 1 inputs { warmup_inputs } }} ] \"\"\" mbs = f \"max_batch_size: { self . max_batch_size } \" if 1 < self . max_batch_size else \"\" return f \"\"\" \\ platform: \"onnxruntime_onnx\" { mbs } input [ { input } ] output [ { output } ] { warmup } { \"\" if self . optimization is None else self . optimization . __str__ () } \"\"\" def __repr__ ( self ): return self . __str__ ()","title":"Module simaticai.helpers.model_config"},{"location":"reference/simaticai/helpers/model_config.html#variables","text":"TPL_TENSORRT_ACCELERATOR tensor_type_dict","title":"Variables"},{"location":"reference/simaticai/helpers/model_config.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/helpers/model_config.html#get_aiis_type","text":"def get_aiis_type ( tensor_proto ) View Source def get_aiis_type ( tensor_proto ): aiis_type = tensor_type_dict . get ( tensor_proto , {}). get ( \"aiis_type\" , None ) if aiis_type is None : raise ValueError ( f \"Unsupported data type: {tensor_proto}\" ) return aiis_type","title":"get_aiis_type"},{"location":"reference/simaticai/helpers/model_config.html#get_data_type","text":"def get_data_type ( tensor_proto ) View Source def get_data_type ( tensor_proto ): data_type = tensor_type_dict . get ( tensor_proto , {}). get ( \"data_type\" , None ) if data_type is None : raise ValueError ( f \"Unsupported data type: {tensor_proto}\" ) return data_type","title":"get_data_type"},{"location":"reference/simaticai/helpers/model_config.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/helpers/model_config.html#modelconfig","text":"class ModelConfig ( onnx_path : Union [ str , os . PathLike ], max_batch_size : int = 0 , warmup : simaticai . helpers . model_config . Warmup = < Warmup . ZERO_DATA : 1 > , optimization : Optional [ simaticai . helpers . model_config . TensorRTOptimization ] = None ) View Source class ModelConfig : def __init__ ( self , onnx_path : Union [ str , os . PathLike ], max_batch_size : int = 0 , warmup : Warmup = Warmup . ZERO_DATA , optimization : Optional [ TensorRTOptimization ] = None ): if onnx_path is None or \"\" == onnx_path : raise AssertionError ( \"ONNX model path must not be empty\" ) model = load ( onnx_path ) if not warmup : self . warmup = Warmup . DISABLED else : self . warmup = warmup self . optimization = optimization self . max_batch_size = max_batch_size dims_from = 1 if self . max_batch_size > 1 else 0 self . inputs = [] self . outputs = [] for input in model . graph . input : tensor = input . type . tensor_type self . inputs . append ({ \"name\" : input . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) if self . max_batch_size > 1 and self . max_batch_size > tensor . shape . dim [ 0 ] . dim_value : _logger . warning ( f \"max_batch_size is greater than dim[0] of input '{input.name}'\" ) for output in model . graph . output : tensor = output . type . tensor_type self . outputs . append ({ \"name\" : output . name , \"type\" : get_aiis_type ( tensor . elem_type ), \"data_type\" : get_data_type ( tensor . elem_type ), \"dims\" : [ k . dim_value for k in tensor . shape . dim [ dims_from :]], }) def __str__ ( self ): input = \", \" . join ([ _var_to_string ( input ) for input in self . inputs ]) output = \", \" . join ([ _var_to_string ( output ) for output in self . outputs ]) warmup = \"\" if self . warmup is not Warmup . DISABLED : warmup_inputs = \", \" . join ([ _warmup_var_to_string ( input , self . warmup ) for input in self . inputs ]) warmup = f \"\"\" \\ model_warmup [{{ batch_size: 1 inputs {warmup_inputs} }}] \"\"\" mbs = f \"max_batch_size: {self.max_batch_size}\" if 1 < self . max_batch_size else \"\" return f \"\"\" \\ platform: \"onnxruntime_onnx\" {mbs} input [{input}] output [{output}] {warmup} {\"\" if self.optimization is None else self.optimization.__str__()} \"\"\" def __repr__ ( self ): return self . __str__ ()","title":"ModelConfig"},{"location":"reference/simaticai/helpers/model_config.html#tensorrtoptimization","text":"Class representing TensorRT optimization configuration. This class provides methods to configure the optimization parameters for TensorRT. class TensorRTOptimization ( precision_mode : simaticai . helpers . model_config . TensorRTOptimization . PrecisionMode = < PrecisionMode . FP32 : 'FP32' > )","title":"TensorRTOptimization"},{"location":"reference/simaticai/helpers/model_config.html#attributes","text":"Name Type Description Default allowed_parameters list List of allowed parameter names. None parameters dict Dictionary containing the optimization parameters. None View Source class TensorRTOptimization : \"\"\" Class representing TensorRT optimization configuration. This class provides methods to configure the optimization parameters for TensorRT. Attributes: allowed_parameters (list): List of allowed parameter names. parameters (dict): Dictionary containing the optimization parameters. Methods: add_extra_parameter(self, key: str, value: str) Adds an extra parameter to the TensorRT optimization. allowed_parameters: \" precision_mode \", \" trt_engine_cache_enable \", \" trt_engine_cache_path \", \" max_cached_engines \", \" minimum_segment_size \", \" max_workspace_size_bytes \" Intended usage: gpu_accelerator = TensorRTOptimization(precision_mode = TensorRTOptimization.PrecisionMode.FP16) .add_extra_parameter(\" minimum_segment_size \", 3) model_config = ModelConfig(model_config, max_batch_size = 1, optimization = gpu_accelerator) \"\"\" class PrecisionMode ( Enum ) : \"\"\" Enum class for different precision modes in TensorRT optimization. \"\"\" FP32 = \"FP32\" FP16 = \"FP16\" allowed_parameters = [ \"precision_mode\", \"max_cached_engines\", \"minimum_segment_size\", \"max_workspace_size_bytes\", \"trt_engine_cache_enable\", \"trt_engine_cache_path\" ] def __init__ ( self , precision_mode : PrecisionMode = PrecisionMode . FP32 ) : \"\"\" Initializes a new instance of the TensorRTOptimization class. Args: precision_mode (PrecisionMode): The precision mode for the TensorRT optimization. \"\"\" self . parameters = { \"precision_mode\" : precision_mode . value , \"trt_engine_cache_enable\" : \"true\" , \"trt_engine_cache_path\" : \"/tmp/triton\" } def add_extra_parameter ( self , key : str , value : str ) : \"\"\" Add extra parameter to the TensorRT optimization. Args: key (str): The key of the parameter. value (str): The value of the parameter. \"\"\" assert key in self . allowed_parameters , f \"Parameter '{key}' is not allowed\" if key in self . parameters : _logger . warn ( f \"Parameter '{key}' already exists with value {self.parameters[key]} and will be overwritten with value {value}\" ) self . parameters [ key ] = value return self def __str__ ( self ) : \"\"\" Returns a string representation of the TensorRTOptimization object. \"\"\" return TPL_TENSORRT_ACCELERATOR . format ( extra_parameters = self . _parameters_to_string ()) def _parameters_to_string ( self ) : \"\"\" Converts the parameters dictionary to a string representation. \"\"\" return \"\\n\\t\\t\" . join ( [ f\"parameters {{ key: \\\"{key}\\\" value: \\\"{value}\\\" }}\" for key, value in self.parameters.items() ] )","title":"Attributes"},{"location":"reference/simaticai/helpers/model_config.html#class-variables","text":"PrecisionMode allowed_parameters","title":"Class variables"},{"location":"reference/simaticai/helpers/model_config.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/helpers/model_config.html#add_extra_parameter","text":"def add_extra_parameter ( self , key : str , value : str ) Add extra parameter to the TensorRT optimization. Parameters: Name Type Description Default key str The key of the parameter. None value str The value of the parameter. None View Source def add_extra_parameter ( self , key : str , value : str ) : \"\"\" Add extra parameter to the TensorRT optimization. Args: key (str): The key of the parameter. value (str): The value of the parameter. \"\"\" assert key in self . allowed_parameters , f \"Parameter '{key}' is not allowed\" if key in self . parameters : _logger . warn ( f \"Parameter '{key}' already exists with value {self.parameters[key]} and will be overwritten with value {value}\" ) self . parameters [ key ] = value return self","title":"add_extra_parameter"},{"location":"reference/simaticai/helpers/model_config.html#warmup","text":"An enumeration. class Warmup ( / , * args , ** kwargs ) View Source class Warmup ( Enum ): DISABLED = 0 ZERO_DATA = 1 RANDOM_DATA = 2","title":"Warmup"},{"location":"reference/simaticai/helpers/model_config.html#ancestors-in-mro","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/helpers/model_config.html#class-variables_1","text":"DISABLED RANDOM_DATA ZERO_DATA name value","title":"Class variables"},{"location":"reference/simaticai/helpers/pep508.html","text":"Module simaticai.helpers.pep508 None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os from typing import NamedTuple , Tuple , Optional , Union , List from parsley import makeGrammar # https://peps.python.org/pep-0508/#complete-grammar GRAMMAR = \"\"\" wsp = ' ' | ' \\t ' version_cmp = wsp* <'<=' | '<' | '!=' | '==' | '>=' | '>' | '~=' | '==='> version = wsp* <( letterOrDigit | '-' | '_' | '.' | '*' | '+' | '!' )+> version_one = version_cmp:op version:v wsp* -> (op, v) version_many = version_one:v1 (wsp* ',' version_one)*:v2 -> [v1] + v2 versionspec = ('(' version_many:v ')' ->v) | version_many urlspec = '@' wsp* <URI_reference> marker_op = version_cmp | (wsp* 'in') | (wsp* 'not' wsp+ 'in') python_str_c = (wsp | letter | digit | '(' | ')' | '.' | '{' | '}' | '-' | '_' | '*' | '#' | ':' | ';' | ',' | '/' | '?' | '[' | ']' | '!' | '~' | '`' | '@' | '$' | '%' | '^' | '&' | '=' | '+' | '|' | '<' | '>' ) dquote = '\"' squote = ' \\\\ '' python_str = (squote <(python_str_c | dquote)*>:s squote | dquote <(python_str_c | squote)*>:s dquote) -> s env_var = ( 'python_version' | 'python_full_version' | 'os_name' | 'sys_platform' | 'platform_release' | 'platform_system' | 'platform_version' | 'platform_machine' | 'platform_python_implementation' | 'implementation_name' | 'implementation_version' | 'extra' # ONLY when defined by a containing layer ) marker_var = wsp* (env_var | python_str) marker_expr = marker_var:l marker_op:o marker_var:r -> Marker(o, l, r) | wsp* '(' marker:m wsp* ')' -> m marker_and = marker_expr:l wsp* 'and' marker_expr:r -> Marker('and', l, r) | marker_expr:m -> m marker_or = marker_and:l wsp* 'or' marker_and:r -> Marker('or', l, r) | marker_and:m -> m marker = marker_or quoted_marker = ';' wsp* marker identifier_end = letterOrDigit | (('-' | '_' | '.' )* letterOrDigit) identifier = < letterOrDigit identifier_end* > name = identifier extras_list = identifier:i (wsp* ',' wsp* identifier)*:ids -> [i] + ids extras = '[' wsp* extras_list?:e wsp* ']' -> e name_req = (name:n wsp* extras?:e wsp* versionspec?:v wsp* quoted_marker?:m -> (n, e or [], v or [], m)) url_req = (name:n wsp* extras?:e wsp* urlspec:v (wsp+ | end) quoted_marker?:m -> (n, e or [], v, m)) specification = wsp* ( url_req | name_req ):s wsp* -> Spec(*s) URI_reference = <URI | relative_ref> URI = scheme ':' hier_part ('?' query )? ('#' fragment)? hier_part = ('//' authority path_abempty) | path_absolute | path_rootless | path_empty absolute_URI = scheme ':' hier_part ( '?' query )? relative_ref = relative_part ( '?' query )? ('#' fragment)? relative_part = '//' authority path_abempty | path_absolute | path_noscheme | path_empty scheme = letter ( letter | digit | '+' | '-' | '.')* authority = ( userinfo '@' )? host ( ':' port )? userinfo = ( unreserved | pct_encoded | sub_delims | ':')* host = IP_literal | IPv4address | reg_name port = digit* IP_literal = '[' ( IPv6address | IPvFuture) ']' IPvFuture = 'v' hexdig+ '.' ( unreserved | sub_delims | ':')+ IPv6address = ( ( h16 ':') {6} ls32 | '::' ( h16 ':') {5} ls32 | ( h16 )? '::' ( h16 ':') {4} ls32 | ( ( h16 ':')? h16 )? '::' ( h16 ':') {3} ls32 | ( ( h16 ':'){0,2} h16 )? '::' ( h16 ':') {2} ls32 | ( ( h16 ':'){0,3} h16 )? '::' h16 ':' ls32 | ( ( h16 ':'){0,4} h16 )? '::' ls32 | ( ( h16 ':'){0,5} h16 )? '::' h16 | ( ( h16 ':'){0,6} h16 )? '::' ) h16 = hexdig{1,4} ls32 = ( h16 ':' h16) | IPv4address IPv4address = dec_octet '.' dec_octet '.' dec_octet '.' dec_octet nz = ~'0' digit dec_octet = ( digit # 0-9 | nz digit # 10-99 | '1' digit {2} # 100-199 | '2' ('0' | '1' | '2' | '3' | '4') digit # 200-249 | '25' ('0' | '1' | '2' | '3' | '4' | '5') ) # %250-255 reg_name = (unreserved | pct_encoded | sub_delims)* path = ( path_abempty # begins with '/' or is empty | path_absolute # begins with '/' but not '//' | path_noscheme # begins with a non-colon segment | path_rootless # begins with a segment | path_empty ) # zero characters path_abempty = ('/' segment)* path_absolute = '/' (segment_nz ('/' segment)* )? path_noscheme = segment_nz_nc ('/' segment)* path_rootless = segment_nz ('/' segment)* path_empty = pchar {0} segment = pchar* segment_nz = pchar+ segment_nz_nc = ( unreserved | pct_encoded | sub_delims | '@')+ # non-zero-length segment without any colon ':' pchar = unreserved | pct_encoded | sub_delims | ':' | '@' query = ( pchar | '/' | '?')* fragment = ( pchar | '/' | '?')* pct_encoded = '%' hexdig unreserved = letter | digit | '-' | '.' | '_' | '~' reserved = gen_delims | sub_delims gen_delims = ':' | '/' | '?' | '#' | '(' | ')?' | '@' sub_delims = '!' | '$' | '&' | ' \\\\ '' | '(' | ')' | '*' | '+' | ',' | ';' | '=' hexdig = digit | 'a' | 'A' | 'b' | 'B' | 'c' | 'C' | 'd' | 'D' | 'e' | 'E' | 'f' | 'F' \"\"\" class Marker ( NamedTuple ): op : str v1 : Union [ str , \"Marker\" ] v2 : Union [ str , \"Marker\" ] def __str__ ( self ) -> str : return self . __repr__ () def __repr__ ( self ) -> str : out = \"\" if isinstance ( self . v1 , Marker ): out += \"( \" + str ( self . v1 ) + \" )\" else : out += self . v1 out += \" \" + self . op + \" \" if isinstance ( self . v2 , Marker ): out += \"( \" + str ( self . v2 ) + \" )\" else : out += '\"' + self . v2 + '\"' return out class Spec ( NamedTuple ): name : str extras : List [ str ] version : List [ Tuple [ str , str ]] marker : Optional [ Marker ] def __str__ ( self ) -> str : return self . __repr__ () def __repr__ ( self ) -> str : out = self . name if len ( self . extras ) > 0 : out += \"[\" + \",\" . join ( self . extras ) + \"]\" out += \" \" if isinstance ( self . version , str ): out += \"@ \" + self . version else : out += \",\" . join ([ v [ 0 ] + v [ 1 ] for v in self . version ]) if self . marker is not None : out += \" ; \" + str ( self . marker ) return out Parse = makeGrammar ( GRAMMAR , { 'Spec' : Spec , 'Marker' : Marker }, name = \"PEP508\" ) def parse_line ( line : str ) -> Spec : return Parse ( line ) . specification () def parse_lines ( lines : \"list[str]\" ) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes dependency requirement lines. Args: lines (list[str]): list of dependencies and (extra) index urls. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" dependencies = {} invalid_lines = [] extra_index = [] index_url = None lines = map ( lambda row : row . strip (), lines ) lines = filter ( lambda row : row != \"\" and not row . startswith ( \"#\" ), lines ) for line in lines : if line . startswith ( '--index-url' ): index_url = line continue if line . startswith ( '--extra-index-url' ): extra_index . append ( line ) continue try : spec : Spec = parse_line ( line ) dependencies [ spec . name ] = spec except Exception as e : invalid_lines . append ( f \" { line } \\n { e } \" ) if len ( invalid_lines ) > 0 : raise AssertionError ( ' \\n ' . join ( invalid_lines )) return ( dependencies , extra_index , index_url ) def parse_requirements ( requirements_path : Union [ str , os . PathLike ]) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes a requirements.txt file line-by-line. Args: requirements_path: Union[str, os.PathLike]: path to the requirements.txt file. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" with open ( requirements_path , \"r\" ) as f : lines = f . readlines () try : return parse_lines ( lines ) except AssertionError as err : raise AssertionError ( f \"Requirements file ' { requirements_path } ' contains invalid dependency specifications: \\n { str ( err ) } \" ) Variables GRAMMAR Functions Parse def Parse ( input ) Creates a parser for the given input, with methods for invoking each rule. Parameters: Name Type Description Default input None The string you want to parse. None View Source def makeParser(input): \"\"\" Creates a parser for the given input, with methods for invoking each rule. :param input: The string you want to parse. \"\"\" parser = g(input) if tracefunc: parser._trace = tracefunc return _GrammarWrapper(parser, input) parse_line def parse_line ( line : str ) -> simaticai . helpers . pep508 . Spec View Source def parse_line ( line : str ) -> Spec : return Parse ( line ). specification () parse_lines def parse_lines ( lines : 'list[str]' ) -> '(dict[str, Spec], list[str], str)' Processes dependency requirement lines. Parameters: Name Type Description Default lines list[str] list of dependencies and (extra) index urls. None Returns: Type Description None (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: Type Description AssertionError if the lines contain invalid dependency specifications. View Source def parse_lines ( lines : \"list[str]\" ) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes dependency requirement lines. Args: lines (list[str]): list of dependencies and (extra) index urls. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" dependencies = {} invalid_lines = [] extra_index = [] index_url = None lines = map ( lambda row : row . strip (), lines ) lines = filter ( lambda row : row != \"\" and not row . startswith ( \"#\" ), lines ) for line in lines : if line . startswith ( '--index-url' ) : index_url = line continue if line . startswith ( '--extra-index-url' ) : extra_index . append ( line ) continue try : spec : Spec = parse_line ( line ) dependencies [ spec.name ] = spec except Exception as e : invalid_lines . append ( f \"{line}\\n{e}\" ) if len ( invalid_lines ) > 0 : raise AssertionError ( '\\n' . join ( invalid_lines )) return ( dependencies , extra_index , index_url ) parse_requirements def parse_requirements ( requirements_path : Union [ str , os . PathLike ] ) -> '(dict[str, Spec], list[str], str)' Processes a requirements.txt file line-by-line. Parameters: Name Type Description Default requirements_path None Union[str, os.PathLike]: path to the requirements.txt file. None Returns: Type Description None (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: Type Description AssertionError if the lines contain invalid dependency specifications. View Source def parse_requirements ( requirements_path : Union [ str, os.PathLike ] ) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes a requirements.txt file line-by-line. Args: requirements_path: Union[str, os.PathLike]: path to the requirements.txt file. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" with open ( requirements_path , \"r\" ) as f : lines = f . readlines () try : return parse_lines ( lines ) except AssertionError as err : raise AssertionError ( f \"Requirements file '{requirements_path}' contains invalid dependency specifications:\\n{str(err)}\" ) Classes Marker Marker(op, v1, v2) class Marker ( / , * args , ** kwargs ) View Source class Marker ( NamedTuple ): op: str v1: Union [ str , \"Marker\" ] v2: Union [ str , \"Marker\" ] def __str__ ( self ) -> str: return self . __repr__ () def __repr__ ( self ) -> str: out = \"\" if isinstance ( self . v1 , Marker ): out += \"( \" + str ( self . v1 ) + \" )\" else: out += self . v1 out += \" \" + self . op + \" \" if isinstance ( self . v2 , Marker ): out += \"( \" + str ( self . v2 ) + \" )\" else: out += '\"' + self . v2 + '\"' return out Ancestors (in MRO) builtins.tuple Class variables op v1 v2 Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. Spec Spec(name, extras, version, marker) class Spec ( / , * args , ** kwargs ) View Source class Spec ( NamedTuple ) : name : str extras : List [ str ] version : List [ Tuple[str, str ] ] marker : Optional [ Marker ] def __str__ ( self ) -> str : return self . __repr__ () def __repr__ ( self ) -> str : out = self . name if len ( self . extras ) > 0 : out += \"[\" + \",\" . join ( self . extras ) + \"]\" out += \" \" if isinstance ( self . version , str ) : out += \"@ \" + self . version else : out += \",\" . join ( [ v[0 ] + v [ 1 ] for v in self . version ] ) if self . marker is not None : out += \" ; \" + str ( self . marker ) return out Ancestors (in MRO) builtins.tuple Class variables extras marker name version Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"Pep508"},{"location":"reference/simaticai/helpers/pep508.html#module-simaticaihelperspep508","text":"None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os from typing import NamedTuple , Tuple , Optional , Union , List from parsley import makeGrammar # https://peps.python.org/pep-0508/#complete-grammar GRAMMAR = \"\"\" wsp = ' ' | ' \\t ' version_cmp = wsp* <'<=' | '<' | '!=' | '==' | '>=' | '>' | '~=' | '==='> version = wsp* <( letterOrDigit | '-' | '_' | '.' | '*' | '+' | '!' )+> version_one = version_cmp:op version:v wsp* -> (op, v) version_many = version_one:v1 (wsp* ',' version_one)*:v2 -> [v1] + v2 versionspec = ('(' version_many:v ')' ->v) | version_many urlspec = '@' wsp* <URI_reference> marker_op = version_cmp | (wsp* 'in') | (wsp* 'not' wsp+ 'in') python_str_c = (wsp | letter | digit | '(' | ')' | '.' | '{' | '}' | '-' | '_' | '*' | '#' | ':' | ';' | ',' | '/' | '?' | '[' | ']' | '!' | '~' | '`' | '@' | '$' | '%' | '^' | '&' | '=' | '+' | '|' | '<' | '>' ) dquote = '\"' squote = ' \\\\ '' python_str = (squote <(python_str_c | dquote)*>:s squote | dquote <(python_str_c | squote)*>:s dquote) -> s env_var = ( 'python_version' | 'python_full_version' | 'os_name' | 'sys_platform' | 'platform_release' | 'platform_system' | 'platform_version' | 'platform_machine' | 'platform_python_implementation' | 'implementation_name' | 'implementation_version' | 'extra' # ONLY when defined by a containing layer ) marker_var = wsp* (env_var | python_str) marker_expr = marker_var:l marker_op:o marker_var:r -> Marker(o, l, r) | wsp* '(' marker:m wsp* ')' -> m marker_and = marker_expr:l wsp* 'and' marker_expr:r -> Marker('and', l, r) | marker_expr:m -> m marker_or = marker_and:l wsp* 'or' marker_and:r -> Marker('or', l, r) | marker_and:m -> m marker = marker_or quoted_marker = ';' wsp* marker identifier_end = letterOrDigit | (('-' | '_' | '.' )* letterOrDigit) identifier = < letterOrDigit identifier_end* > name = identifier extras_list = identifier:i (wsp* ',' wsp* identifier)*:ids -> [i] + ids extras = '[' wsp* extras_list?:e wsp* ']' -> e name_req = (name:n wsp* extras?:e wsp* versionspec?:v wsp* quoted_marker?:m -> (n, e or [], v or [], m)) url_req = (name:n wsp* extras?:e wsp* urlspec:v (wsp+ | end) quoted_marker?:m -> (n, e or [], v, m)) specification = wsp* ( url_req | name_req ):s wsp* -> Spec(*s) URI_reference = <URI | relative_ref> URI = scheme ':' hier_part ('?' query )? ('#' fragment)? hier_part = ('//' authority path_abempty) | path_absolute | path_rootless | path_empty absolute_URI = scheme ':' hier_part ( '?' query )? relative_ref = relative_part ( '?' query )? ('#' fragment)? relative_part = '//' authority path_abempty | path_absolute | path_noscheme | path_empty scheme = letter ( letter | digit | '+' | '-' | '.')* authority = ( userinfo '@' )? host ( ':' port )? userinfo = ( unreserved | pct_encoded | sub_delims | ':')* host = IP_literal | IPv4address | reg_name port = digit* IP_literal = '[' ( IPv6address | IPvFuture) ']' IPvFuture = 'v' hexdig+ '.' ( unreserved | sub_delims | ':')+ IPv6address = ( ( h16 ':') {6} ls32 | '::' ( h16 ':') {5} ls32 | ( h16 )? '::' ( h16 ':') {4} ls32 | ( ( h16 ':')? h16 )? '::' ( h16 ':') {3} ls32 | ( ( h16 ':'){0,2} h16 )? '::' ( h16 ':') {2} ls32 | ( ( h16 ':'){0,3} h16 )? '::' h16 ':' ls32 | ( ( h16 ':'){0,4} h16 )? '::' ls32 | ( ( h16 ':'){0,5} h16 )? '::' h16 | ( ( h16 ':'){0,6} h16 )? '::' ) h16 = hexdig{1,4} ls32 = ( h16 ':' h16) | IPv4address IPv4address = dec_octet '.' dec_octet '.' dec_octet '.' dec_octet nz = ~'0' digit dec_octet = ( digit # 0-9 | nz digit # 10-99 | '1' digit {2} # 100-199 | '2' ('0' | '1' | '2' | '3' | '4') digit # 200-249 | '25' ('0' | '1' | '2' | '3' | '4' | '5') ) # %250-255 reg_name = (unreserved | pct_encoded | sub_delims)* path = ( path_abempty # begins with '/' or is empty | path_absolute # begins with '/' but not '//' | path_noscheme # begins with a non-colon segment | path_rootless # begins with a segment | path_empty ) # zero characters path_abempty = ('/' segment)* path_absolute = '/' (segment_nz ('/' segment)* )? path_noscheme = segment_nz_nc ('/' segment)* path_rootless = segment_nz ('/' segment)* path_empty = pchar {0} segment = pchar* segment_nz = pchar+ segment_nz_nc = ( unreserved | pct_encoded | sub_delims | '@')+ # non-zero-length segment without any colon ':' pchar = unreserved | pct_encoded | sub_delims | ':' | '@' query = ( pchar | '/' | '?')* fragment = ( pchar | '/' | '?')* pct_encoded = '%' hexdig unreserved = letter | digit | '-' | '.' | '_' | '~' reserved = gen_delims | sub_delims gen_delims = ':' | '/' | '?' | '#' | '(' | ')?' | '@' sub_delims = '!' | '$' | '&' | ' \\\\ '' | '(' | ')' | '*' | '+' | ',' | ';' | '=' hexdig = digit | 'a' | 'A' | 'b' | 'B' | 'c' | 'C' | 'd' | 'D' | 'e' | 'E' | 'f' | 'F' \"\"\" class Marker ( NamedTuple ): op : str v1 : Union [ str , \"Marker\" ] v2 : Union [ str , \"Marker\" ] def __str__ ( self ) -> str : return self . __repr__ () def __repr__ ( self ) -> str : out = \"\" if isinstance ( self . v1 , Marker ): out += \"( \" + str ( self . v1 ) + \" )\" else : out += self . v1 out += \" \" + self . op + \" \" if isinstance ( self . v2 , Marker ): out += \"( \" + str ( self . v2 ) + \" )\" else : out += '\"' + self . v2 + '\"' return out class Spec ( NamedTuple ): name : str extras : List [ str ] version : List [ Tuple [ str , str ]] marker : Optional [ Marker ] def __str__ ( self ) -> str : return self . __repr__ () def __repr__ ( self ) -> str : out = self . name if len ( self . extras ) > 0 : out += \"[\" + \",\" . join ( self . extras ) + \"]\" out += \" \" if isinstance ( self . version , str ): out += \"@ \" + self . version else : out += \",\" . join ([ v [ 0 ] + v [ 1 ] for v in self . version ]) if self . marker is not None : out += \" ; \" + str ( self . marker ) return out Parse = makeGrammar ( GRAMMAR , { 'Spec' : Spec , 'Marker' : Marker }, name = \"PEP508\" ) def parse_line ( line : str ) -> Spec : return Parse ( line ) . specification () def parse_lines ( lines : \"list[str]\" ) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes dependency requirement lines. Args: lines (list[str]): list of dependencies and (extra) index urls. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" dependencies = {} invalid_lines = [] extra_index = [] index_url = None lines = map ( lambda row : row . strip (), lines ) lines = filter ( lambda row : row != \"\" and not row . startswith ( \"#\" ), lines ) for line in lines : if line . startswith ( '--index-url' ): index_url = line continue if line . startswith ( '--extra-index-url' ): extra_index . append ( line ) continue try : spec : Spec = parse_line ( line ) dependencies [ spec . name ] = spec except Exception as e : invalid_lines . append ( f \" { line } \\n { e } \" ) if len ( invalid_lines ) > 0 : raise AssertionError ( ' \\n ' . join ( invalid_lines )) return ( dependencies , extra_index , index_url ) def parse_requirements ( requirements_path : Union [ str , os . PathLike ]) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes a requirements.txt file line-by-line. Args: requirements_path: Union[str, os.PathLike]: path to the requirements.txt file. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" with open ( requirements_path , \"r\" ) as f : lines = f . readlines () try : return parse_lines ( lines ) except AssertionError as err : raise AssertionError ( f \"Requirements file ' { requirements_path } ' contains invalid dependency specifications: \\n { str ( err ) } \" )","title":"Module simaticai.helpers.pep508"},{"location":"reference/simaticai/helpers/pep508.html#variables","text":"GRAMMAR","title":"Variables"},{"location":"reference/simaticai/helpers/pep508.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/helpers/pep508.html#parse","text":"def Parse ( input ) Creates a parser for the given input, with methods for invoking each rule. Parameters: Name Type Description Default input None The string you want to parse. None View Source def makeParser(input): \"\"\" Creates a parser for the given input, with methods for invoking each rule. :param input: The string you want to parse. \"\"\" parser = g(input) if tracefunc: parser._trace = tracefunc return _GrammarWrapper(parser, input)","title":"Parse"},{"location":"reference/simaticai/helpers/pep508.html#parse_line","text":"def parse_line ( line : str ) -> simaticai . helpers . pep508 . Spec View Source def parse_line ( line : str ) -> Spec : return Parse ( line ). specification ()","title":"parse_line"},{"location":"reference/simaticai/helpers/pep508.html#parse_lines","text":"def parse_lines ( lines : 'list[str]' ) -> '(dict[str, Spec], list[str], str)' Processes dependency requirement lines. Parameters: Name Type Description Default lines list[str] list of dependencies and (extra) index urls. None Returns: Type Description None (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: Type Description AssertionError if the lines contain invalid dependency specifications. View Source def parse_lines ( lines : \"list[str]\" ) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes dependency requirement lines. Args: lines (list[str]): list of dependencies and (extra) index urls. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" dependencies = {} invalid_lines = [] extra_index = [] index_url = None lines = map ( lambda row : row . strip (), lines ) lines = filter ( lambda row : row != \"\" and not row . startswith ( \"#\" ), lines ) for line in lines : if line . startswith ( '--index-url' ) : index_url = line continue if line . startswith ( '--extra-index-url' ) : extra_index . append ( line ) continue try : spec : Spec = parse_line ( line ) dependencies [ spec.name ] = spec except Exception as e : invalid_lines . append ( f \"{line}\\n{e}\" ) if len ( invalid_lines ) > 0 : raise AssertionError ( '\\n' . join ( invalid_lines )) return ( dependencies , extra_index , index_url )","title":"parse_lines"},{"location":"reference/simaticai/helpers/pep508.html#parse_requirements","text":"def parse_requirements ( requirements_path : Union [ str , os . PathLike ] ) -> '(dict[str, Spec], list[str], str)' Processes a requirements.txt file line-by-line. Parameters: Name Type Description Default requirements_path None Union[str, os.PathLike]: path to the requirements.txt file. None Returns: Type Description None (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: Type Description AssertionError if the lines contain invalid dependency specifications. View Source def parse_requirements ( requirements_path : Union [ str, os.PathLike ] ) -> \"(dict[str, Spec], list[str], str)\" : \"\"\" Processes a requirements.txt file line-by-line. Args: requirements_path: Union[str, os.PathLike]: path to the requirements.txt file. Returns: (dict[str, Spec], list[str]): dictionary that contains the dependency specifcations and a list of (extra) index urls. Raises: AssertionError: if the lines contain invalid dependency specifications. \"\"\" with open ( requirements_path , \"r\" ) as f : lines = f . readlines () try : return parse_lines ( lines ) except AssertionError as err : raise AssertionError ( f \"Requirements file '{requirements_path}' contains invalid dependency specifications:\\n{str(err)}\" )","title":"parse_requirements"},{"location":"reference/simaticai/helpers/pep508.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/helpers/pep508.html#marker","text":"Marker(op, v1, v2) class Marker ( / , * args , ** kwargs ) View Source class Marker ( NamedTuple ): op: str v1: Union [ str , \"Marker\" ] v2: Union [ str , \"Marker\" ] def __str__ ( self ) -> str: return self . __repr__ () def __repr__ ( self ) -> str: out = \"\" if isinstance ( self . v1 , Marker ): out += \"( \" + str ( self . v1 ) + \" )\" else: out += self . v1 out += \" \" + self . op + \" \" if isinstance ( self . v2 , Marker ): out += \"( \" + str ( self . v2 ) + \" )\" else: out += '\"' + self . v2 + '\"' return out","title":"Marker"},{"location":"reference/simaticai/helpers/pep508.html#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/helpers/pep508.html#class-variables","text":"op v1 v2","title":"Class variables"},{"location":"reference/simaticai/helpers/pep508.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/helpers/pep508.html#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/simaticai/helpers/pep508.html#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/simaticai/helpers/pep508.html#spec","text":"Spec(name, extras, version, marker) class Spec ( / , * args , ** kwargs ) View Source class Spec ( NamedTuple ) : name : str extras : List [ str ] version : List [ Tuple[str, str ] ] marker : Optional [ Marker ] def __str__ ( self ) -> str : return self . __repr__ () def __repr__ ( self ) -> str : out = self . name if len ( self . extras ) > 0 : out += \"[\" + \",\" . join ( self . extras ) + \"]\" out += \" \" if isinstance ( self . version , str ) : out += \"@ \" + self . version else : out += \",\" . join ( [ v[0 ] + v [ 1 ] for v in self . version ] ) if self . marker is not None : out += \" ; \" + str ( self . marker ) return out","title":"Spec"},{"location":"reference/simaticai/helpers/pep508.html#ancestors-in-mro_1","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/helpers/pep508.html#class-variables_1","text":"extras marker name version","title":"Class variables"},{"location":"reference/simaticai/helpers/pep508.html#methods_1","text":"","title":"Methods"},{"location":"reference/simaticai/helpers/pep508.html#count_1","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/simaticai/helpers/pep508.html#index_1","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/simaticai/helpers/reporter.html","text":"Module simaticai.helpers.reporter Classes to generate a report for a dataflow pipeline and local pipeline runner. None View Source # Copyright (C) Siemens AG 2025. All Rights Reserved. Confidential. \"\"\" Classes to generate a report for a dataflow pipeline and local pipeline runner. \"\"\" from pathlib import Path import requests import logging import zipfile import json import io WARNINGS_HEADLINE = \"## Warnings \\n\\n \" WARNING_LINE = \" {name} : {filename} : {line_number} (W) {warning_msg} \\n\\n \" class ReportWriter : \"\"\" Base class for report writers. \"\"\" def __init__ ( self ): self . report_path = None self . warnings_text = \"\" def set_path ( self , report_path : Path ): self . report_path = report_path def add_warning ( self , name , filename , line_number , warning_msg ): self . warnings_text += WARNING_LINE . format ( name = name , filename = filename , line_number = line_number , warning_msg = warning_msg ) def write_report ( self ): raise NotImplementedError ( \"Subclasses should implement this method\" ) def _write_warnings ( self , file ): file . write ( WARNINGS_HEADLINE ) file . write ( self . warnings_text ) class ReportWriterHandler ( logging . Handler ): \"\"\" A handler that can be given to a logger, so the report writer can capture logged warning messages \"\"\" def __init__ ( self , report_writer : ReportWriter ): super () . __init__ () self . report_writer = report_writer def emit ( self , record ): if record . levelno == logging . WARNING : self . report_writer . add_warning ( record . name , record . filename , record . lineno , record . getMessage ()) class ZipTreeElement : \"\"\" A class to represent a file or folder in a zip file. During the recursive traversal of the zip file, the full name and file size are stored in this class. \"\"\" def __init__ ( self , full_name , file_size ): self . full_name = full_name self . file_size = file_size PL_REPORT_HEADLINE = \"# Report on ` {pipeline_name} ` \\n\\n \" PL_INFO_HEADLINE = \"## Pipeline info \\n\\n \" PL_INFO = \"\"\"- Author: {author} - Created on: {created_on} - Dataflow Pipeline version: {pipeline_version} - Package ID: {package_id} - Project name: {project_name} Description: {description} \"\"\" # TODO: check other type of PlantUML diagrams if they look better or generating images into the markdown file PL_STRUCTURE_HEADLINE = \"\"\"## Pipeline structure The pipeline structure is visualized using PlantUML. The components are connected by arrows. Metrics are drawn with dashed arrows. \"\"\" PL_STRUCTURE = \" {source_component} {arrow} {target_component} : {variable_name} ( {variable_type} ) \\n \" PL_PACKAGE_VULNERABILITIES_HEADLINE = \"\"\"## Package vulnerabilities Package vulnerability information is collected from the [PyPI repository](https://pypi.org/). | Package name | Package version | Vulnerability | Link | Details | Fixed in | In pipeline components | |--------------|-----------------|---------------|------|---------|----------|------------------------| \"\"\" PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED = \"| {package_name} | {package_version} | Can not be checked | - | - | - | {components} | \\n \" PL_PACKAGE_VULNERABILITY_NOT_KNOWN = \"| {package_name} | {package_version} | No known vulnerability | - | - | - | {components} | \\n \" PL_PACKAGE_VULNERABILITY = \"| {package_name} | {package_version} | {vulnerability_aliases} | {vulnerability_link} | {vulnerability_details} | {vulnerability_fixed_in} | {components} | \\n \" PL_COMPONMENT_DEPENDENCIES_HEADLINE = \"## Component dependencies for ` {component_name} ` \\n\\n \" PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE = \"### Direct dependencies \\n\\n \" PL_COMPONENT_DIRECT_DEPENDENCY = \"- {dependency_name} {dependency_version} \\n \" PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE = \"### Transitive dependencies \\n\\n \" PL_COMPONENT_TRANSITIVE_DEPENDENCY = \"- {dependency_name} {dependency_version} \\n \" class PipelineReportWriter ( ReportWriter ): \"\"\" A class to generate a report for a dataflow pipeline, including pipeline structure, component dependencies, and package vulnerabilities. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_pipeline_config(pipeline_config: dict): Sets the pipeline configuration and updates the pipeline info and structure. add_full_dependency_set(component_name: str, dependency_set: set[tuple]): Adds a full set of dependencies for a component and updates the vulnerability dictionary. add_direct_dependencies(component_name: str, direct_dependencies: dict): Adds direct dependencies for a component. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. \"\"\" def __init__ ( self ): super () . __init__ () self . pipeline_config = {} # pipeline config json self . pipeline_name = \"Unnamed pipeline\" # dependency_names and package_names are transformed dependency names (lowercase, underscore instead of dash) # set from outside self . component_direct_dependency_namelist = {} # component -> list of dependency_names (with NO version) self . component_all_dependencies = {} # component -> set of tuples of (dependency_name, dependency_version) # collected before writing self . component_direct_dependencies = {} # component -> set of dependency_names self . component_transitive_dependencies = {} # component -> set of tuples of (dependency_name, dependency_version) self . vulnerability_dict = {} # (package_name, package_version) -> vulnerabilities (None | list of dictionaries) # report text sections to fill self . pipeline_structure_text = \"\" self . pipeline_info_text = \"\" self . warnings_text = \"\" def set_pipeline_config ( self , pipeline_config : dict ): self . pipeline_config = pipeline_config self . _set_pipeline_info () self . _set_pipeline_structure () @staticmethod def _sort_pipeline_dag ( pipeline_dag : list ) -> list : \"\"\" Sorts a pipeline DAG in order to show dataflow from Pipeline Inputs to Pipeline Outputs. Databus component is a privileged source, and it is always the first component in the report. Args: pipeline_dag (list): The pipeline DAG is a list of dictionaries with \"source\" and \"target\" keys. Returns: A sorted list of dictionaries representing the pipeline DAG. \"\"\" pipeline_dag . sort ( key = lambda x : ( x [ \"source\" ], x [ \"target\" ])) sorted_dag = [ edge for edge in pipeline_dag if \"Databus\" in edge [ 'source' ]] if sorted_dag == []: return pipeline_dag pipeline_dag = [ edge for edge in pipeline_dag if \"Databus\" not in edge [ 'source' ]] # Extracts name of the target or source component from the edge name_of_component = lambda edge , target_or_source : edge [ target_or_source ] . rsplit ( \".\" , 1 )[ 0 ] while len ( pipeline_dag ) > 0 : sorted_targets = [ name_of_component ( edge , \"target\" ) for edge in sorted_dag ] sorted_dag . extend ([ edge for edge in pipeline_dag if name_of_component ( edge , \"source\" ) in sorted_targets ]) pipeline_dag = [ edge for edge in pipeline_dag if edge not in sorted_dag ] return sorted_dag def _set_pipeline_structure ( self ): self . pipeline_name = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}) . get ( \"projectName\" , \"n/a\" ) self . pipeline_structure_text = \"```plantuml \\n \" components = self . pipeline_config . get ( \"dataFlowPipeline\" , {}) . get ( \"components\" , []) variables = {} # name: (type, is_metric) for component in components : input_variables = { _input [ \"name\" ]: ( _input [ \"type\" ], False ) for _input in component . get ( \"inputType\" )} output_variables = { _output [ \"name\" ]: ( _output [ \"type\" ], _output . get ( \"metric\" , False )) for _output in component . get ( \"outputType\" )} variables . update ({ ** input_variables , ** output_variables }) pipeline_dag = self . pipeline_config . get ( \"dataFlowPipeline\" , {}) . get ( \"pipelineDag\" , []) sorted_pipeline_dag = PipelineReportWriter . _sort_pipeline_dag ( pipeline_dag ) for transition in sorted_pipeline_dag : source_component_name , source_variable_name = transition [ \"source\" ] . rsplit ( \".\" , 1 ) target_component_name , target_variable_name = transition [ \"target\" ] . rsplit ( \".\" , 1 ) variable_name_to_show = source_variable_name if source_variable_name == target_variable_name else f \" { source_variable_name } -> { target_variable_name } \" source_component_name = source_component_name . replace ( \"Databus\" , \"AIIS\" ) target_component_name = target_component_name . replace ( \"Databus\" , \"AIIS\" ) variable_type , is_metric = variables [ source_variable_name ] arrow = \"-->\" if is_metric else \"->\" # metric variables are drawn with a dashed line self . pipeline_structure_text += PL_STRUCTURE . format ( source_component = source_component_name , arrow = arrow , target_component = target_component_name , variable_name = variable_name_to_show , variable_type = variable_type ) self . pipeline_structure_text += \"``` \\n\\n \" def _set_pipeline_info ( self ): dataflow_pipeline_info = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}) author = dataflow_pipeline_info . get ( \"author\" , \"n/a\" ) created_on = dataflow_pipeline_info . get ( \"createdOn\" , \"n/a\" ) pipeline_version = dataflow_pipeline_info . get ( \"dataFlowPipelineVersion\" , \"n/a\" ) description = dataflow_pipeline_info . get ( \"description\" , \"n/a\" ) package_id = dataflow_pipeline_info . get ( \"packageId\" , \"n/a\" ) project_name = dataflow_pipeline_info . get ( \"projectName\" , \"n/a\" ) self . pipeline_info_text = PL_INFO . format ( author = author , created_on = created_on , pipeline_version = pipeline_version , description = description , package_id = package_id , project_name = project_name ) # Transform every dependency and package name for consistency; i.e., # opencv-python-headless -> opencv_python_headless; Django -> django @staticmethod def transform_package_name ( name : str ): new_name = name . replace ( \"-\" , \"_\" ) return new_name . lower () # A full dependency set is a set of (package_name, package_version) tuples # and contains all the dependencies installed for a component def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ]): dependency_list = sorted ( list ( dependency_set ), key = lambda x : x [ 0 ]) self . _expand_component_all_dependencies ( component_name , dependency_list ) self . _update_vulnerability_dict ( dependency_list ) def _expand_component_all_dependencies ( self , component_name : str , dependency_list : list [ tuple ]): if component_name not in self . component_all_dependencies : self . component_all_dependencies [ component_name ] = set () for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) self . component_all_dependencies [ component_name ] . add (( transformed_package_name , package_version )) def _update_vulnerability_dict ( self , dependency_list : list [ tuple ]): vulnerability_dict = {} for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) vulnerability_dict [( transformed_package_name , package_version )] = None url = f \"https://pypi.org/pypi/ { package_name } / { package_version } /json\" try : response = requests . get ( url , timeout = 5 ) if response . status_code == 200 : data = response . json () if 'vulnerabilities' in data : vulnerability_dict [( transformed_package_name , package_version )] = data [ 'vulnerabilities' ] except requests . exceptions . Timeout : pass self . vulnerability_dict . update ( vulnerability_dict ) def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ): self . component_direct_dependency_namelist [ component_name ] = [ PipelineReportWriter . transform_package_name ( name ) for name in list ( direct_dependencies . keys ())] def write_report ( self ): if self . report_path is None : return self . _set_component_dependencies () with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_pipeline_info ( file ) self . _write_pipeline_structure ( file ) self . _write_dependencies ( file ) self . _write_package_vulnerabilities ( file ) self . _write_warnings ( file ) def _set_component_dependencies ( self ): for component in self . component_all_dependencies . keys (): # self.component_direct_dependencies should contain everything from self.component_all_dependencies # if it is direct, i.e., the name is in self.component_direct_dependency_namelist # self.component_transitive_dependencies should contain everything else self . component_transitive_dependencies [ component ] = set () self . component_direct_dependencies [ component ] = set () all_dependencies = self . component_all_dependencies [ component ] for dependency_name , dependency_version in all_dependencies : if dependency_name in self . component_direct_dependency_namelist . get ( component , []): self . component_direct_dependencies [ component ] . add (( dependency_name , dependency_version )) else : self . component_transitive_dependencies [ component ] . add (( dependency_name , dependency_version )) def _write_headline ( self , file ): file . write ( PL_REPORT_HEADLINE . format ( pipeline_name = self . pipeline_name )) def _write_pipeline_info ( self , file ): file . write ( PL_INFO_HEADLINE ) file . write ( self . pipeline_info_text ) def _write_pipeline_structure ( self , file ): file . write ( PL_STRUCTURE_HEADLINE ) file . write ( self . pipeline_structure_text ) def _write_dependencies ( self , file ): for component_name in self . component_all_dependencies . keys (): direct_dependencies = self . component_direct_dependencies . get ( component_name , set ()) transitive_dependencies = self . component_transitive_dependencies . get ( component_name , set ()) file . write ( PL_COMPONMENT_DEPENDENCIES_HEADLINE . format ( component_name = component_name )) file . write ( PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE ) sorted_direct_dependencies = sorted ( list ( direct_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_direct_dependencies : file . write ( PL_COMPONENT_DIRECT_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE ) sorted_transitive_dependencies = sorted ( list ( transitive_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_transitive_dependencies : file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) def _get_components_who_have_given_package ( self , package_name , package_version ): components = [] for component in self . component_all_dependencies : dependencies = self . component_all_dependencies [ component ] if ( package_name , package_version ) in dependencies : components . append ( component ) return components def _write_package_vulnerabilities ( self , file ): file . write ( PL_PACKAGE_VULNERABILITIES_HEADLINE ) sorted_vulnerability_dict_items = sorted ( self . vulnerability_dict . items (), key = lambda x : x [ 0 ][ 0 ]) for ( package_name , package_version ), vulnerabilities in sorted_vulnerability_dict_items : components = ', ' . join ( self . _get_components_who_have_given_package ( package_name , package_version )) if vulnerabilities is None : file . write ( PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED . format ( package_name = package_name , package_version = package_version , components = components )) elif vulnerabilities == []: file . write ( PL_PACKAGE_VULNERABILITY_NOT_KNOWN . format ( package_name = package_name , package_version = package_version , components = components )) else : for vulnerability in vulnerabilities : vulnerability_aliases = vulnerability . get ( 'aliases' , 'Vulnerability found with no alias. Check [PyPI repository](https://pypi.org/) for more details.' ) vulnerability_link = vulnerability . get ( 'link' , 'No link found' ) if vulnerability_link != 'No link found' : vulnerability_link = f \"[ { vulnerability_link } ]( { vulnerability_link } )\" vulnerability_details = vulnerability . get ( 'details' , 'No details found' ) vulnerability_fixed_in = vulnerability . get ( 'fixed_in' , '' ) file . write ( PL_PACKAGE_VULNERABILITY . format ( package_name = package_name , package_version = package_version , vulnerability_aliases = vulnerability_aliases , vulnerability_link = vulnerability_link , vulnerability_details = vulnerability_details , vulnerability_fixed_in = vulnerability_fixed_in , components = components )) file . write ( \" \\n \" ) LPLR_REPORT_HEADLINE = \"# Report on Local Pipeline Runner \\n\\n \" LPLR_FOLDER_STRUCTURE_HEADLINE = \"\"\"## Folder structure File sizes represent uncompressed sizes. \"\"\" LPLR_FOLDER_STRUCTURE = \"\"\"``` {file_name} {folder_structure} ``` \"\"\" LPLR_FOLDER_STRUCTURE_FOLDER_LINE = \" {prefix}{connector}{folder} \\n \" LPLR_FOLDER_STRUCTURE_FILE_LINE = \" {prefix}{connector}{file} ( {size} ) \\n \" LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL = \"\u251c\u2500\u2500 \" LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL = \"\u2514\u2500\u2500 \" LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL = \"\u2502 \" LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL = \" \" LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE = \"## PythonPackages.zip content \\n\\n \" LPLR_PYTHON_PACKAGES_ZIP_CONTENT = \"- {python_package} \\n \" LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE = \"\"\"## Installed packages Installed packages are listed in the `first report`, when the virtual environment is created for the Local Pipeline Runner. Subsequent reports will result in an empty list. \"\"\" LPLR_COMPONENT_INSTALLED_PACKAGES = \"\"\"### Component ` {component} ` | Package name | Package version | wheel name | |--------------|-----------------|------------| \"\"\" LPLR_COMPONENT_INSTALLED_PACKAGES_ROW = \"| {package_name} | {package_version} | {wheel_name} | \\n \" LPLR_PAYLOAD_LENGTHS_HEADLINE = \"## Payload counts \\n\\n \" LPLR_PAYLOAD_LENGTHS = \"\"\"### Component ` {component} ` - Input payload count: {input_payload_length} - Output payload count: {output_payload_length} \"\"\" class PipelineRunnerReportWriter ( ReportWriter ): \"\"\" PipelineRunnerReportWriter is responsible for generating a detailed report of a local pipeline execution. It builds folder structures from zip files, manages component payload counts, and adds installed packages information. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_package_zip_path(zip_path: Path): Sets the path to the package zip file and updates the folder tree. set_input_payload_length(component_name: str, length: int): Sets the input payload length for a component. set_output_payload_length(component_name: str, length: int): Sets the output payload length for a component. add_installed_packages(component_name: str, pip_report_file: Path): Adds installed packages for a component from a pip report file. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. \"\"\" def __init__ ( self ): super () . __init__ () self . package_zip_path = None self . zip_file_name = \"\" self . component_installed_packages = {} # component_name -> list[tuple(package_name, package_version, whl_name)] self . component_payload_length = {} # component_name -> [input_payload_length, output_payload_length] self . python_packages_zip_content = set () # report text sections to fill self . folder_tree_text = \"\" self . warnings_text = \"\" def set_package_zip_path ( self , zip_path : Path ): self . package_zip_path = zip_path with zipfile . ZipFile ( zip_path , 'r' ) as zipf : self . zip_file_name = zipf . filename zip_tree = {} for item_name in zipf . namelist (): zip_tree [ item_name ] = ZipTreeElement ( full_name = item_name , file_size = zipf . getinfo ( item_name ) . file_size ) self . _print_structure_recursively ( zip_tree , zipf ) @staticmethod def _get_folder_and_file_list ( item_names : list ) -> tuple [ list , list ]: \"\"\" Given a list of item names, each item name is a file that either starts with a folder name, or not. This function separates the folder names and the standalone file names. E.g., [\"a/b/something.txt\", \"c/another.txt\", \"else.txt\"] -> [\"a/\", \"c/\"], [\"else.txt\"] \"\"\" folder_names = set () file_names = [] for item in item_names : item_parts = item . split ( '/' ) if len ( item_parts ) > 1 : if item_parts [ 0 ] != '' : folder_names . add ( item_parts [ 0 ] + '/' ) else : if item != '' : file_names . append ( item ) return sorted ( list ( folder_names )), sorted ( file_names ) @staticmethod def format_size ( size ): \"\"\"Format file size in human-readable form.\"\"\" for unit in [ 'B' , 'KB' , 'MB' ]: if size < 1000 : return f \" { size } { unit } \" size //= 1000 return f \" { size } GB\" def _print_structure_recursively ( self , zip_tree , zipf , prefix = \"\" ): folder_names , file_names = PipelineRunnerReportWriter . _get_folder_and_file_list ( zip_tree . keys ()) is_file_names_empty = file_names == [] self . _print_folder_structure ( zip_tree , zipf , prefix , folder_names , is_file_names_empty ) self . _print_file_structure ( zip_tree , zipf , prefix , file_names ) def _print_folder_structure ( self , zip_tree , zipf , prefix , folder_names , is_file_names_empty ): for i , folder in enumerate ( folder_names ): is_last = ( i == len ( folder_names ) - 1 ) and is_file_names_empty connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FOLDER_LINE . format ( prefix = prefix , connector = connector , folder = folder ) # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) new_zip_tree_from_folder = {} for k , v in zip_tree . items (): if k . startswith ( folder ): new_file_name = k . split ( '/' , 1 )[ 1 ] new_zip_tree_from_folder [ new_file_name ] = v prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_folder = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_folder , zipf , new_prefix_from_folder ) def _print_file_structure ( self , zip_tree , zipf , prefix , file_names ): for i , file_name in enumerate ( file_names ): is_last = ( i == len ( file_names ) - 1 ) size_str = PipelineRunnerReportWriter . format_size ( zip_tree [ file_name ] . file_size ) connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FILE_LINE . format ( prefix = prefix , connector = connector , file = file_name , size = size_str ) # zip files are handled similarly to folders: # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) if not file_name . endswith ( '.zip' ): continue full_name = zip_tree [ file_name ] . full_name with zipf . open ( full_name ) as nested_zip_file : nested_zip_data = io . BytesIO ( nested_zip_file . read ()) with zipfile . ZipFile ( nested_zip_data , 'r' ) as nested_zipf : if file_name == \"PythonPackages.zip\" : self . python_packages_zip_content . update ( sorted ( list ( nested_zipf . namelist ()))) # create a new tree where items start with the same folder name; but cut out the folder name new_zip_tree_from_zip = {} for nested_item_name in nested_zipf . namelist (): new_zip_tree_from_zip [ nested_item_name ] = ZipTreeElement ( nested_item_name , nested_zipf . getinfo ( nested_item_name ) . file_size ) prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_zip = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_zip , nested_zipf , new_prefix_from_zip ) def set_input_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 0 ] = length else : self . component_payload_length [ component_name ] = [ length , 0 ] def set_output_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 1 ] = length else : self . component_payload_length [ component_name ] = [ 0 , length ] def add_installed_packages ( self , component_name : str , pip_report_file : Path ): pip_report = {} with open ( pip_report_file , 'r' ) as file : pip_report = json . load ( file ) if component_name not in self . component_installed_packages : self . component_installed_packages [ component_name ] = [] installed_packages = pip_report . get ( \"install\" , []) for package in installed_packages : package_url = package . get ( \"download_info\" , {}) . get ( \"url\" , \"\" ) wheel_name = package_url . split ( \"/\" )[ - 1 ] if package_url . endswith ( \".whl\" ) else \"n/a\" metadata = package . get ( \"metadata\" , {}) package_name = metadata . get ( \"name\" , \"n/a\" ) package_version = metadata . get ( \"version\" , \"n/a\" ) self . component_installed_packages [ component_name ] . append (( package_name , package_version , wheel_name )) # check if one or more reports already exists; set the report path so a new report will have a new index def _set_path_from_zip_path ( self ): if self . package_zip_path is None : return workdir = self . package_zip_path . parent base_name = self . package_zip_path . stem report_files = list ( workdir . glob ( f \" { base_name } _execution_report_*.md\" )) max_index = 0 for report_file in report_files : try : index = int ( report_file . stem . split ( '_' )[ - 1 ]) if index > max_index : max_index = index except ValueError : continue self . set_path ( workdir / f \" { base_name } _execution_report_ { max_index + 1 } .md\" ) def write_report ( self ): # if the report path is not set, set it from the zip path self . _set_path_from_zip_path () if self . report_path is None : return with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_folder_structure ( file ) self . _write_python_packages_zip_content ( file ) self . _write_component_installed_packages ( file ) self . _write_payload_lengths ( file ) self . _write_warnings ( file ) def _write_headline ( self , file ): file . write ( LPLR_REPORT_HEADLINE ) def _write_folder_structure ( self , file ): file . write ( LPLR_FOLDER_STRUCTURE_HEADLINE . format ( file_name = self . zip_file_name )) file . write ( LPLR_FOLDER_STRUCTURE . format ( file_name = self . zip_file_name , folder_structure = self . folder_tree_text )) def _write_python_packages_zip_content ( self , file ): file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE ) sorted_zip_content = sorted ( list ( self . python_packages_zip_content )) for package in sorted_zip_content : file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT . format ( python_package = package )) file . write ( \" \\n \" ) def _write_component_installed_packages ( self , file ): file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE ) for component in self . component_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES . format ( component = component )) sorted_installed_packages = sorted ( self . component_installed_packages [ component ], key = lambda x : x [ 0 ]) for package_name , package_version , wheel_name in sorted_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_ROW . format ( package_name = package_name , package_version = package_version , wheel_name = wheel_name )) file . write ( \" \\n \" ) def _write_payload_lengths ( self , file ): file . write ( LPLR_PAYLOAD_LENGTHS_HEADLINE ) for component in self . component_payload_length : input_payload_length , output_payload_length = self . component_payload_length [ component ] file . write ( LPLR_PAYLOAD_LENGTHS . format ( component = component , input_payload_length = input_payload_length , output_payload_length = output_payload_length )) Variables LPLR_COMPONENT_INSTALLED_PACKAGES LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE LPLR_COMPONENT_INSTALLED_PACKAGES_ROW LPLR_FOLDER_STRUCTURE LPLR_FOLDER_STRUCTURE_FILE_LINE LPLR_FOLDER_STRUCTURE_FOLDER_LINE LPLR_FOLDER_STRUCTURE_HEADLINE LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL LPLR_PAYLOAD_LENGTHS LPLR_PAYLOAD_LENGTHS_HEADLINE LPLR_PYTHON_PACKAGES_ZIP_CONTENT LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE LPLR_REPORT_HEADLINE PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE PL_COMPONENT_DIRECT_DEPENDENCY PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE PL_COMPONENT_TRANSITIVE_DEPENDENCY PL_COMPONMENT_DEPENDENCIES_HEADLINE PL_INFO PL_INFO_HEADLINE PL_PACKAGE_VULNERABILITIES_HEADLINE PL_PACKAGE_VULNERABILITY PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED PL_PACKAGE_VULNERABILITY_NOT_KNOWN PL_REPORT_HEADLINE PL_STRUCTURE PL_STRUCTURE_HEADLINE WARNINGS_HEADLINE WARNING_LINE Classes PipelineReportWriter A class to generate a report for a dataflow pipeline, including pipeline structure, component dependencies, and package vulnerabilities. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_pipeline_config(pipeline_config: dict): Sets the pipeline configuration and updates the pipeline info and structure. add_full_dependency_set(component_name: str, dependency_set: set[tuple]): Adds a full set of dependencies for a component and updates the vulnerability dictionary. add_direct_dependencies(component_name: str, direct_dependencies: dict): Adds direct dependencies for a component. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. class PipelineReportWriter ( ) View Source class PipelineReportWriter ( ReportWriter ) : \"\"\" A class to generate a report for a dataflow pipeline , including pipeline structure , component dependencies , and package vulnerabilities . Methods : set_path ( report_path : Path ) : Sets the path where the report will be saved . set_pipeline_config ( pipeline_config : dict ) : Sets the pipeline configuration and updates the pipeline info and structure . add_full_dependency_set ( component_name : str , dependency_set : set [ tuple ]) : Adds a full set of dependencies for a component and updates the vulnerability dictionary . add_direct_dependencies ( component_name : str , direct_dependencies : dict ) : Adds direct dependencies for a component . add_warning ( name , filename , line_number , warning_msg ) : Adds a warning to the report . write_report () : Writes the report to the specified path . \"\"\" def __init__ ( self ) : super (). __init__ () self . pipeline_config = {} # pipeline config json self . pipeline_name = \"Unnamed pipeline\" # dependency_names and package_names are transformed dependency names (lowercase, underscore instead of dash) # set from outside self . component_direct_dependency_namelist = {} # component -> list of dependency_names ( with NO version ) self . component_all_dependencies = {} # component -> set of tuples of ( dependency_name , dependency_version ) # collected before writing self . component_direct_dependencies = {} # component -> set of dependency_names self . component_transitive_dependencies = {} # component -> set of tuples of ( dependency_name , dependency_version ) self . vulnerability_dict = {} # ( package_name , package_version ) -> vulnerabilities ( None | list of dictionaries ) # report text sections to fill self . pipeline_structure_text = \"\" self . pipeline_info_text = \"\" self . warnings_text = \"\" def set_pipeline_config ( self , pipeline_config : dict ) : self . pipeline_config = pipeline_config self . _set_pipeline_info () self . _set_pipeline_structure () @ staticmethod def _sort_pipeline_dag ( pipeline_dag : list ) -> list : \"\"\" Sorts a pipeline DAG in order to show dataflow from Pipeline Inputs to Pipeline Outputs . Databus component is a privileged source , and it is always the first component in the report . Args : pipeline_dag ( list ) : The pipeline DAG is a list of dictionaries with \"source\" and \"target\" keys . Returns : A sorted list of dictionaries representing the pipeline DAG . \"\"\" pipeline_dag . sort ( key = lambda x : ( x [ \"source\" ], x [ \"target\" ])) sorted_dag = [ edge for edge in pipeline_dag if \"Databus\" in edge [ ' source ' ]] if sorted_dag == [] : return pipeline_dag pipeline_dag = [ edge for edge in pipeline_dag if \"Databus\" not in edge [ ' source ' ]] # Extracts name of the target or source component from the edge name_of_component = lambda edge , target_or_source : edge [ target_or_source ]. rsplit ( \".\" , 1 )[ 0 ] while len ( pipeline_dag ) > 0 : sorted_targets = [ name_of_component ( edge , \"target\" ) for edge in sorted_dag ] sorted_dag . extend ([ edge for edge in pipeline_dag if name_of_component ( edge , \"source\" ) in sorted_targets ]) pipeline_dag = [ edge for edge in pipeline_dag if edge not in sorted_dag ] return sorted_dag def _set_pipeline_structure ( self ) : self . pipeline_name = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}). get ( \"projectName\" , \"n/a\" ) self . pipeline_structure_text = \"```plantuml \\n \" components = self . pipeline_config . get ( \"dataFlowPipeline\" , {}). get ( \"components\" , []) variables = {} # name : ( type , is_metric ) for component in components : input_variables = { _input [ \"name\" ] : ( _input [ \"type\" ], False ) for _input in component . get ( \"inputType\" )} output_variables = { _output [ \"name\" ] : ( _output [ \"type\" ], _output . get ( \"metric\" , False )) for _output in component . get ( \"outputType\" )} variables . update ({ ** input_variables , ** output_variables }) pipeline_dag = self . pipeline_config . get ( \"dataFlowPipeline\" , {}). get ( \"pipelineDag\" , []) sorted_pipeline_dag = PipelineReportWriter . _sort_pipeline_dag ( pipeline_dag ) for transition in sorted_pipeline_dag : source_component_name , source_variable_name = transition [ \"source\" ]. rsplit ( \".\" , 1 ) target_component_name , target_variable_name = transition [ \"target\" ]. rsplit ( \".\" , 1 ) variable_name_to_show = source_variable_name if source_variable_name == target_variable_name else f \"{source_variable_name} -> {target_variable_name}\" source_component_name = source_component_name . replace ( \"Databus\" , \"AIIS\" ) target_component_name = target_component_name . replace ( \"Databus\" , \"AIIS\" ) variable_type , is_metric = variables [ source_variable_name ] arrow = \"-->\" if is_metric else \"->\" # metric variables are drawn with a dashed line self . pipeline_structure_text += PL_STRUCTURE . format ( source_component = source_component_name , arrow = arrow , target_component = target_component_name , variable_name = variable_name_to_show , variable_type = variable_type ) self . pipeline_structure_text += \"``` \\n\\n \" def _set_pipeline_info ( self ) : dataflow_pipeline_info = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}) author = dataflow_pipeline_info . get ( \"author\" , \"n/a\" ) created_on = dataflow_pipeline_info . get ( \"createdOn\" , \"n/a\" ) pipeline_version = dataflow_pipeline_info . get ( \"dataFlowPipelineVersion\" , \"n/a\" ) description = dataflow_pipeline_info . get ( \"description\" , \"n/a\" ) package_id = dataflow_pipeline_info . get ( \"packageId\" , \"n/a\" ) project_name = dataflow_pipeline_info . get ( \"projectName\" , \"n/a\" ) self . pipeline_info_text = PL_INFO . format ( author = author , created_on = created_on , pipeline_version = pipeline_version , description = description , package_id = package_id , project_name = project_name ) # Transform every dependency and package name for consistency; i.e., # opencv-python-headless -> opencv_python_headless; Django -> django @ staticmethod def transform_package_name ( name : str ) : new_name = name . replace ( \"-\" , \"_\" ) return new_name . lower () # A full dependency set is a set of (package_name, package_version) tuples # and contains all the dependencies installed for a component def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ]) : dependency_list = sorted ( list ( dependency_set ), key = lambda x : x [ 0 ]) self . _expand_component_all_dependencies ( component_name , dependency_list ) self . _update_vulnerability_dict ( dependency_list ) def _expand_component_all_dependencies ( self , component_name : str , dependency_list : list [ tuple ]) : if component_name not in self . component_all_dependencies : self . component_all_dependencies [ component_name ] = set () for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) self . component_all_dependencies [ component_name ]. add (( transformed_package_name , package_version )) def _update_vulnerability_dict ( self , dependency_list : list [ tuple ]) : vulnerability_dict = {} for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) vulnerability_dict [( transformed_package_name , package_version )] = None url = f \"https://pypi.org/pypi/{package_name}/{package_version}/json\" try : response = requests . get ( url , timeout = 5 ) if response . status_code == 200 : data = response . json () if ' vulnerabilities ' in data : vulnerability_dict [( transformed_package_name , package_version )] = data [ ' vulnerabilities ' ] except requests . exceptions . Timeout : pass self . vulnerability_dict . update ( vulnerability_dict ) def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ) : self . component_direct_dependency_namelist [ component_name ] = [ PipelineReportWriter . transform_package_name ( name ) for name in list ( direct_dependencies . keys ())] def write_report ( self ) : if self . report_path is None : return self . _set_component_dependencies () with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_pipeline_info ( file ) self . _write_pipeline_structure ( file ) self . _write_dependencies ( file ) self . _write_package_vulnerabilities ( file ) self . _write_warnings ( file ) def _set_component_dependencies ( self ) : for component in self . component_all_dependencies . keys () : # self.component_direct_dependencies should contain everything from self.component_all_dependencies # if it is direct, i.e., the name is in self.component_direct_dependency_namelist # self.component_transitive_dependencies should contain everything else self . component_transitive_dependencies [ component ] = set () self . component_direct_dependencies [ component ] = set () all_dependencies = self . component_all_dependencies [ component ] for dependency_name , dependency_version in all_dependencies : if dependency_name in self . component_direct_dependency_namelist . get ( component , []) : self . component_direct_dependencies [ component ]. add (( dependency_name , dependency_version )) else : self . component_transitive_dependencies [ component ]. add (( dependency_name , dependency_version )) def _write_headline ( self , file ) : file . write ( PL_REPORT_HEADLINE . format ( pipeline_name = self . pipeline_name )) def _write_pipeline_info ( self , file ) : file . write ( PL_INFO_HEADLINE ) file . write ( self . pipeline_info_text ) def _write_pipeline_structure ( self , file ) : file . write ( PL_STRUCTURE_HEADLINE ) file . write ( self . pipeline_structure_text ) def _write_dependencies ( self , file ) : for component_name in self . component_all_dependencies . keys () : direct_dependencies = self . component_direct_dependencies . get ( component_name , set ()) transitive_dependencies = self . component_transitive_dependencies . get ( component_name , set ()) file . write ( PL_COMPONMENT_DEPENDENCIES_HEADLINE . format ( component_name = component_name )) file . write ( PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE ) sorted_direct_dependencies = sorted ( list ( direct_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_direct_dependencies : file . write ( PL_COMPONENT_DIRECT_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE ) sorted_transitive_dependencies = sorted ( list ( transitive_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_transitive_dependencies : file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) def _get_components_who_have_given_package ( self , package_name , package_version ) : components = [] for component in self . component_all_dependencies : dependencies = self . component_all_dependencies [ component ] if ( package_name , package_version ) in dependencies : components . append ( component ) return components def _write_package_vulnerabilities ( self , file ) : file . write ( PL_PACKAGE_VULNERABILITIES_HEADLINE ) sorted_vulnerability_dict_items = sorted ( self . vulnerability_dict . items (), key = lambda x : x [ 0 ][ 0 ]) for ( package_name , package_version ), vulnerabilities in sorted_vulnerability_dict_items : components = ' , ' . join ( self . _get_components_who_have_given_package ( package_name , package_version )) if vulnerabilities is None : file . write ( PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED . format ( package_name = package_name , package_version = package_version , components = components )) elif vulnerabilities == [] : file . write ( PL_PACKAGE_VULNERABILITY_NOT_KNOWN . format ( package_name = package_name , package_version = package_version , components = components )) else : for vulnerability in vulnerabilities : vulnerability_aliases = vulnerability . get ( ' aliases ' , ' Vulnerability found with no alias . Check [ PyPI repository ]( https : //pypi.org/) for more details.') vulnerability_link = vulnerability . get ( ' link ' , ' No link found ' ) if vulnerability_link != ' No link found ' : vulnerability_link = f \"[{vulnerability_link}]({vulnerability_link})\" vulnerability_details = vulnerability . get ( ' details ' , ' No details found ' ) vulnerability_fixed_in = vulnerability . get ( ' fixed_in ' , '' ) file . write ( PL_PACKAGE_VULNERABILITY . format ( package_name = package_name , package_version = package_version , vulnerability_aliases = vulnerability_aliases , vulnerability_link = vulnerability_link , vulnerability_details = vulnerability_details , vulnerability_fixed_in = vulnerability_fixed_in , components = components )) file . write ( \" \\n \" ) Ancestors (in MRO) simaticai.helpers.reporter.ReportWriter Static methods transform_package_name def transform_package_name ( name : str ) View Source @ staticmethod def transform_package_name ( name : str ): new_name = name . replace ( \"-\" , \"_\" ) return new_name . lower () Methods add_direct_dependencies def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ) View Source def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ) : self . component_direct_dependency_namelist [ component_name ] = [ PipelineReportWriter.transform_package_name(name) for name in list(direct_dependencies.keys()) ] add_full_dependency_set def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ] ) View Source def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ] ) : dependency_list = sorted ( list ( dependency_set ), key = lambda x : x [ 0 ] ) self . _expand_component_all_dependencies ( component_name , dependency_list ) self . _update_vulnerability_dict ( dependency_list ) add_warning def add_warning ( self , name , filename , line_number , warning_msg ) View Source def add_warning(self, name, filename, line_number, warning_msg): self.warnings_text += WARNING_LINE.format(name=name, filename=filename, line_number=line_number, warning_msg=warning_msg) set_path def set_path ( self , report_path : pathlib . Path ) View Source def set_path(self, report_path: Path): self.report_path = report_path set_pipeline_config def set_pipeline_config ( self , pipeline_config : dict ) View Source def set_pipeline_config(self, pipeline_config: dict): self.pipeline_config = pipeline_config self._set_pipeline_info() self._set_pipeline_structure() write_report def write_report ( self ) View Source def write_report ( self ): if self . report_path is None : return self . _set_component_dependencies () with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_pipeline_info ( file ) self . _write_pipeline_structure ( file ) self . _write_dependencies ( file ) self . _write_package_vulnerabilities ( file ) self . _write_warnings ( file ) PipelineRunnerReportWriter PipelineRunnerReportWriter is responsible for generating a detailed report of a local pipeline execution. It builds folder structures from zip files, manages component payload counts, and adds installed packages information. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_package_zip_path(zip_path: Path): Sets the path to the package zip file and updates the folder tree. set_input_payload_length(component_name: str, length: int): Sets the input payload length for a component. set_output_payload_length(component_name: str, length: int): Sets the output payload length for a component. add_installed_packages(component_name: str, pip_report_file: Path): Adds installed packages for a component from a pip report file. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. class PipelineRunnerReportWriter ( ) View Source class PipelineRunnerReportWriter ( ReportWriter ): \"\"\" PipelineRunnerReportWriter is responsible for generating a detailed report of a local pipeline execution. It builds folder structures from zip files, manages component payload counts, and adds installed packages information. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_package_zip_path(zip_path: Path): Sets the path to the package zip file and updates the folder tree. set_input_payload_length(component_name: str, length: int): Sets the input payload length for a component. set_output_payload_length(component_name: str, length: int): Sets the output payload length for a component. add_installed_packages(component_name: str, pip_report_file: Path): Adds installed packages for a component from a pip report file. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. \"\"\" def __init__ ( self ): super () . __init__ () self . package_zip_path = None self . zip_file_name = \"\" self . component_installed_packages = {} # component_name -> list[tuple(package_name, package_version, whl_name)] self . component_payload_length = {} # component_name -> [input_payload_length, output_payload_length] self . python_packages_zip_content = set () # report text sections to fill self . folder_tree_text = \"\" self . warnings_text = \"\" def set_package_zip_path ( self , zip_path : Path ): self . package_zip_path = zip_path with zipfile . ZipFile ( zip_path , 'r' ) as zipf : self . zip_file_name = zipf . filename zip_tree = {} for item_name in zipf . namelist (): zip_tree [ item_name ] = ZipTreeElement ( full_name = item_name , file_size = zipf . getinfo ( item_name ) . file_size ) self . _print_structure_recursively ( zip_tree , zipf ) @ staticmethod def _get_folder_and_file_list ( item_names : list ) -> tuple [ list , list ]: \"\"\" Given a list of item names, each item name is a file that either starts with a folder name, or not. This function separates the folder names and the standalone file names. E.g., [\"a/b/something.txt\", \"c/another.txt\", \"else.txt\"] -> [\"a/\", \"c/\"], [\"else.txt\"] \"\"\" folder_names = set () file_names = [] for item in item_names : item_parts = item . split ( '/' ) if len ( item_parts ) > 1 : if item_parts [ 0 ] != '' : folder_names . add ( item_parts [ 0 ] + '/' ) else : if item != '' : file_names . append ( item ) return sorted ( list ( folder_names )), sorted ( file_names ) @ staticmethod def format_size ( size ): \"\"\"Format file size in human-readable form.\"\"\" for unit in [ 'B' , 'KB' , 'MB' ]: if size < 1000 : return f \"{size} {unit}\" size //= 1000 return f \"{size} GB\" def _print_structure_recursively ( self , zip_tree , zipf , prefix = \"\" ): folder_names , file_names = PipelineRunnerReportWriter . _get_folder_and_file_list ( zip_tree . keys ()) is_file_names_empty = file_names == [] self . _print_folder_structure ( zip_tree , zipf , prefix , folder_names , is_file_names_empty ) self . _print_file_structure ( zip_tree , zipf , prefix , file_names ) def _print_folder_structure ( self , zip_tree , zipf , prefix , folder_names , is_file_names_empty ): for i , folder in enumerate ( folder_names ): is_last = ( i == len ( folder_names ) - 1 ) and is_file_names_empty connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FOLDER_LINE . format ( prefix = prefix , connector = connector , folder = folder ) # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) new_zip_tree_from_folder = {} for k , v in zip_tree . items (): if k . startswith ( folder ): new_file_name = k . split ( '/' , 1 )[ 1 ] new_zip_tree_from_folder [ new_file_name ] = v prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_folder = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_folder , zipf , new_prefix_from_folder ) def _print_file_structure ( self , zip_tree , zipf , prefix , file_names ): for i , file_name in enumerate ( file_names ): is_last = ( i == len ( file_names ) - 1 ) size_str = PipelineRunnerReportWriter . format_size ( zip_tree [ file_name ] . file_size ) connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FILE_LINE . format ( prefix = prefix , connector = connector , file = file_name , size = size_str ) # zip files are handled similarly to folders: # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) if not file_name . endswith ( '.zip' ): continue full_name = zip_tree [ file_name ] . full_name with zipf . open ( full_name ) as nested_zip_file : nested_zip_data = io . BytesIO ( nested_zip_file . read ()) with zipfile . ZipFile ( nested_zip_data , 'r' ) as nested_zipf : if file_name == \"PythonPackages.zip\" : self . python_packages_zip_content . update ( sorted ( list ( nested_zipf . namelist ()))) # create a new tree where items start with the same folder name; but cut out the folder name new_zip_tree_from_zip = {} for nested_item_name in nested_zipf . namelist (): new_zip_tree_from_zip [ nested_item_name ] = ZipTreeElement ( nested_item_name , nested_zipf . getinfo ( nested_item_name ) . file_size ) prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_zip = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_zip , nested_zipf , new_prefix_from_zip ) def set_input_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 0 ] = length else : self . component_payload_length [ component_name ] = [ length , 0 ] def set_output_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 1 ] = length else : self . component_payload_length [ component_name ] = [ 0 , length ] def add_installed_packages ( self , component_name : str , pip_report_file : Path ): pip_report = {} with open ( pip_report_file , 'r' ) as file : pip_report = json . load ( file ) if component_name not in self . component_installed_packages : self . component_installed_packages [ component_name ] = [] installed_packages = pip_report . get ( \"install\" , []) for package in installed_packages : package_url = package . get ( \"download_info\" , {}) . get ( \"url\" , \"\" ) wheel_name = package_url . split ( \"/\" )[ - 1 ] if package_url . endswith ( \".whl\" ) else \"n/a\" metadata = package . get ( \"metadata\" , {}) package_name = metadata . get ( \"name\" , \"n/a\" ) package_version = metadata . get ( \"version\" , \"n/a\" ) self . component_installed_packages [ component_name ] . append (( package_name , package_version , wheel_name )) # check if one or more reports already exists; set the report path so a new report will have a new index def _set_path_from_zip_path ( self ): if self . package_zip_path is None : return workdir = self . package_zip_path . parent base_name = self . package_zip_path . stem report_files = list ( workdir . glob ( f \"{base_name}_execution_report_*.md\" )) max_index = 0 for report_file in report_files : try : index = int ( report_file . stem . split ( '_' )[ - 1 ]) if index > max_index : max_index = index except ValueError : continue self . set_path ( workdir / f \"{base_name}_execution_report_{max_index + 1}.md\" ) def write_report ( self ): # if the report path is not set, set it from the zip path self . _set_path_from_zip_path () if self . report_path is None : return with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_folder_structure ( file ) self . _write_python_packages_zip_content ( file ) self . _write_component_installed_packages ( file ) self . _write_payload_lengths ( file ) self . _write_warnings ( file ) def _write_headline ( self , file ): file . write ( LPLR_REPORT_HEADLINE ) def _write_folder_structure ( self , file ): file . write ( LPLR_FOLDER_STRUCTURE_HEADLINE . format ( file_name = self . zip_file_name )) file . write ( LPLR_FOLDER_STRUCTURE . format ( file_name = self . zip_file_name , folder_structure = self . folder_tree_text )) def _write_python_packages_zip_content ( self , file ): file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE ) sorted_zip_content = sorted ( list ( self . python_packages_zip_content )) for package in sorted_zip_content : file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT . format ( python_package = package )) file . write ( \" \\n \" ) def _write_component_installed_packages ( self , file ): file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE ) for component in self . component_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES . format ( component = component )) sorted_installed_packages = sorted ( self . component_installed_packages [ component ], key = lambda x : x [ 0 ]) for package_name , package_version , wheel_name in sorted_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_ROW . format ( package_name = package_name , package_version = package_version , wheel_name = wheel_name )) file . write ( \" \\n \" ) def _write_payload_lengths ( self , file ): file . write ( LPLR_PAYLOAD_LENGTHS_HEADLINE ) for component in self . component_payload_length : input_payload_length , output_payload_length = self . component_payload_length [ component ] file . write ( LPLR_PAYLOAD_LENGTHS . format ( component = component , input_payload_length = input_payload_length , output_payload_length = output_payload_length )) Ancestors (in MRO) simaticai.helpers.reporter.ReportWriter Static methods format_size def format_size ( size ) Format file size in human-readable form. View Source @staticmethod def format_size ( size ) : \"\"\"Format file size in human-readable form.\"\"\" for unit in [ 'B', 'KB', 'MB' ] : if size < 1000 : return f \"{size} {unit}\" size //= 1000 return f \"{size} GB\" Methods add_installed_packages def add_installed_packages ( self , component_name : str , pip_report_file : pathlib . Path ) View Source def add_installed_packages ( self , component_name : str , pip_report_file : Path ) : pip_report = {} with open ( pip_report_file , 'r' ) as file : pip_report = json . load ( file ) if component_name not in self . component_installed_packages : self . component_installed_packages [ component_name ] = [] installed_packages = pip_report . get ( \"install\" , [] ) for package in installed_packages : package_url = package . get ( \"download_info\" , {} ). get ( \"url\" , \"\" ) wheel_name = package_url . split ( \"/\" ) [ -1 ] if package_url . endswith ( \".whl\" ) else \"n/a\" metadata = package . get ( \"metadata\" , {} ) package_name = metadata . get ( \"name\" , \"n/a\" ) package_version = metadata . get ( \"version\" , \"n/a\" ) self . component_installed_packages [ component_name ] . append (( package_name , package_version , wheel_name )) add_warning def add_warning ( self , name , filename , line_number , warning_msg ) View Source def add_warning(self, name, filename, line_number, warning_msg): self.warnings_text += WARNING_LINE.format(name=name, filename=filename, line_number=line_number, warning_msg=warning_msg) set_input_payload_length def set_input_payload_length ( self , component_name : str , length : int ) View Source def set_input_payload_length ( self , component_name : str , length : int ) : if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 0 ] = length else : self . component_payload_length [ component_name ] = [ length, 0 ] set_output_payload_length def set_output_payload_length ( self , component_name : str , length : int ) View Source def set_output_payload_length ( self , component_name : str , length : int ) : if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 1 ] = length else : self . component_payload_length [ component_name ] = [ 0, length ] set_package_zip_path def set_package_zip_path ( self , zip_path : pathlib . Path ) View Source def set_package_zip_path ( self , zip_path : Path ) : self . package_zip_path = zip_path with zipfile . ZipFile ( zip_path , 'r' ) as zipf : self . zip_file_name = zipf . filename zip_tree = {} for item_name in zipf . namelist () : zip_tree [ item_name ] = ZipTreeElement ( full_name = item_name , file_size = zipf . getinfo ( item_name ). file_size ) self . _print_structure_recursively ( zip_tree , zipf ) set_path def set_path ( self , report_path : pathlib . Path ) View Source def set_path(self, report_path: Path): self.report_path = report_path write_report def write_report ( self ) View Source def write_report ( self ): # if the report path is not set, set it from the zip path self . _set_path_from_zip_path () if self . report_path is None : return with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_folder_structure ( file ) self . _write_python_packages_zip_content ( file ) self . _write_component_installed_packages ( file ) self . _write_payload_lengths ( file ) self . _write_warnings ( file ) ReportWriter Base class for report writers. class ReportWriter ( ) View Source class ReportWriter : \"\"\" Base class for report writers. \"\"\" def __init__ ( self ): self . report_path = None self . warnings_text = \"\" def set_path ( self , report_path : Path ): self . report_path = report_path def add_warning ( self , name , filename , line_number , warning_msg ): self . warnings_text += WARNING_LINE . format ( name = name , filename = filename , line_number = line_number , warning_msg = warning_msg ) def write_report ( self ): raise NotImplementedError ( \"Subclasses should implement this method\" ) def _write_warnings ( self , file ): file . write ( WARNINGS_HEADLINE ) file . write ( self . warnings_text ) Descendants simaticai.helpers.reporter.PipelineReportWriter simaticai.helpers.reporter.PipelineRunnerReportWriter Methods add_warning def add_warning ( self , name , filename , line_number , warning_msg ) View Source def add_warning(self, name, filename, line_number, warning_msg): self.warnings_text += WARNING_LINE.format(name=name, filename=filename, line_number=line_number, warning_msg=warning_msg) set_path def set_path ( self , report_path : pathlib . Path ) View Source def set_path(self, report_path: Path): self.report_path = report_path write_report def write_report ( self ) View Source def write_report ( self ): raise NotImplementedError ( \"Subclasses should implement this method\" ) ReportWriterHandler A handler that can be given to a logger, so the report writer can capture logged warning messages class ReportWriterHandler ( report_writer : simaticai . helpers . reporter . ReportWriter ) View Source class ReportWriterHandler ( logging . Handler ): \"\"\" A handler that can be given to a logger, so the report writer can capture logged warning messages \"\"\" def __init__ ( self , report_writer: ReportWriter ): super (). __init__ () self . report_writer = report_writer def emit ( self , record ): if record . levelno == logging . WARNING: self . report_writer . add_warning ( record . name , record . filename , record . lineno , record . getMessage ()) Ancestors (in MRO) logging.Handler logging.Filterer Instance variables name Methods acquire def acquire ( self ) Acquire the I/O thread lock. View Source def acquire ( self ) : \"\" \" Acquire the I/O thread lock. \"\" \" if self.lock: self.lock.acquire() addFilter def addFilter ( self , filter ) Add the specified filter to this handler. View Source def addFilter ( self , filter ) : \"\" \" Add the specified filter to this handler. \"\" \" if not (filter in self.filters): self.filters.append(filter) close def close ( self ) Tidy up any resources used by the handler. This version removes the handler from an internal map of handlers, _handlers, which is used for handler lookup by name. Subclasses should ensure that this gets called from overridden close() methods. View Source def close(self): \"\"\" Tidy up any resources used by the handler. This version removes the handler from an internal map of handlers, _handlers, which is used for handler lookup by name. Subclasses should ensure that this gets called from overridden close() methods. \"\"\" #get the module data lock, as we're updating a shared structure. _acquireLock() try: #unlikely to raise an exception, but you never know... self._closed = True if self._name and self._name in _handlers: del _handlers[self._name] finally: _releaseLock() createLock def createLock ( self ) Acquire a thread lock for serializing access to the underlying I/O. View Source def createLock ( self ) : \"\"\" Acquire a thread lock for serializing access to the underlying I / O . \"\"\" self . lock = threading . RLock () _register_at_fork_reinit_lock ( self ) emit def emit ( self , record ) Do whatever it takes to actually log the specified logging record. This version is intended to be implemented by subclasses and so raises a NotImplementedError. View Source def emit(self, record): if record.levelno == logging.WARNING: self.report_writer.add_warning(record.name, record.filename, record.lineno, record.getMessage()) filter def filter ( self , record ) Determine if a record is loggable by consulting all the filters. The default is to allow the record to be logged; any filter can veto this and the record is then dropped. Returns a zero value if a record is to be dropped, else non-zero. .. versionchanged:: 3.2 Allow filters to be just callables. View Source def filter ( self , record ) : \"\" \" Determine if a record is loggable by consulting all the filters. The default is to allow the record to be logged; any filter can veto this and the record is then dropped. Returns a zero value if a record is to be dropped, else non-zero. .. versionchanged:: 3.2 Allow filters to be just callables. \"\" \" rv = True for f in self.filters: if hasattr(f, 'filter'): result = f.filter(record) else: result = f(record) # assume callable - will raise if not if not result: rv = False break return rv flush def flush ( self ) Ensure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses. View Source def flush ( self ): \"\"\" Ensure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses. \"\"\" pass format def format ( self , record ) Format the specified record. If a formatter is set, use it. Otherwise, use the default formatter for the module. View Source def format(self, record): \"\"\" Format the specified record. If a formatter is set, use it. Otherwise, use the default formatter for the module. \"\"\" if self.formatter: fmt = self.formatter else: fmt = _defaultFormatter return fmt.format(record) get_name def get_name ( self ) View Source def get_name(self): return self._name handle def handle ( self , record ) Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler. Wrap the actual emission of the record with acquisition/release of the I/O thread lock. Returns whether the filter passed the record for emission. View Source def handle ( self , record ) : \"\" \" Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler. Wrap the actual emission of the record with acquisition/release of the I/O thread lock. Returns whether the filter passed the record for emission. \"\" \" rv = self.filter(record) if rv: self.acquire() try: self.emit(record) finally: self.release() return rv handleError def handleError ( self , record ) Handle errors which occur during an emit() call. This method should be called from handlers when an exception is encountered during an emit() call. If raiseExceptions is false, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The record which was being processed is passed in to this method. View Source def handleError(self, record): \"\"\" Handle errors which occur during an emit() call. This method should be called from handlers when an exception is encountered during an emit() call. If raiseExceptions is false, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The record which was being processed is passed in to this method. \"\"\" if raiseExceptions and sys.stderr: # see issue 13807 t, v, tb = sys.exc_info() try: sys.stderr.write('--- Logging error ---\\n') traceback.print_exception(t, v, tb, None, sys.stderr) sys.stderr.write('Call stack:\\n') # Walk the stack frame up until we're out of logging, # so as to print the calling context. frame = tb.tb_frame while (frame and os.path.dirname(frame.f_code.co_filename) == __path__[0]): frame = frame.f_back if frame: traceback.print_stack(frame, file=sys.stderr) else: # couldn't find the right stack frame, for some reason sys.stderr.write('Logged from file %s, line %s\\n' % ( record.filename, record.lineno)) # Issue 18671: output logging message and arguments try: sys.stderr.write('Message: %r\\n' 'Arguments: %s\\n' % (record.msg, record.args)) except RecursionError: # See issue 36272 raise except Exception: sys.stderr.write('Unable to print the message and arguments' ' - possible formatting error.\\nUse the' ' traceback above to help find the error.\\n' ) except OSError: #pragma: no cover pass # see issue 5971 finally: del t, v, tb release def release ( self ) Release the I/O thread lock. View Source def release ( self ) : \"\" \" Release the I/O thread lock. \"\" \" if self.lock: self.lock.release() removeFilter def removeFilter ( self , filter ) Remove the specified filter from this handler. View Source def removeFilter ( self , filter ) : \"\" \" Remove the specified filter from this handler. \"\" \" if filter in self.filters: self.filters.remove(filter) setFormatter def setFormatter ( self , fmt ) Set the formatter for this handler. View Source def setFormatter ( self , fmt ) : \"\" \" Set the formatter for this handler. \"\" \" self.formatter = fmt setLevel def setLevel ( self , level ) Set the logging level of this handler. level must be an int or a str. View Source def setLevel(self, level): \"\"\" Set the logging level of this handler. level must be an int or a str. \"\"\" self.level = _checkLevel(level) set_name def set_name ( self , name ) View Source def set_name ( self , name ) : _acquireLock () try : if self . _name in _handlers : del _handlers [ self._name ] self . _name = name if name : _handlers [ name ] = self finally : _releaseLock () ZipTreeElement A class to represent a file or folder in a zip file. During the recursive traversal of the zip file, the full name and file size are stored in this class. class ZipTreeElement ( full_name , file_size ) View Source class ZipTreeElement: \"\"\" A class to represent a file or folder in a zip file. During the recursive traversal of the zip file, the full name and file size are stored in this class. \"\"\" def __init__ ( self , full_name , file_size ): self . full_name = full_name self . file_size = file_size","title":"Reporter"},{"location":"reference/simaticai/helpers/reporter.html#module-simaticaihelpersreporter","text":"Classes to generate a report for a dataflow pipeline and local pipeline runner. None View Source # Copyright (C) Siemens AG 2025. All Rights Reserved. Confidential. \"\"\" Classes to generate a report for a dataflow pipeline and local pipeline runner. \"\"\" from pathlib import Path import requests import logging import zipfile import json import io WARNINGS_HEADLINE = \"## Warnings \\n\\n \" WARNING_LINE = \" {name} : {filename} : {line_number} (W) {warning_msg} \\n\\n \" class ReportWriter : \"\"\" Base class for report writers. \"\"\" def __init__ ( self ): self . report_path = None self . warnings_text = \"\" def set_path ( self , report_path : Path ): self . report_path = report_path def add_warning ( self , name , filename , line_number , warning_msg ): self . warnings_text += WARNING_LINE . format ( name = name , filename = filename , line_number = line_number , warning_msg = warning_msg ) def write_report ( self ): raise NotImplementedError ( \"Subclasses should implement this method\" ) def _write_warnings ( self , file ): file . write ( WARNINGS_HEADLINE ) file . write ( self . warnings_text ) class ReportWriterHandler ( logging . Handler ): \"\"\" A handler that can be given to a logger, so the report writer can capture logged warning messages \"\"\" def __init__ ( self , report_writer : ReportWriter ): super () . __init__ () self . report_writer = report_writer def emit ( self , record ): if record . levelno == logging . WARNING : self . report_writer . add_warning ( record . name , record . filename , record . lineno , record . getMessage ()) class ZipTreeElement : \"\"\" A class to represent a file or folder in a zip file. During the recursive traversal of the zip file, the full name and file size are stored in this class. \"\"\" def __init__ ( self , full_name , file_size ): self . full_name = full_name self . file_size = file_size PL_REPORT_HEADLINE = \"# Report on ` {pipeline_name} ` \\n\\n \" PL_INFO_HEADLINE = \"## Pipeline info \\n\\n \" PL_INFO = \"\"\"- Author: {author} - Created on: {created_on} - Dataflow Pipeline version: {pipeline_version} - Package ID: {package_id} - Project name: {project_name} Description: {description} \"\"\" # TODO: check other type of PlantUML diagrams if they look better or generating images into the markdown file PL_STRUCTURE_HEADLINE = \"\"\"## Pipeline structure The pipeline structure is visualized using PlantUML. The components are connected by arrows. Metrics are drawn with dashed arrows. \"\"\" PL_STRUCTURE = \" {source_component} {arrow} {target_component} : {variable_name} ( {variable_type} ) \\n \" PL_PACKAGE_VULNERABILITIES_HEADLINE = \"\"\"## Package vulnerabilities Package vulnerability information is collected from the [PyPI repository](https://pypi.org/). | Package name | Package version | Vulnerability | Link | Details | Fixed in | In pipeline components | |--------------|-----------------|---------------|------|---------|----------|------------------------| \"\"\" PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED = \"| {package_name} | {package_version} | Can not be checked | - | - | - | {components} | \\n \" PL_PACKAGE_VULNERABILITY_NOT_KNOWN = \"| {package_name} | {package_version} | No known vulnerability | - | - | - | {components} | \\n \" PL_PACKAGE_VULNERABILITY = \"| {package_name} | {package_version} | {vulnerability_aliases} | {vulnerability_link} | {vulnerability_details} | {vulnerability_fixed_in} | {components} | \\n \" PL_COMPONMENT_DEPENDENCIES_HEADLINE = \"## Component dependencies for ` {component_name} ` \\n\\n \" PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE = \"### Direct dependencies \\n\\n \" PL_COMPONENT_DIRECT_DEPENDENCY = \"- {dependency_name} {dependency_version} \\n \" PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE = \"### Transitive dependencies \\n\\n \" PL_COMPONENT_TRANSITIVE_DEPENDENCY = \"- {dependency_name} {dependency_version} \\n \" class PipelineReportWriter ( ReportWriter ): \"\"\" A class to generate a report for a dataflow pipeline, including pipeline structure, component dependencies, and package vulnerabilities. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_pipeline_config(pipeline_config: dict): Sets the pipeline configuration and updates the pipeline info and structure. add_full_dependency_set(component_name: str, dependency_set: set[tuple]): Adds a full set of dependencies for a component and updates the vulnerability dictionary. add_direct_dependencies(component_name: str, direct_dependencies: dict): Adds direct dependencies for a component. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. \"\"\" def __init__ ( self ): super () . __init__ () self . pipeline_config = {} # pipeline config json self . pipeline_name = \"Unnamed pipeline\" # dependency_names and package_names are transformed dependency names (lowercase, underscore instead of dash) # set from outside self . component_direct_dependency_namelist = {} # component -> list of dependency_names (with NO version) self . component_all_dependencies = {} # component -> set of tuples of (dependency_name, dependency_version) # collected before writing self . component_direct_dependencies = {} # component -> set of dependency_names self . component_transitive_dependencies = {} # component -> set of tuples of (dependency_name, dependency_version) self . vulnerability_dict = {} # (package_name, package_version) -> vulnerabilities (None | list of dictionaries) # report text sections to fill self . pipeline_structure_text = \"\" self . pipeline_info_text = \"\" self . warnings_text = \"\" def set_pipeline_config ( self , pipeline_config : dict ): self . pipeline_config = pipeline_config self . _set_pipeline_info () self . _set_pipeline_structure () @staticmethod def _sort_pipeline_dag ( pipeline_dag : list ) -> list : \"\"\" Sorts a pipeline DAG in order to show dataflow from Pipeline Inputs to Pipeline Outputs. Databus component is a privileged source, and it is always the first component in the report. Args: pipeline_dag (list): The pipeline DAG is a list of dictionaries with \"source\" and \"target\" keys. Returns: A sorted list of dictionaries representing the pipeline DAG. \"\"\" pipeline_dag . sort ( key = lambda x : ( x [ \"source\" ], x [ \"target\" ])) sorted_dag = [ edge for edge in pipeline_dag if \"Databus\" in edge [ 'source' ]] if sorted_dag == []: return pipeline_dag pipeline_dag = [ edge for edge in pipeline_dag if \"Databus\" not in edge [ 'source' ]] # Extracts name of the target or source component from the edge name_of_component = lambda edge , target_or_source : edge [ target_or_source ] . rsplit ( \".\" , 1 )[ 0 ] while len ( pipeline_dag ) > 0 : sorted_targets = [ name_of_component ( edge , \"target\" ) for edge in sorted_dag ] sorted_dag . extend ([ edge for edge in pipeline_dag if name_of_component ( edge , \"source\" ) in sorted_targets ]) pipeline_dag = [ edge for edge in pipeline_dag if edge not in sorted_dag ] return sorted_dag def _set_pipeline_structure ( self ): self . pipeline_name = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}) . get ( \"projectName\" , \"n/a\" ) self . pipeline_structure_text = \"```plantuml \\n \" components = self . pipeline_config . get ( \"dataFlowPipeline\" , {}) . get ( \"components\" , []) variables = {} # name: (type, is_metric) for component in components : input_variables = { _input [ \"name\" ]: ( _input [ \"type\" ], False ) for _input in component . get ( \"inputType\" )} output_variables = { _output [ \"name\" ]: ( _output [ \"type\" ], _output . get ( \"metric\" , False )) for _output in component . get ( \"outputType\" )} variables . update ({ ** input_variables , ** output_variables }) pipeline_dag = self . pipeline_config . get ( \"dataFlowPipeline\" , {}) . get ( \"pipelineDag\" , []) sorted_pipeline_dag = PipelineReportWriter . _sort_pipeline_dag ( pipeline_dag ) for transition in sorted_pipeline_dag : source_component_name , source_variable_name = transition [ \"source\" ] . rsplit ( \".\" , 1 ) target_component_name , target_variable_name = transition [ \"target\" ] . rsplit ( \".\" , 1 ) variable_name_to_show = source_variable_name if source_variable_name == target_variable_name else f \" { source_variable_name } -> { target_variable_name } \" source_component_name = source_component_name . replace ( \"Databus\" , \"AIIS\" ) target_component_name = target_component_name . replace ( \"Databus\" , \"AIIS\" ) variable_type , is_metric = variables [ source_variable_name ] arrow = \"-->\" if is_metric else \"->\" # metric variables are drawn with a dashed line self . pipeline_structure_text += PL_STRUCTURE . format ( source_component = source_component_name , arrow = arrow , target_component = target_component_name , variable_name = variable_name_to_show , variable_type = variable_type ) self . pipeline_structure_text += \"``` \\n\\n \" def _set_pipeline_info ( self ): dataflow_pipeline_info = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}) author = dataflow_pipeline_info . get ( \"author\" , \"n/a\" ) created_on = dataflow_pipeline_info . get ( \"createdOn\" , \"n/a\" ) pipeline_version = dataflow_pipeline_info . get ( \"dataFlowPipelineVersion\" , \"n/a\" ) description = dataflow_pipeline_info . get ( \"description\" , \"n/a\" ) package_id = dataflow_pipeline_info . get ( \"packageId\" , \"n/a\" ) project_name = dataflow_pipeline_info . get ( \"projectName\" , \"n/a\" ) self . pipeline_info_text = PL_INFO . format ( author = author , created_on = created_on , pipeline_version = pipeline_version , description = description , package_id = package_id , project_name = project_name ) # Transform every dependency and package name for consistency; i.e., # opencv-python-headless -> opencv_python_headless; Django -> django @staticmethod def transform_package_name ( name : str ): new_name = name . replace ( \"-\" , \"_\" ) return new_name . lower () # A full dependency set is a set of (package_name, package_version) tuples # and contains all the dependencies installed for a component def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ]): dependency_list = sorted ( list ( dependency_set ), key = lambda x : x [ 0 ]) self . _expand_component_all_dependencies ( component_name , dependency_list ) self . _update_vulnerability_dict ( dependency_list ) def _expand_component_all_dependencies ( self , component_name : str , dependency_list : list [ tuple ]): if component_name not in self . component_all_dependencies : self . component_all_dependencies [ component_name ] = set () for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) self . component_all_dependencies [ component_name ] . add (( transformed_package_name , package_version )) def _update_vulnerability_dict ( self , dependency_list : list [ tuple ]): vulnerability_dict = {} for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) vulnerability_dict [( transformed_package_name , package_version )] = None url = f \"https://pypi.org/pypi/ { package_name } / { package_version } /json\" try : response = requests . get ( url , timeout = 5 ) if response . status_code == 200 : data = response . json () if 'vulnerabilities' in data : vulnerability_dict [( transformed_package_name , package_version )] = data [ 'vulnerabilities' ] except requests . exceptions . Timeout : pass self . vulnerability_dict . update ( vulnerability_dict ) def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ): self . component_direct_dependency_namelist [ component_name ] = [ PipelineReportWriter . transform_package_name ( name ) for name in list ( direct_dependencies . keys ())] def write_report ( self ): if self . report_path is None : return self . _set_component_dependencies () with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_pipeline_info ( file ) self . _write_pipeline_structure ( file ) self . _write_dependencies ( file ) self . _write_package_vulnerabilities ( file ) self . _write_warnings ( file ) def _set_component_dependencies ( self ): for component in self . component_all_dependencies . keys (): # self.component_direct_dependencies should contain everything from self.component_all_dependencies # if it is direct, i.e., the name is in self.component_direct_dependency_namelist # self.component_transitive_dependencies should contain everything else self . component_transitive_dependencies [ component ] = set () self . component_direct_dependencies [ component ] = set () all_dependencies = self . component_all_dependencies [ component ] for dependency_name , dependency_version in all_dependencies : if dependency_name in self . component_direct_dependency_namelist . get ( component , []): self . component_direct_dependencies [ component ] . add (( dependency_name , dependency_version )) else : self . component_transitive_dependencies [ component ] . add (( dependency_name , dependency_version )) def _write_headline ( self , file ): file . write ( PL_REPORT_HEADLINE . format ( pipeline_name = self . pipeline_name )) def _write_pipeline_info ( self , file ): file . write ( PL_INFO_HEADLINE ) file . write ( self . pipeline_info_text ) def _write_pipeline_structure ( self , file ): file . write ( PL_STRUCTURE_HEADLINE ) file . write ( self . pipeline_structure_text ) def _write_dependencies ( self , file ): for component_name in self . component_all_dependencies . keys (): direct_dependencies = self . component_direct_dependencies . get ( component_name , set ()) transitive_dependencies = self . component_transitive_dependencies . get ( component_name , set ()) file . write ( PL_COMPONMENT_DEPENDENCIES_HEADLINE . format ( component_name = component_name )) file . write ( PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE ) sorted_direct_dependencies = sorted ( list ( direct_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_direct_dependencies : file . write ( PL_COMPONENT_DIRECT_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE ) sorted_transitive_dependencies = sorted ( list ( transitive_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_transitive_dependencies : file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) def _get_components_who_have_given_package ( self , package_name , package_version ): components = [] for component in self . component_all_dependencies : dependencies = self . component_all_dependencies [ component ] if ( package_name , package_version ) in dependencies : components . append ( component ) return components def _write_package_vulnerabilities ( self , file ): file . write ( PL_PACKAGE_VULNERABILITIES_HEADLINE ) sorted_vulnerability_dict_items = sorted ( self . vulnerability_dict . items (), key = lambda x : x [ 0 ][ 0 ]) for ( package_name , package_version ), vulnerabilities in sorted_vulnerability_dict_items : components = ', ' . join ( self . _get_components_who_have_given_package ( package_name , package_version )) if vulnerabilities is None : file . write ( PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED . format ( package_name = package_name , package_version = package_version , components = components )) elif vulnerabilities == []: file . write ( PL_PACKAGE_VULNERABILITY_NOT_KNOWN . format ( package_name = package_name , package_version = package_version , components = components )) else : for vulnerability in vulnerabilities : vulnerability_aliases = vulnerability . get ( 'aliases' , 'Vulnerability found with no alias. Check [PyPI repository](https://pypi.org/) for more details.' ) vulnerability_link = vulnerability . get ( 'link' , 'No link found' ) if vulnerability_link != 'No link found' : vulnerability_link = f \"[ { vulnerability_link } ]( { vulnerability_link } )\" vulnerability_details = vulnerability . get ( 'details' , 'No details found' ) vulnerability_fixed_in = vulnerability . get ( 'fixed_in' , '' ) file . write ( PL_PACKAGE_VULNERABILITY . format ( package_name = package_name , package_version = package_version , vulnerability_aliases = vulnerability_aliases , vulnerability_link = vulnerability_link , vulnerability_details = vulnerability_details , vulnerability_fixed_in = vulnerability_fixed_in , components = components )) file . write ( \" \\n \" ) LPLR_REPORT_HEADLINE = \"# Report on Local Pipeline Runner \\n\\n \" LPLR_FOLDER_STRUCTURE_HEADLINE = \"\"\"## Folder structure File sizes represent uncompressed sizes. \"\"\" LPLR_FOLDER_STRUCTURE = \"\"\"``` {file_name} {folder_structure} ``` \"\"\" LPLR_FOLDER_STRUCTURE_FOLDER_LINE = \" {prefix}{connector}{folder} \\n \" LPLR_FOLDER_STRUCTURE_FILE_LINE = \" {prefix}{connector}{file} ( {size} ) \\n \" LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL = \"\u251c\u2500\u2500 \" LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL = \"\u2514\u2500\u2500 \" LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL = \"\u2502 \" LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL = \" \" LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE = \"## PythonPackages.zip content \\n\\n \" LPLR_PYTHON_PACKAGES_ZIP_CONTENT = \"- {python_package} \\n \" LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE = \"\"\"## Installed packages Installed packages are listed in the `first report`, when the virtual environment is created for the Local Pipeline Runner. Subsequent reports will result in an empty list. \"\"\" LPLR_COMPONENT_INSTALLED_PACKAGES = \"\"\"### Component ` {component} ` | Package name | Package version | wheel name | |--------------|-----------------|------------| \"\"\" LPLR_COMPONENT_INSTALLED_PACKAGES_ROW = \"| {package_name} | {package_version} | {wheel_name} | \\n \" LPLR_PAYLOAD_LENGTHS_HEADLINE = \"## Payload counts \\n\\n \" LPLR_PAYLOAD_LENGTHS = \"\"\"### Component ` {component} ` - Input payload count: {input_payload_length} - Output payload count: {output_payload_length} \"\"\" class PipelineRunnerReportWriter ( ReportWriter ): \"\"\" PipelineRunnerReportWriter is responsible for generating a detailed report of a local pipeline execution. It builds folder structures from zip files, manages component payload counts, and adds installed packages information. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_package_zip_path(zip_path: Path): Sets the path to the package zip file and updates the folder tree. set_input_payload_length(component_name: str, length: int): Sets the input payload length for a component. set_output_payload_length(component_name: str, length: int): Sets the output payload length for a component. add_installed_packages(component_name: str, pip_report_file: Path): Adds installed packages for a component from a pip report file. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. \"\"\" def __init__ ( self ): super () . __init__ () self . package_zip_path = None self . zip_file_name = \"\" self . component_installed_packages = {} # component_name -> list[tuple(package_name, package_version, whl_name)] self . component_payload_length = {} # component_name -> [input_payload_length, output_payload_length] self . python_packages_zip_content = set () # report text sections to fill self . folder_tree_text = \"\" self . warnings_text = \"\" def set_package_zip_path ( self , zip_path : Path ): self . package_zip_path = zip_path with zipfile . ZipFile ( zip_path , 'r' ) as zipf : self . zip_file_name = zipf . filename zip_tree = {} for item_name in zipf . namelist (): zip_tree [ item_name ] = ZipTreeElement ( full_name = item_name , file_size = zipf . getinfo ( item_name ) . file_size ) self . _print_structure_recursively ( zip_tree , zipf ) @staticmethod def _get_folder_and_file_list ( item_names : list ) -> tuple [ list , list ]: \"\"\" Given a list of item names, each item name is a file that either starts with a folder name, or not. This function separates the folder names and the standalone file names. E.g., [\"a/b/something.txt\", \"c/another.txt\", \"else.txt\"] -> [\"a/\", \"c/\"], [\"else.txt\"] \"\"\" folder_names = set () file_names = [] for item in item_names : item_parts = item . split ( '/' ) if len ( item_parts ) > 1 : if item_parts [ 0 ] != '' : folder_names . add ( item_parts [ 0 ] + '/' ) else : if item != '' : file_names . append ( item ) return sorted ( list ( folder_names )), sorted ( file_names ) @staticmethod def format_size ( size ): \"\"\"Format file size in human-readable form.\"\"\" for unit in [ 'B' , 'KB' , 'MB' ]: if size < 1000 : return f \" { size } { unit } \" size //= 1000 return f \" { size } GB\" def _print_structure_recursively ( self , zip_tree , zipf , prefix = \"\" ): folder_names , file_names = PipelineRunnerReportWriter . _get_folder_and_file_list ( zip_tree . keys ()) is_file_names_empty = file_names == [] self . _print_folder_structure ( zip_tree , zipf , prefix , folder_names , is_file_names_empty ) self . _print_file_structure ( zip_tree , zipf , prefix , file_names ) def _print_folder_structure ( self , zip_tree , zipf , prefix , folder_names , is_file_names_empty ): for i , folder in enumerate ( folder_names ): is_last = ( i == len ( folder_names ) - 1 ) and is_file_names_empty connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FOLDER_LINE . format ( prefix = prefix , connector = connector , folder = folder ) # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) new_zip_tree_from_folder = {} for k , v in zip_tree . items (): if k . startswith ( folder ): new_file_name = k . split ( '/' , 1 )[ 1 ] new_zip_tree_from_folder [ new_file_name ] = v prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_folder = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_folder , zipf , new_prefix_from_folder ) def _print_file_structure ( self , zip_tree , zipf , prefix , file_names ): for i , file_name in enumerate ( file_names ): is_last = ( i == len ( file_names ) - 1 ) size_str = PipelineRunnerReportWriter . format_size ( zip_tree [ file_name ] . file_size ) connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FILE_LINE . format ( prefix = prefix , connector = connector , file = file_name , size = size_str ) # zip files are handled similarly to folders: # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) if not file_name . endswith ( '.zip' ): continue full_name = zip_tree [ file_name ] . full_name with zipf . open ( full_name ) as nested_zip_file : nested_zip_data = io . BytesIO ( nested_zip_file . read ()) with zipfile . ZipFile ( nested_zip_data , 'r' ) as nested_zipf : if file_name == \"PythonPackages.zip\" : self . python_packages_zip_content . update ( sorted ( list ( nested_zipf . namelist ()))) # create a new tree where items start with the same folder name; but cut out the folder name new_zip_tree_from_zip = {} for nested_item_name in nested_zipf . namelist (): new_zip_tree_from_zip [ nested_item_name ] = ZipTreeElement ( nested_item_name , nested_zipf . getinfo ( nested_item_name ) . file_size ) prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_zip = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_zip , nested_zipf , new_prefix_from_zip ) def set_input_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 0 ] = length else : self . component_payload_length [ component_name ] = [ length , 0 ] def set_output_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 1 ] = length else : self . component_payload_length [ component_name ] = [ 0 , length ] def add_installed_packages ( self , component_name : str , pip_report_file : Path ): pip_report = {} with open ( pip_report_file , 'r' ) as file : pip_report = json . load ( file ) if component_name not in self . component_installed_packages : self . component_installed_packages [ component_name ] = [] installed_packages = pip_report . get ( \"install\" , []) for package in installed_packages : package_url = package . get ( \"download_info\" , {}) . get ( \"url\" , \"\" ) wheel_name = package_url . split ( \"/\" )[ - 1 ] if package_url . endswith ( \".whl\" ) else \"n/a\" metadata = package . get ( \"metadata\" , {}) package_name = metadata . get ( \"name\" , \"n/a\" ) package_version = metadata . get ( \"version\" , \"n/a\" ) self . component_installed_packages [ component_name ] . append (( package_name , package_version , wheel_name )) # check if one or more reports already exists; set the report path so a new report will have a new index def _set_path_from_zip_path ( self ): if self . package_zip_path is None : return workdir = self . package_zip_path . parent base_name = self . package_zip_path . stem report_files = list ( workdir . glob ( f \" { base_name } _execution_report_*.md\" )) max_index = 0 for report_file in report_files : try : index = int ( report_file . stem . split ( '_' )[ - 1 ]) if index > max_index : max_index = index except ValueError : continue self . set_path ( workdir / f \" { base_name } _execution_report_ { max_index + 1 } .md\" ) def write_report ( self ): # if the report path is not set, set it from the zip path self . _set_path_from_zip_path () if self . report_path is None : return with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_folder_structure ( file ) self . _write_python_packages_zip_content ( file ) self . _write_component_installed_packages ( file ) self . _write_payload_lengths ( file ) self . _write_warnings ( file ) def _write_headline ( self , file ): file . write ( LPLR_REPORT_HEADLINE ) def _write_folder_structure ( self , file ): file . write ( LPLR_FOLDER_STRUCTURE_HEADLINE . format ( file_name = self . zip_file_name )) file . write ( LPLR_FOLDER_STRUCTURE . format ( file_name = self . zip_file_name , folder_structure = self . folder_tree_text )) def _write_python_packages_zip_content ( self , file ): file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE ) sorted_zip_content = sorted ( list ( self . python_packages_zip_content )) for package in sorted_zip_content : file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT . format ( python_package = package )) file . write ( \" \\n \" ) def _write_component_installed_packages ( self , file ): file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE ) for component in self . component_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES . format ( component = component )) sorted_installed_packages = sorted ( self . component_installed_packages [ component ], key = lambda x : x [ 0 ]) for package_name , package_version , wheel_name in sorted_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_ROW . format ( package_name = package_name , package_version = package_version , wheel_name = wheel_name )) file . write ( \" \\n \" ) def _write_payload_lengths ( self , file ): file . write ( LPLR_PAYLOAD_LENGTHS_HEADLINE ) for component in self . component_payload_length : input_payload_length , output_payload_length = self . component_payload_length [ component ] file . write ( LPLR_PAYLOAD_LENGTHS . format ( component = component , input_payload_length = input_payload_length , output_payload_length = output_payload_length ))","title":"Module simaticai.helpers.reporter"},{"location":"reference/simaticai/helpers/reporter.html#variables","text":"LPLR_COMPONENT_INSTALLED_PACKAGES LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE LPLR_COMPONENT_INSTALLED_PACKAGES_ROW LPLR_FOLDER_STRUCTURE LPLR_FOLDER_STRUCTURE_FILE_LINE LPLR_FOLDER_STRUCTURE_FOLDER_LINE LPLR_FOLDER_STRUCTURE_HEADLINE LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL LPLR_PAYLOAD_LENGTHS LPLR_PAYLOAD_LENGTHS_HEADLINE LPLR_PYTHON_PACKAGES_ZIP_CONTENT LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE LPLR_REPORT_HEADLINE PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE PL_COMPONENT_DIRECT_DEPENDENCY PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE PL_COMPONENT_TRANSITIVE_DEPENDENCY PL_COMPONMENT_DEPENDENCIES_HEADLINE PL_INFO PL_INFO_HEADLINE PL_PACKAGE_VULNERABILITIES_HEADLINE PL_PACKAGE_VULNERABILITY PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED PL_PACKAGE_VULNERABILITY_NOT_KNOWN PL_REPORT_HEADLINE PL_STRUCTURE PL_STRUCTURE_HEADLINE WARNINGS_HEADLINE WARNING_LINE","title":"Variables"},{"location":"reference/simaticai/helpers/reporter.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/helpers/reporter.html#pipelinereportwriter","text":"A class to generate a report for a dataflow pipeline, including pipeline structure, component dependencies, and package vulnerabilities. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_pipeline_config(pipeline_config: dict): Sets the pipeline configuration and updates the pipeline info and structure. add_full_dependency_set(component_name: str, dependency_set: set[tuple]): Adds a full set of dependencies for a component and updates the vulnerability dictionary. add_direct_dependencies(component_name: str, direct_dependencies: dict): Adds direct dependencies for a component. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. class PipelineReportWriter ( ) View Source class PipelineReportWriter ( ReportWriter ) : \"\"\" A class to generate a report for a dataflow pipeline , including pipeline structure , component dependencies , and package vulnerabilities . Methods : set_path ( report_path : Path ) : Sets the path where the report will be saved . set_pipeline_config ( pipeline_config : dict ) : Sets the pipeline configuration and updates the pipeline info and structure . add_full_dependency_set ( component_name : str , dependency_set : set [ tuple ]) : Adds a full set of dependencies for a component and updates the vulnerability dictionary . add_direct_dependencies ( component_name : str , direct_dependencies : dict ) : Adds direct dependencies for a component . add_warning ( name , filename , line_number , warning_msg ) : Adds a warning to the report . write_report () : Writes the report to the specified path . \"\"\" def __init__ ( self ) : super (). __init__ () self . pipeline_config = {} # pipeline config json self . pipeline_name = \"Unnamed pipeline\" # dependency_names and package_names are transformed dependency names (lowercase, underscore instead of dash) # set from outside self . component_direct_dependency_namelist = {} # component -> list of dependency_names ( with NO version ) self . component_all_dependencies = {} # component -> set of tuples of ( dependency_name , dependency_version ) # collected before writing self . component_direct_dependencies = {} # component -> set of dependency_names self . component_transitive_dependencies = {} # component -> set of tuples of ( dependency_name , dependency_version ) self . vulnerability_dict = {} # ( package_name , package_version ) -> vulnerabilities ( None | list of dictionaries ) # report text sections to fill self . pipeline_structure_text = \"\" self . pipeline_info_text = \"\" self . warnings_text = \"\" def set_pipeline_config ( self , pipeline_config : dict ) : self . pipeline_config = pipeline_config self . _set_pipeline_info () self . _set_pipeline_structure () @ staticmethod def _sort_pipeline_dag ( pipeline_dag : list ) -> list : \"\"\" Sorts a pipeline DAG in order to show dataflow from Pipeline Inputs to Pipeline Outputs . Databus component is a privileged source , and it is always the first component in the report . Args : pipeline_dag ( list ) : The pipeline DAG is a list of dictionaries with \"source\" and \"target\" keys . Returns : A sorted list of dictionaries representing the pipeline DAG . \"\"\" pipeline_dag . sort ( key = lambda x : ( x [ \"source\" ], x [ \"target\" ])) sorted_dag = [ edge for edge in pipeline_dag if \"Databus\" in edge [ ' source ' ]] if sorted_dag == [] : return pipeline_dag pipeline_dag = [ edge for edge in pipeline_dag if \"Databus\" not in edge [ ' source ' ]] # Extracts name of the target or source component from the edge name_of_component = lambda edge , target_or_source : edge [ target_or_source ]. rsplit ( \".\" , 1 )[ 0 ] while len ( pipeline_dag ) > 0 : sorted_targets = [ name_of_component ( edge , \"target\" ) for edge in sorted_dag ] sorted_dag . extend ([ edge for edge in pipeline_dag if name_of_component ( edge , \"source\" ) in sorted_targets ]) pipeline_dag = [ edge for edge in pipeline_dag if edge not in sorted_dag ] return sorted_dag def _set_pipeline_structure ( self ) : self . pipeline_name = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}). get ( \"projectName\" , \"n/a\" ) self . pipeline_structure_text = \"```plantuml \\n \" components = self . pipeline_config . get ( \"dataFlowPipeline\" , {}). get ( \"components\" , []) variables = {} # name : ( type , is_metric ) for component in components : input_variables = { _input [ \"name\" ] : ( _input [ \"type\" ], False ) for _input in component . get ( \"inputType\" )} output_variables = { _output [ \"name\" ] : ( _output [ \"type\" ], _output . get ( \"metric\" , False )) for _output in component . get ( \"outputType\" )} variables . update ({ ** input_variables , ** output_variables }) pipeline_dag = self . pipeline_config . get ( \"dataFlowPipeline\" , {}). get ( \"pipelineDag\" , []) sorted_pipeline_dag = PipelineReportWriter . _sort_pipeline_dag ( pipeline_dag ) for transition in sorted_pipeline_dag : source_component_name , source_variable_name = transition [ \"source\" ]. rsplit ( \".\" , 1 ) target_component_name , target_variable_name = transition [ \"target\" ]. rsplit ( \".\" , 1 ) variable_name_to_show = source_variable_name if source_variable_name == target_variable_name else f \"{source_variable_name} -> {target_variable_name}\" source_component_name = source_component_name . replace ( \"Databus\" , \"AIIS\" ) target_component_name = target_component_name . replace ( \"Databus\" , \"AIIS\" ) variable_type , is_metric = variables [ source_variable_name ] arrow = \"-->\" if is_metric else \"->\" # metric variables are drawn with a dashed line self . pipeline_structure_text += PL_STRUCTURE . format ( source_component = source_component_name , arrow = arrow , target_component = target_component_name , variable_name = variable_name_to_show , variable_type = variable_type ) self . pipeline_structure_text += \"``` \\n\\n \" def _set_pipeline_info ( self ) : dataflow_pipeline_info = self . pipeline_config . get ( \"dataFlowPipelineInfo\" , {}) author = dataflow_pipeline_info . get ( \"author\" , \"n/a\" ) created_on = dataflow_pipeline_info . get ( \"createdOn\" , \"n/a\" ) pipeline_version = dataflow_pipeline_info . get ( \"dataFlowPipelineVersion\" , \"n/a\" ) description = dataflow_pipeline_info . get ( \"description\" , \"n/a\" ) package_id = dataflow_pipeline_info . get ( \"packageId\" , \"n/a\" ) project_name = dataflow_pipeline_info . get ( \"projectName\" , \"n/a\" ) self . pipeline_info_text = PL_INFO . format ( author = author , created_on = created_on , pipeline_version = pipeline_version , description = description , package_id = package_id , project_name = project_name ) # Transform every dependency and package name for consistency; i.e., # opencv-python-headless -> opencv_python_headless; Django -> django @ staticmethod def transform_package_name ( name : str ) : new_name = name . replace ( \"-\" , \"_\" ) return new_name . lower () # A full dependency set is a set of (package_name, package_version) tuples # and contains all the dependencies installed for a component def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ]) : dependency_list = sorted ( list ( dependency_set ), key = lambda x : x [ 0 ]) self . _expand_component_all_dependencies ( component_name , dependency_list ) self . _update_vulnerability_dict ( dependency_list ) def _expand_component_all_dependencies ( self , component_name : str , dependency_list : list [ tuple ]) : if component_name not in self . component_all_dependencies : self . component_all_dependencies [ component_name ] = set () for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) self . component_all_dependencies [ component_name ]. add (( transformed_package_name , package_version )) def _update_vulnerability_dict ( self , dependency_list : list [ tuple ]) : vulnerability_dict = {} for package_name , package_version in dependency_list : transformed_package_name = PipelineReportWriter . transform_package_name ( package_name ) vulnerability_dict [( transformed_package_name , package_version )] = None url = f \"https://pypi.org/pypi/{package_name}/{package_version}/json\" try : response = requests . get ( url , timeout = 5 ) if response . status_code == 200 : data = response . json () if ' vulnerabilities ' in data : vulnerability_dict [( transformed_package_name , package_version )] = data [ ' vulnerabilities ' ] except requests . exceptions . Timeout : pass self . vulnerability_dict . update ( vulnerability_dict ) def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ) : self . component_direct_dependency_namelist [ component_name ] = [ PipelineReportWriter . transform_package_name ( name ) for name in list ( direct_dependencies . keys ())] def write_report ( self ) : if self . report_path is None : return self . _set_component_dependencies () with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_pipeline_info ( file ) self . _write_pipeline_structure ( file ) self . _write_dependencies ( file ) self . _write_package_vulnerabilities ( file ) self . _write_warnings ( file ) def _set_component_dependencies ( self ) : for component in self . component_all_dependencies . keys () : # self.component_direct_dependencies should contain everything from self.component_all_dependencies # if it is direct, i.e., the name is in self.component_direct_dependency_namelist # self.component_transitive_dependencies should contain everything else self . component_transitive_dependencies [ component ] = set () self . component_direct_dependencies [ component ] = set () all_dependencies = self . component_all_dependencies [ component ] for dependency_name , dependency_version in all_dependencies : if dependency_name in self . component_direct_dependency_namelist . get ( component , []) : self . component_direct_dependencies [ component ]. add (( dependency_name , dependency_version )) else : self . component_transitive_dependencies [ component ]. add (( dependency_name , dependency_version )) def _write_headline ( self , file ) : file . write ( PL_REPORT_HEADLINE . format ( pipeline_name = self . pipeline_name )) def _write_pipeline_info ( self , file ) : file . write ( PL_INFO_HEADLINE ) file . write ( self . pipeline_info_text ) def _write_pipeline_structure ( self , file ) : file . write ( PL_STRUCTURE_HEADLINE ) file . write ( self . pipeline_structure_text ) def _write_dependencies ( self , file ) : for component_name in self . component_all_dependencies . keys () : direct_dependencies = self . component_direct_dependencies . get ( component_name , set ()) transitive_dependencies = self . component_transitive_dependencies . get ( component_name , set ()) file . write ( PL_COMPONMENT_DEPENDENCIES_HEADLINE . format ( component_name = component_name )) file . write ( PL_COMPONENT_DIRECT_DEPENDENCIES_HEADLINE ) sorted_direct_dependencies = sorted ( list ( direct_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_direct_dependencies : file . write ( PL_COMPONENT_DIRECT_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCIES_HEADLINE ) sorted_transitive_dependencies = sorted ( list ( transitive_dependencies ), key = lambda x : x [ 0 ]) for dependency_name , dependency_version in sorted_transitive_dependencies : file . write ( PL_COMPONENT_TRANSITIVE_DEPENDENCY . format ( dependency_name = dependency_name , dependency_version = dependency_version )) file . write ( \" \\n \" ) def _get_components_who_have_given_package ( self , package_name , package_version ) : components = [] for component in self . component_all_dependencies : dependencies = self . component_all_dependencies [ component ] if ( package_name , package_version ) in dependencies : components . append ( component ) return components def _write_package_vulnerabilities ( self , file ) : file . write ( PL_PACKAGE_VULNERABILITIES_HEADLINE ) sorted_vulnerability_dict_items = sorted ( self . vulnerability_dict . items (), key = lambda x : x [ 0 ][ 0 ]) for ( package_name , package_version ), vulnerabilities in sorted_vulnerability_dict_items : components = ' , ' . join ( self . _get_components_who_have_given_package ( package_name , package_version )) if vulnerabilities is None : file . write ( PL_PACKAGE_VULNERABILITY_CANNOT_BE_CHECKED . format ( package_name = package_name , package_version = package_version , components = components )) elif vulnerabilities == [] : file . write ( PL_PACKAGE_VULNERABILITY_NOT_KNOWN . format ( package_name = package_name , package_version = package_version , components = components )) else : for vulnerability in vulnerabilities : vulnerability_aliases = vulnerability . get ( ' aliases ' , ' Vulnerability found with no alias . Check [ PyPI repository ]( https : //pypi.org/) for more details.') vulnerability_link = vulnerability . get ( ' link ' , ' No link found ' ) if vulnerability_link != ' No link found ' : vulnerability_link = f \"[{vulnerability_link}]({vulnerability_link})\" vulnerability_details = vulnerability . get ( ' details ' , ' No details found ' ) vulnerability_fixed_in = vulnerability . get ( ' fixed_in ' , '' ) file . write ( PL_PACKAGE_VULNERABILITY . format ( package_name = package_name , package_version = package_version , vulnerability_aliases = vulnerability_aliases , vulnerability_link = vulnerability_link , vulnerability_details = vulnerability_details , vulnerability_fixed_in = vulnerability_fixed_in , components = components )) file . write ( \" \\n \" )","title":"PipelineReportWriter"},{"location":"reference/simaticai/helpers/reporter.html#ancestors-in-mro","text":"simaticai.helpers.reporter.ReportWriter","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/helpers/reporter.html#static-methods","text":"","title":"Static methods"},{"location":"reference/simaticai/helpers/reporter.html#transform_package_name","text":"def transform_package_name ( name : str ) View Source @ staticmethod def transform_package_name ( name : str ): new_name = name . replace ( \"-\" , \"_\" ) return new_name . lower ()","title":"transform_package_name"},{"location":"reference/simaticai/helpers/reporter.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/helpers/reporter.html#add_direct_dependencies","text":"def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ) View Source def add_direct_dependencies ( self , component_name : str , direct_dependencies : dict ) : self . component_direct_dependency_namelist [ component_name ] = [ PipelineReportWriter.transform_package_name(name) for name in list(direct_dependencies.keys()) ]","title":"add_direct_dependencies"},{"location":"reference/simaticai/helpers/reporter.html#add_full_dependency_set","text":"def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ] ) View Source def add_full_dependency_set ( self , component_name : str , dependency_set : set [ tuple ] ) : dependency_list = sorted ( list ( dependency_set ), key = lambda x : x [ 0 ] ) self . _expand_component_all_dependencies ( component_name , dependency_list ) self . _update_vulnerability_dict ( dependency_list )","title":"add_full_dependency_set"},{"location":"reference/simaticai/helpers/reporter.html#add_warning","text":"def add_warning ( self , name , filename , line_number , warning_msg ) View Source def add_warning(self, name, filename, line_number, warning_msg): self.warnings_text += WARNING_LINE.format(name=name, filename=filename, line_number=line_number, warning_msg=warning_msg)","title":"add_warning"},{"location":"reference/simaticai/helpers/reporter.html#set_path","text":"def set_path ( self , report_path : pathlib . Path ) View Source def set_path(self, report_path: Path): self.report_path = report_path","title":"set_path"},{"location":"reference/simaticai/helpers/reporter.html#set_pipeline_config","text":"def set_pipeline_config ( self , pipeline_config : dict ) View Source def set_pipeline_config(self, pipeline_config: dict): self.pipeline_config = pipeline_config self._set_pipeline_info() self._set_pipeline_structure()","title":"set_pipeline_config"},{"location":"reference/simaticai/helpers/reporter.html#write_report","text":"def write_report ( self ) View Source def write_report ( self ): if self . report_path is None : return self . _set_component_dependencies () with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_pipeline_info ( file ) self . _write_pipeline_structure ( file ) self . _write_dependencies ( file ) self . _write_package_vulnerabilities ( file ) self . _write_warnings ( file )","title":"write_report"},{"location":"reference/simaticai/helpers/reporter.html#pipelinerunnerreportwriter","text":"PipelineRunnerReportWriter is responsible for generating a detailed report of a local pipeline execution. It builds folder structures from zip files, manages component payload counts, and adds installed packages information. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_package_zip_path(zip_path: Path): Sets the path to the package zip file and updates the folder tree. set_input_payload_length(component_name: str, length: int): Sets the input payload length for a component. set_output_payload_length(component_name: str, length: int): Sets the output payload length for a component. add_installed_packages(component_name: str, pip_report_file: Path): Adds installed packages for a component from a pip report file. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. class PipelineRunnerReportWriter ( ) View Source class PipelineRunnerReportWriter ( ReportWriter ): \"\"\" PipelineRunnerReportWriter is responsible for generating a detailed report of a local pipeline execution. It builds folder structures from zip files, manages component payload counts, and adds installed packages information. Methods: set_path(report_path: Path): Sets the path where the report will be saved. set_package_zip_path(zip_path: Path): Sets the path to the package zip file and updates the folder tree. set_input_payload_length(component_name: str, length: int): Sets the input payload length for a component. set_output_payload_length(component_name: str, length: int): Sets the output payload length for a component. add_installed_packages(component_name: str, pip_report_file: Path): Adds installed packages for a component from a pip report file. add_warning(name, filename, line_number, warning_msg): Adds a warning to the report. write_report(): Writes the report to the specified path. \"\"\" def __init__ ( self ): super () . __init__ () self . package_zip_path = None self . zip_file_name = \"\" self . component_installed_packages = {} # component_name -> list[tuple(package_name, package_version, whl_name)] self . component_payload_length = {} # component_name -> [input_payload_length, output_payload_length] self . python_packages_zip_content = set () # report text sections to fill self . folder_tree_text = \"\" self . warnings_text = \"\" def set_package_zip_path ( self , zip_path : Path ): self . package_zip_path = zip_path with zipfile . ZipFile ( zip_path , 'r' ) as zipf : self . zip_file_name = zipf . filename zip_tree = {} for item_name in zipf . namelist (): zip_tree [ item_name ] = ZipTreeElement ( full_name = item_name , file_size = zipf . getinfo ( item_name ) . file_size ) self . _print_structure_recursively ( zip_tree , zipf ) @ staticmethod def _get_folder_and_file_list ( item_names : list ) -> tuple [ list , list ]: \"\"\" Given a list of item names, each item name is a file that either starts with a folder name, or not. This function separates the folder names and the standalone file names. E.g., [\"a/b/something.txt\", \"c/another.txt\", \"else.txt\"] -> [\"a/\", \"c/\"], [\"else.txt\"] \"\"\" folder_names = set () file_names = [] for item in item_names : item_parts = item . split ( '/' ) if len ( item_parts ) > 1 : if item_parts [ 0 ] != '' : folder_names . add ( item_parts [ 0 ] + '/' ) else : if item != '' : file_names . append ( item ) return sorted ( list ( folder_names )), sorted ( file_names ) @ staticmethod def format_size ( size ): \"\"\"Format file size in human-readable form.\"\"\" for unit in [ 'B' , 'KB' , 'MB' ]: if size < 1000 : return f \"{size} {unit}\" size //= 1000 return f \"{size} GB\" def _print_structure_recursively ( self , zip_tree , zipf , prefix = \"\" ): folder_names , file_names = PipelineRunnerReportWriter . _get_folder_and_file_list ( zip_tree . keys ()) is_file_names_empty = file_names == [] self . _print_folder_structure ( zip_tree , zipf , prefix , folder_names , is_file_names_empty ) self . _print_file_structure ( zip_tree , zipf , prefix , file_names ) def _print_folder_structure ( self , zip_tree , zipf , prefix , folder_names , is_file_names_empty ): for i , folder in enumerate ( folder_names ): is_last = ( i == len ( folder_names ) - 1 ) and is_file_names_empty connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FOLDER_LINE . format ( prefix = prefix , connector = connector , folder = folder ) # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) new_zip_tree_from_folder = {} for k , v in zip_tree . items (): if k . startswith ( folder ): new_file_name = k . split ( '/' , 1 )[ 1 ] new_zip_tree_from_folder [ new_file_name ] = v prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_folder = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_folder , zipf , new_prefix_from_folder ) def _print_file_structure ( self , zip_tree , zipf , prefix , file_names ): for i , file_name in enumerate ( file_names ): is_last = ( i == len ( file_names ) - 1 ) size_str = PipelineRunnerReportWriter . format_size ( zip_tree [ file_name ] . file_size ) connector = LPLR_FOLDER_STRUCTURE_LAST_CONNECTOR_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_CONNECTOR_SYMBOL self . folder_tree_text += LPLR_FOLDER_STRUCTURE_FILE_LINE . format ( prefix = prefix , connector = connector , file = file_name , size = size_str ) # zip files are handled similarly to folders: # create a new tree where items start with the same folder name; but cut out the folder name # (going deeper in the recursion) if not file_name . endswith ( '.zip' ): continue full_name = zip_tree [ file_name ] . full_name with zipf . open ( full_name ) as nested_zip_file : nested_zip_data = io . BytesIO ( nested_zip_file . read ()) with zipfile . ZipFile ( nested_zip_data , 'r' ) as nested_zipf : if file_name == \"PythonPackages.zip\" : self . python_packages_zip_content . update ( sorted ( list ( nested_zipf . namelist ()))) # create a new tree where items start with the same folder name; but cut out the folder name new_zip_tree_from_zip = {} for nested_item_name in nested_zipf . namelist (): new_zip_tree_from_zip [ nested_item_name ] = ZipTreeElement ( nested_item_name , nested_zipf . getinfo ( nested_item_name ) . file_size ) prefix_post = LPLR_FOLDER_STRUCTURE_LAST_PREFIX_SYMBOL if is_last else LPLR_FOLDER_STRUCTURE_MID_PREFIX_SYMBOL new_prefix_from_zip = prefix + prefix_post self . _print_structure_recursively ( new_zip_tree_from_zip , nested_zipf , new_prefix_from_zip ) def set_input_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 0 ] = length else : self . component_payload_length [ component_name ] = [ length , 0 ] def set_output_payload_length ( self , component_name : str , length : int ): if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 1 ] = length else : self . component_payload_length [ component_name ] = [ 0 , length ] def add_installed_packages ( self , component_name : str , pip_report_file : Path ): pip_report = {} with open ( pip_report_file , 'r' ) as file : pip_report = json . load ( file ) if component_name not in self . component_installed_packages : self . component_installed_packages [ component_name ] = [] installed_packages = pip_report . get ( \"install\" , []) for package in installed_packages : package_url = package . get ( \"download_info\" , {}) . get ( \"url\" , \"\" ) wheel_name = package_url . split ( \"/\" )[ - 1 ] if package_url . endswith ( \".whl\" ) else \"n/a\" metadata = package . get ( \"metadata\" , {}) package_name = metadata . get ( \"name\" , \"n/a\" ) package_version = metadata . get ( \"version\" , \"n/a\" ) self . component_installed_packages [ component_name ] . append (( package_name , package_version , wheel_name )) # check if one or more reports already exists; set the report path so a new report will have a new index def _set_path_from_zip_path ( self ): if self . package_zip_path is None : return workdir = self . package_zip_path . parent base_name = self . package_zip_path . stem report_files = list ( workdir . glob ( f \"{base_name}_execution_report_*.md\" )) max_index = 0 for report_file in report_files : try : index = int ( report_file . stem . split ( '_' )[ - 1 ]) if index > max_index : max_index = index except ValueError : continue self . set_path ( workdir / f \"{base_name}_execution_report_{max_index + 1}.md\" ) def write_report ( self ): # if the report path is not set, set it from the zip path self . _set_path_from_zip_path () if self . report_path is None : return with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_folder_structure ( file ) self . _write_python_packages_zip_content ( file ) self . _write_component_installed_packages ( file ) self . _write_payload_lengths ( file ) self . _write_warnings ( file ) def _write_headline ( self , file ): file . write ( LPLR_REPORT_HEADLINE ) def _write_folder_structure ( self , file ): file . write ( LPLR_FOLDER_STRUCTURE_HEADLINE . format ( file_name = self . zip_file_name )) file . write ( LPLR_FOLDER_STRUCTURE . format ( file_name = self . zip_file_name , folder_structure = self . folder_tree_text )) def _write_python_packages_zip_content ( self , file ): file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT_HEADLINE ) sorted_zip_content = sorted ( list ( self . python_packages_zip_content )) for package in sorted_zip_content : file . write ( LPLR_PYTHON_PACKAGES_ZIP_CONTENT . format ( python_package = package )) file . write ( \" \\n \" ) def _write_component_installed_packages ( self , file ): file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_HEADLINE ) for component in self . component_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES . format ( component = component )) sorted_installed_packages = sorted ( self . component_installed_packages [ component ], key = lambda x : x [ 0 ]) for package_name , package_version , wheel_name in sorted_installed_packages : file . write ( LPLR_COMPONENT_INSTALLED_PACKAGES_ROW . format ( package_name = package_name , package_version = package_version , wheel_name = wheel_name )) file . write ( \" \\n \" ) def _write_payload_lengths ( self , file ): file . write ( LPLR_PAYLOAD_LENGTHS_HEADLINE ) for component in self . component_payload_length : input_payload_length , output_payload_length = self . component_payload_length [ component ] file . write ( LPLR_PAYLOAD_LENGTHS . format ( component = component , input_payload_length = input_payload_length , output_payload_length = output_payload_length ))","title":"PipelineRunnerReportWriter"},{"location":"reference/simaticai/helpers/reporter.html#ancestors-in-mro_1","text":"simaticai.helpers.reporter.ReportWriter","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/helpers/reporter.html#static-methods_1","text":"","title":"Static methods"},{"location":"reference/simaticai/helpers/reporter.html#format_size","text":"def format_size ( size ) Format file size in human-readable form. View Source @staticmethod def format_size ( size ) : \"\"\"Format file size in human-readable form.\"\"\" for unit in [ 'B', 'KB', 'MB' ] : if size < 1000 : return f \"{size} {unit}\" size //= 1000 return f \"{size} GB\"","title":"format_size"},{"location":"reference/simaticai/helpers/reporter.html#methods_1","text":"","title":"Methods"},{"location":"reference/simaticai/helpers/reporter.html#add_installed_packages","text":"def add_installed_packages ( self , component_name : str , pip_report_file : pathlib . Path ) View Source def add_installed_packages ( self , component_name : str , pip_report_file : Path ) : pip_report = {} with open ( pip_report_file , 'r' ) as file : pip_report = json . load ( file ) if component_name not in self . component_installed_packages : self . component_installed_packages [ component_name ] = [] installed_packages = pip_report . get ( \"install\" , [] ) for package in installed_packages : package_url = package . get ( \"download_info\" , {} ). get ( \"url\" , \"\" ) wheel_name = package_url . split ( \"/\" ) [ -1 ] if package_url . endswith ( \".whl\" ) else \"n/a\" metadata = package . get ( \"metadata\" , {} ) package_name = metadata . get ( \"name\" , \"n/a\" ) package_version = metadata . get ( \"version\" , \"n/a\" ) self . component_installed_packages [ component_name ] . append (( package_name , package_version , wheel_name ))","title":"add_installed_packages"},{"location":"reference/simaticai/helpers/reporter.html#add_warning_1","text":"def add_warning ( self , name , filename , line_number , warning_msg ) View Source def add_warning(self, name, filename, line_number, warning_msg): self.warnings_text += WARNING_LINE.format(name=name, filename=filename, line_number=line_number, warning_msg=warning_msg)","title":"add_warning"},{"location":"reference/simaticai/helpers/reporter.html#set_input_payload_length","text":"def set_input_payload_length ( self , component_name : str , length : int ) View Source def set_input_payload_length ( self , component_name : str , length : int ) : if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 0 ] = length else : self . component_payload_length [ component_name ] = [ length, 0 ]","title":"set_input_payload_length"},{"location":"reference/simaticai/helpers/reporter.html#set_output_payload_length","text":"def set_output_payload_length ( self , component_name : str , length : int ) View Source def set_output_payload_length ( self , component_name : str , length : int ) : if component_name in self . component_payload_length : self . component_payload_length [ component_name ][ 1 ] = length else : self . component_payload_length [ component_name ] = [ 0, length ]","title":"set_output_payload_length"},{"location":"reference/simaticai/helpers/reporter.html#set_package_zip_path","text":"def set_package_zip_path ( self , zip_path : pathlib . Path ) View Source def set_package_zip_path ( self , zip_path : Path ) : self . package_zip_path = zip_path with zipfile . ZipFile ( zip_path , 'r' ) as zipf : self . zip_file_name = zipf . filename zip_tree = {} for item_name in zipf . namelist () : zip_tree [ item_name ] = ZipTreeElement ( full_name = item_name , file_size = zipf . getinfo ( item_name ). file_size ) self . _print_structure_recursively ( zip_tree , zipf )","title":"set_package_zip_path"},{"location":"reference/simaticai/helpers/reporter.html#set_path_1","text":"def set_path ( self , report_path : pathlib . Path ) View Source def set_path(self, report_path: Path): self.report_path = report_path","title":"set_path"},{"location":"reference/simaticai/helpers/reporter.html#write_report_1","text":"def write_report ( self ) View Source def write_report ( self ): # if the report path is not set, set it from the zip path self . _set_path_from_zip_path () if self . report_path is None : return with open ( self . report_path , \"w\" ) as file : self . _write_headline ( file ) self . _write_folder_structure ( file ) self . _write_python_packages_zip_content ( file ) self . _write_component_installed_packages ( file ) self . _write_payload_lengths ( file ) self . _write_warnings ( file )","title":"write_report"},{"location":"reference/simaticai/helpers/reporter.html#reportwriter","text":"Base class for report writers. class ReportWriter ( ) View Source class ReportWriter : \"\"\" Base class for report writers. \"\"\" def __init__ ( self ): self . report_path = None self . warnings_text = \"\" def set_path ( self , report_path : Path ): self . report_path = report_path def add_warning ( self , name , filename , line_number , warning_msg ): self . warnings_text += WARNING_LINE . format ( name = name , filename = filename , line_number = line_number , warning_msg = warning_msg ) def write_report ( self ): raise NotImplementedError ( \"Subclasses should implement this method\" ) def _write_warnings ( self , file ): file . write ( WARNINGS_HEADLINE ) file . write ( self . warnings_text )","title":"ReportWriter"},{"location":"reference/simaticai/helpers/reporter.html#descendants","text":"simaticai.helpers.reporter.PipelineReportWriter simaticai.helpers.reporter.PipelineRunnerReportWriter","title":"Descendants"},{"location":"reference/simaticai/helpers/reporter.html#methods_2","text":"","title":"Methods"},{"location":"reference/simaticai/helpers/reporter.html#add_warning_2","text":"def add_warning ( self , name , filename , line_number , warning_msg ) View Source def add_warning(self, name, filename, line_number, warning_msg): self.warnings_text += WARNING_LINE.format(name=name, filename=filename, line_number=line_number, warning_msg=warning_msg)","title":"add_warning"},{"location":"reference/simaticai/helpers/reporter.html#set_path_2","text":"def set_path ( self , report_path : pathlib . Path ) View Source def set_path(self, report_path: Path): self.report_path = report_path","title":"set_path"},{"location":"reference/simaticai/helpers/reporter.html#write_report_2","text":"def write_report ( self ) View Source def write_report ( self ): raise NotImplementedError ( \"Subclasses should implement this method\" )","title":"write_report"},{"location":"reference/simaticai/helpers/reporter.html#reportwriterhandler","text":"A handler that can be given to a logger, so the report writer can capture logged warning messages class ReportWriterHandler ( report_writer : simaticai . helpers . reporter . ReportWriter ) View Source class ReportWriterHandler ( logging . Handler ): \"\"\" A handler that can be given to a logger, so the report writer can capture logged warning messages \"\"\" def __init__ ( self , report_writer: ReportWriter ): super (). __init__ () self . report_writer = report_writer def emit ( self , record ): if record . levelno == logging . WARNING: self . report_writer . add_warning ( record . name , record . filename , record . lineno , record . getMessage ())","title":"ReportWriterHandler"},{"location":"reference/simaticai/helpers/reporter.html#ancestors-in-mro_2","text":"logging.Handler logging.Filterer","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/helpers/reporter.html#instance-variables","text":"name","title":"Instance variables"},{"location":"reference/simaticai/helpers/reporter.html#methods_3","text":"","title":"Methods"},{"location":"reference/simaticai/helpers/reporter.html#acquire","text":"def acquire ( self ) Acquire the I/O thread lock. View Source def acquire ( self ) : \"\" \" Acquire the I/O thread lock. \"\" \" if self.lock: self.lock.acquire()","title":"acquire"},{"location":"reference/simaticai/helpers/reporter.html#addfilter","text":"def addFilter ( self , filter ) Add the specified filter to this handler. View Source def addFilter ( self , filter ) : \"\" \" Add the specified filter to this handler. \"\" \" if not (filter in self.filters): self.filters.append(filter)","title":"addFilter"},{"location":"reference/simaticai/helpers/reporter.html#close","text":"def close ( self ) Tidy up any resources used by the handler. This version removes the handler from an internal map of handlers, _handlers, which is used for handler lookup by name. Subclasses should ensure that this gets called from overridden close() methods. View Source def close(self): \"\"\" Tidy up any resources used by the handler. This version removes the handler from an internal map of handlers, _handlers, which is used for handler lookup by name. Subclasses should ensure that this gets called from overridden close() methods. \"\"\" #get the module data lock, as we're updating a shared structure. _acquireLock() try: #unlikely to raise an exception, but you never know... self._closed = True if self._name and self._name in _handlers: del _handlers[self._name] finally: _releaseLock()","title":"close"},{"location":"reference/simaticai/helpers/reporter.html#createlock","text":"def createLock ( self ) Acquire a thread lock for serializing access to the underlying I/O. View Source def createLock ( self ) : \"\"\" Acquire a thread lock for serializing access to the underlying I / O . \"\"\" self . lock = threading . RLock () _register_at_fork_reinit_lock ( self )","title":"createLock"},{"location":"reference/simaticai/helpers/reporter.html#emit","text":"def emit ( self , record ) Do whatever it takes to actually log the specified logging record. This version is intended to be implemented by subclasses and so raises a NotImplementedError. View Source def emit(self, record): if record.levelno == logging.WARNING: self.report_writer.add_warning(record.name, record.filename, record.lineno, record.getMessage())","title":"emit"},{"location":"reference/simaticai/helpers/reporter.html#filter","text":"def filter ( self , record ) Determine if a record is loggable by consulting all the filters. The default is to allow the record to be logged; any filter can veto this and the record is then dropped. Returns a zero value if a record is to be dropped, else non-zero. .. versionchanged:: 3.2 Allow filters to be just callables. View Source def filter ( self , record ) : \"\" \" Determine if a record is loggable by consulting all the filters. The default is to allow the record to be logged; any filter can veto this and the record is then dropped. Returns a zero value if a record is to be dropped, else non-zero. .. versionchanged:: 3.2 Allow filters to be just callables. \"\" \" rv = True for f in self.filters: if hasattr(f, 'filter'): result = f.filter(record) else: result = f(record) # assume callable - will raise if not if not result: rv = False break return rv","title":"filter"},{"location":"reference/simaticai/helpers/reporter.html#flush","text":"def flush ( self ) Ensure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses. View Source def flush ( self ): \"\"\" Ensure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses. \"\"\" pass","title":"flush"},{"location":"reference/simaticai/helpers/reporter.html#format","text":"def format ( self , record ) Format the specified record. If a formatter is set, use it. Otherwise, use the default formatter for the module. View Source def format(self, record): \"\"\" Format the specified record. If a formatter is set, use it. Otherwise, use the default formatter for the module. \"\"\" if self.formatter: fmt = self.formatter else: fmt = _defaultFormatter return fmt.format(record)","title":"format"},{"location":"reference/simaticai/helpers/reporter.html#get_name","text":"def get_name ( self ) View Source def get_name(self): return self._name","title":"get_name"},{"location":"reference/simaticai/helpers/reporter.html#handle","text":"def handle ( self , record ) Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler. Wrap the actual emission of the record with acquisition/release of the I/O thread lock. Returns whether the filter passed the record for emission. View Source def handle ( self , record ) : \"\" \" Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler. Wrap the actual emission of the record with acquisition/release of the I/O thread lock. Returns whether the filter passed the record for emission. \"\" \" rv = self.filter(record) if rv: self.acquire() try: self.emit(record) finally: self.release() return rv","title":"handle"},{"location":"reference/simaticai/helpers/reporter.html#handleerror","text":"def handleError ( self , record ) Handle errors which occur during an emit() call. This method should be called from handlers when an exception is encountered during an emit() call. If raiseExceptions is false, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The record which was being processed is passed in to this method. View Source def handleError(self, record): \"\"\" Handle errors which occur during an emit() call. This method should be called from handlers when an exception is encountered during an emit() call. If raiseExceptions is false, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The record which was being processed is passed in to this method. \"\"\" if raiseExceptions and sys.stderr: # see issue 13807 t, v, tb = sys.exc_info() try: sys.stderr.write('--- Logging error ---\\n') traceback.print_exception(t, v, tb, None, sys.stderr) sys.stderr.write('Call stack:\\n') # Walk the stack frame up until we're out of logging, # so as to print the calling context. frame = tb.tb_frame while (frame and os.path.dirname(frame.f_code.co_filename) == __path__[0]): frame = frame.f_back if frame: traceback.print_stack(frame, file=sys.stderr) else: # couldn't find the right stack frame, for some reason sys.stderr.write('Logged from file %s, line %s\\n' % ( record.filename, record.lineno)) # Issue 18671: output logging message and arguments try: sys.stderr.write('Message: %r\\n' 'Arguments: %s\\n' % (record.msg, record.args)) except RecursionError: # See issue 36272 raise except Exception: sys.stderr.write('Unable to print the message and arguments' ' - possible formatting error.\\nUse the' ' traceback above to help find the error.\\n' ) except OSError: #pragma: no cover pass # see issue 5971 finally: del t, v, tb","title":"handleError"},{"location":"reference/simaticai/helpers/reporter.html#release","text":"def release ( self ) Release the I/O thread lock. View Source def release ( self ) : \"\" \" Release the I/O thread lock. \"\" \" if self.lock: self.lock.release()","title":"release"},{"location":"reference/simaticai/helpers/reporter.html#removefilter","text":"def removeFilter ( self , filter ) Remove the specified filter from this handler. View Source def removeFilter ( self , filter ) : \"\" \" Remove the specified filter from this handler. \"\" \" if filter in self.filters: self.filters.remove(filter)","title":"removeFilter"},{"location":"reference/simaticai/helpers/reporter.html#setformatter","text":"def setFormatter ( self , fmt ) Set the formatter for this handler. View Source def setFormatter ( self , fmt ) : \"\" \" Set the formatter for this handler. \"\" \" self.formatter = fmt","title":"setFormatter"},{"location":"reference/simaticai/helpers/reporter.html#setlevel","text":"def setLevel ( self , level ) Set the logging level of this handler. level must be an int or a str. View Source def setLevel(self, level): \"\"\" Set the logging level of this handler. level must be an int or a str. \"\"\" self.level = _checkLevel(level)","title":"setLevel"},{"location":"reference/simaticai/helpers/reporter.html#set_name","text":"def set_name ( self , name ) View Source def set_name ( self , name ) : _acquireLock () try : if self . _name in _handlers : del _handlers [ self._name ] self . _name = name if name : _handlers [ name ] = self finally : _releaseLock ()","title":"set_name"},{"location":"reference/simaticai/helpers/reporter.html#ziptreeelement","text":"A class to represent a file or folder in a zip file. During the recursive traversal of the zip file, the full name and file size are stored in this class. class ZipTreeElement ( full_name , file_size ) View Source class ZipTreeElement: \"\"\" A class to represent a file or folder in a zip file. During the recursive traversal of the zip file, the full name and file size are stored in this class. \"\"\" def __init__ ( self , full_name , file_size ): self . full_name = full_name self . file_size = file_size","title":"ZipTreeElement"},{"location":"reference/simaticai/helpers/tempfiles.html","text":"Module simaticai.helpers.tempfiles Module for dealing with temporary files. This module helps with extracting a zip file into a temporary folder. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Module for dealing with temporary files. This module helps with extracting a zip file into a temporary folder. \"\"\" import os import shutil import tempfile import zipfile from pathlib import Path from typing import Union class OpenZipInTemp : \"\"\" Unzip a zip archive into a temporary directory. Example usage: with OpenZipInTemp(\"path_to_zip_file.zip\") as temp_dir: # do something with temp_dir pass Args: zip_path (path-like): path to the archive. \"\"\" def __init__ ( self , zip_path : Union [ str , os . PathLike ], clean_up : bool = True ): if not zipfile . is_zipfile ( zip_path ): raise ValueError ( f \"File does not exist or not a zip file: { zip_path } \" ) self . zip_path = zip_path self . tmp_path = None self . clean_up = clean_up def __enter__ ( self ) -> Path : self . tmp_path = Path ( tempfile . mkdtemp ( prefix = \"unzip-\" )) with zipfile . ZipFile ( self . zip_path , \"r\" ) as zip_file : zip_file . extractall ( path = self . tmp_path ) return self . tmp_path def __exit__ ( self , e_type , e_val , e_trace ): if self . clean_up : shutil . rmtree ( self . tmp_path , ignore_errors = True ) Classes OpenZipInTemp Unzip a zip archive into a temporary directory. Example usage: with OpenZipInTemp(\"path_to_zip_file.zip\") as temp_dir: # do something with temp_dir pass class OpenZipInTemp ( zip_path : Union [ str , os . PathLike ], clean_up : bool = True ) Attributes Name Type Description Default zip_path path-like path to the archive. None View Source class OpenZipInTemp : \"\"\" Unzip a zip archive into a temporary directory. Example usage: with OpenZipInTemp(\" path_to_zip_file . zip \") as temp_dir: # do something with temp_dir pass Args: zip_path (path-like): path to the archive. \"\"\" def __init__ ( self , zip_path : Union [ str , os . PathLike ], clean_up : bool = True ): if not zipfile . is_zipfile ( zip_path ): raise ValueError ( f \"File does not exist or not a zip file: {zip_path}\" ) self . zip_path = zip_path self . tmp_path = None self . clean_up = clean_up def __enter__ ( self ) -> Path : self . tmp_path = Path ( tempfile . mkdtemp ( prefix = \"unzip-\" )) with zipfile . ZipFile ( self . zip_path , \"r\" ) as zip_file : zip_file . extractall ( path = self . tmp_path ) return self . tmp_path def __exit__ ( self , e_type , e_val , e_trace ): if self . clean_up : shutil . rmtree ( self . tmp_path , ignore_errors = True )","title":"Tempfiles"},{"location":"reference/simaticai/helpers/tempfiles.html#module-simaticaihelperstempfiles","text":"Module for dealing with temporary files. This module helps with extracting a zip file into a temporary folder. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Module for dealing with temporary files. This module helps with extracting a zip file into a temporary folder. \"\"\" import os import shutil import tempfile import zipfile from pathlib import Path from typing import Union class OpenZipInTemp : \"\"\" Unzip a zip archive into a temporary directory. Example usage: with OpenZipInTemp(\"path_to_zip_file.zip\") as temp_dir: # do something with temp_dir pass Args: zip_path (path-like): path to the archive. \"\"\" def __init__ ( self , zip_path : Union [ str , os . PathLike ], clean_up : bool = True ): if not zipfile . is_zipfile ( zip_path ): raise ValueError ( f \"File does not exist or not a zip file: { zip_path } \" ) self . zip_path = zip_path self . tmp_path = None self . clean_up = clean_up def __enter__ ( self ) -> Path : self . tmp_path = Path ( tempfile . mkdtemp ( prefix = \"unzip-\" )) with zipfile . ZipFile ( self . zip_path , \"r\" ) as zip_file : zip_file . extractall ( path = self . tmp_path ) return self . tmp_path def __exit__ ( self , e_type , e_val , e_trace ): if self . clean_up : shutil . rmtree ( self . tmp_path , ignore_errors = True )","title":"Module simaticai.helpers.tempfiles"},{"location":"reference/simaticai/helpers/tempfiles.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/helpers/tempfiles.html#openzipintemp","text":"Unzip a zip archive into a temporary directory. Example usage: with OpenZipInTemp(\"path_to_zip_file.zip\") as temp_dir: # do something with temp_dir pass class OpenZipInTemp ( zip_path : Union [ str , os . PathLike ], clean_up : bool = True )","title":"OpenZipInTemp"},{"location":"reference/simaticai/helpers/tempfiles.html#attributes","text":"Name Type Description Default zip_path path-like path to the archive. None View Source class OpenZipInTemp : \"\"\" Unzip a zip archive into a temporary directory. Example usage: with OpenZipInTemp(\" path_to_zip_file . zip \") as temp_dir: # do something with temp_dir pass Args: zip_path (path-like): path to the archive. \"\"\" def __init__ ( self , zip_path : Union [ str , os . PathLike ], clean_up : bool = True ): if not zipfile . is_zipfile ( zip_path ): raise ValueError ( f \"File does not exist or not a zip file: {zip_path}\" ) self . zip_path = zip_path self . tmp_path = None self . clean_up = clean_up def __enter__ ( self ) -> Path : self . tmp_path = Path ( tempfile . mkdtemp ( prefix = \"unzip-\" )) with zipfile . ZipFile ( self . zip_path , \"r\" ) as zip_file : zip_file . extractall ( path = self . tmp_path ) return self . tmp_path def __exit__ ( self , e_type , e_val , e_trace ): if self . clean_up : shutil . rmtree ( self . tmp_path , ignore_errors = True )","title":"Attributes"},{"location":"reference/simaticai/helpers/yaml_helper.html","text":"Module simaticai.helpers.yaml_helper Helper module for YAML files. Reads YAML files into a dictionary with a custom loader. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Helper module for YAML files. Reads YAML files into a dictionary with a custom loader. \"\"\" import os import yaml def read_yaml ( path : os . PathLike ): \"\"\" Read a YAML file into a dictionary. Loads the YAML file specified by `path`. The YAML loader is configured to read datetime objects as strings, for simplifying validation with a JSON schema. Args: path (path-like): Path of the YAML file. Returns: dict: A dictionary, populated from the YAML file. \"\"\" _remove_implicit_resolver ( yaml . SafeLoader ) with open ( path , \"r\" , encoding = \"utf8\" ) as file : return yaml . load ( file , Loader = yaml . SafeLoader ) def _remove_implicit_resolver ( cls , tag_to_remove = 'tag:yaml.org,2002:timestamp' ): \"\"\" Remove implicit resolvers for a particular tag Takes care not to modify resolvers in super classes. We want to load datetime objects as strings, not dates, because we go on to serialise as JSON which doesn't have the advanced types of YAML, and leads to incompatibilities down the track. \"\"\" if 'yaml_implicit_resolvers' not in cls . __dict__ : cls . yaml_implicit_resolvers = cls . yaml_implicit_resolvers . copy () for first_letter , mappings in cls . yaml_implicit_resolvers . items (): cls . yaml_implicit_resolvers [ first_letter ] = [ ( tag , regexp ) for tag , regexp in mappings if tag != tag_to_remove ] Functions read_yaml def read_yaml ( path : os . PathLike ) Read a YAML file into a dictionary. Loads the YAML file specified by path . The YAML loader is configured to read datetime objects as strings, for simplifying validation with a JSON schema. Parameters: Name Type Description Default path path-like Path of the YAML file. None Returns: Type Description dict A dictionary, populated from the YAML file. View Source def read_yaml ( path : os . PathLike ) : \" \"\" Read a YAML file into a dictionary. Loads the YAML file specified by `path`. The YAML loader is configured to read datetime objects as strings, for simplifying validation with a JSON schema. Args: path (path-like): Path of the YAML file. Returns: dict: A dictionary, populated from the YAML file. \"\" \" _remove_implicit_resolver ( yaml . SafeLoader ) with open ( path , \"r\" , encoding = \"utf8\" ) as file : return yaml . load ( file , Loader = yaml . SafeLoader )","title":"Yaml Helper"},{"location":"reference/simaticai/helpers/yaml_helper.html#module-simaticaihelpersyaml_helper","text":"Helper module for YAML files. Reads YAML files into a dictionary with a custom loader. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Helper module for YAML files. Reads YAML files into a dictionary with a custom loader. \"\"\" import os import yaml def read_yaml ( path : os . PathLike ): \"\"\" Read a YAML file into a dictionary. Loads the YAML file specified by `path`. The YAML loader is configured to read datetime objects as strings, for simplifying validation with a JSON schema. Args: path (path-like): Path of the YAML file. Returns: dict: A dictionary, populated from the YAML file. \"\"\" _remove_implicit_resolver ( yaml . SafeLoader ) with open ( path , \"r\" , encoding = \"utf8\" ) as file : return yaml . load ( file , Loader = yaml . SafeLoader ) def _remove_implicit_resolver ( cls , tag_to_remove = 'tag:yaml.org,2002:timestamp' ): \"\"\" Remove implicit resolvers for a particular tag Takes care not to modify resolvers in super classes. We want to load datetime objects as strings, not dates, because we go on to serialise as JSON which doesn't have the advanced types of YAML, and leads to incompatibilities down the track. \"\"\" if 'yaml_implicit_resolvers' not in cls . __dict__ : cls . yaml_implicit_resolvers = cls . yaml_implicit_resolvers . copy () for first_letter , mappings in cls . yaml_implicit_resolvers . items (): cls . yaml_implicit_resolvers [ first_letter ] = [ ( tag , regexp ) for tag , regexp in mappings if tag != tag_to_remove ]","title":"Module simaticai.helpers.yaml_helper"},{"location":"reference/simaticai/helpers/yaml_helper.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/helpers/yaml_helper.html#read_yaml","text":"def read_yaml ( path : os . PathLike ) Read a YAML file into a dictionary. Loads the YAML file specified by path . The YAML loader is configured to read datetime objects as strings, for simplifying validation with a JSON schema. Parameters: Name Type Description Default path path-like Path of the YAML file. None Returns: Type Description dict A dictionary, populated from the YAML file. View Source def read_yaml ( path : os . PathLike ) : \" \"\" Read a YAML file into a dictionary. Loads the YAML file specified by `path`. The YAML loader is configured to read datetime objects as strings, for simplifying validation with a JSON schema. Args: path (path-like): Path of the YAML file. Returns: dict: A dictionary, populated from the YAML file. \"\" \" _remove_implicit_resolver ( yaml . SafeLoader ) with open ( path , \"r\" , encoding = \"utf8\" ) as file : return yaml . load ( file , Loader = yaml . SafeLoader )","title":"read_yaml"},{"location":"reference/simaticai/packaging/index.html","text":"Module simaticai.packaging Pipeline packaging. This module contains classes and functionality for creating and validating pipeline configuration packages. View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . \"\"\" Pipeline packaging. This module contains classes and functionality for creating and validating pipeline configuration packages. \"\"\" Sub-modules simaticai.packaging.constants simaticai.packaging.python_dependencies simaticai.packaging.wheelhouse","title":"Index"},{"location":"reference/simaticai/packaging/index.html#module-simaticaipackaging","text":"Pipeline packaging. This module contains classes and functionality for creating and validating pipeline configuration packages. View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . \"\"\" Pipeline packaging. This module contains classes and functionality for creating and validating pipeline configuration packages. \"\"\"","title":"Module simaticai.packaging"},{"location":"reference/simaticai/packaging/index.html#sub-modules","text":"simaticai.packaging.constants simaticai.packaging.python_dependencies simaticai.packaging.wheelhouse","title":"Sub-modules"},{"location":"reference/simaticai/packaging/constants.html","text":"Module simaticai.packaging.constants Common constants used in 'packaging' module. None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Common constants used in 'packaging' module. \"\"\" README_HTML = \"README.html\" REQUIREMENTS_TXT = \"requirements.txt\" PYTHON_PACKAGES = \"PythonPackages\" PYTHON_PACKAGES_ZIP = f \"{PYTHON_PACKAGES}.zip\" PIPELINE_CONFIG = \"pipeline_config.yml\" DATALINK_METADATA = \"datalink_metadata.yml\" RUNTIME_CONFIG = \"runtime_config.yml\" TELEMETRY_YAML = \"telemetry_data.yml\" MSG_NOT_FOUND = \"not found\" PIPELINE_SIZE_LIMIT = int ( 2.2 * 1000 * 1000 * 1000 ) # zipped pipeline size limit of 2.2 GB # Based on https://code.siemens.com/siemens-ai-launcher-sail/ai-on-edge/sail-pipes-orchestrator/sail-pipes-orchestrator-ui/-/blob/developer/src/app/models/databus/connector.constant.ts?ref_type=heads#L199 supported_types = [ 'Boolean' , 'Integer' , 'Double' , 'String' , 'BooleanArray' , 'IntegerArray' , \"UInt8Array\" , \"UInt16Array\" , \"UInt32Array\" , \"UInt64Array\" , \"Int8Array\" , \"Int16Array\" , \"Int32Array\" , \"Int64Array\" , 'DoubleArray' , \"Float16Array\" , \"Float32Array\" , \"Float64Array\" , 'StringArray' , 'Object' , 'Binary' , 'ImageSet' , ] \"\"\" List of input/output data types supported by AI Inference Server. Custom types can also be provided when specifying pipeline inputs/outputs, but AI SDK will raise a warning message in this case. \"\"\" PLATFORMS = [ \"any\" , \"manylinux1_x86_64\" , \"manylinux2010_x86_64\" , \"manylinux2014_x86_64\" , \"linux_x86_64\" ] + [ f \"manylinux_2_{glibc_minor}_x86_64\" for glibc_minor in range ( 5 , 32 ) ] \"\"\" List of platform tags supported by AI Inference Server. \"\"\" Variables DATALINK_METADATA MSG_NOT_FOUND PIPELINE_CONFIG PIPELINE_SIZE_LIMIT PLATFORMS List of platform tags supported by AI Inference Server. PYTHON_PACKAGES PYTHON_PACKAGES_ZIP README_HTML REQUIREMENTS_TXT RUNTIME_CONFIG TELEMETRY_YAML supported_types List of input/output data types supported by AI Inference Server. Custom types can also be provided when specifying pipeline inputs/outputs, but AI SDK will raise a warning message in this case.","title":"Constants"},{"location":"reference/simaticai/packaging/constants.html#module-simaticaipackagingconstants","text":"Common constants used in 'packaging' module. None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Common constants used in 'packaging' module. \"\"\" README_HTML = \"README.html\" REQUIREMENTS_TXT = \"requirements.txt\" PYTHON_PACKAGES = \"PythonPackages\" PYTHON_PACKAGES_ZIP = f \"{PYTHON_PACKAGES}.zip\" PIPELINE_CONFIG = \"pipeline_config.yml\" DATALINK_METADATA = \"datalink_metadata.yml\" RUNTIME_CONFIG = \"runtime_config.yml\" TELEMETRY_YAML = \"telemetry_data.yml\" MSG_NOT_FOUND = \"not found\" PIPELINE_SIZE_LIMIT = int ( 2.2 * 1000 * 1000 * 1000 ) # zipped pipeline size limit of 2.2 GB # Based on https://code.siemens.com/siemens-ai-launcher-sail/ai-on-edge/sail-pipes-orchestrator/sail-pipes-orchestrator-ui/-/blob/developer/src/app/models/databus/connector.constant.ts?ref_type=heads#L199 supported_types = [ 'Boolean' , 'Integer' , 'Double' , 'String' , 'BooleanArray' , 'IntegerArray' , \"UInt8Array\" , \"UInt16Array\" , \"UInt32Array\" , \"UInt64Array\" , \"Int8Array\" , \"Int16Array\" , \"Int32Array\" , \"Int64Array\" , 'DoubleArray' , \"Float16Array\" , \"Float32Array\" , \"Float64Array\" , 'StringArray' , 'Object' , 'Binary' , 'ImageSet' , ] \"\"\" List of input/output data types supported by AI Inference Server. Custom types can also be provided when specifying pipeline inputs/outputs, but AI SDK will raise a warning message in this case. \"\"\" PLATFORMS = [ \"any\" , \"manylinux1_x86_64\" , \"manylinux2010_x86_64\" , \"manylinux2014_x86_64\" , \"linux_x86_64\" ] + [ f \"manylinux_2_{glibc_minor}_x86_64\" for glibc_minor in range ( 5 , 32 ) ] \"\"\" List of platform tags supported by AI Inference Server. \"\"\"","title":"Module simaticai.packaging.constants"},{"location":"reference/simaticai/packaging/constants.html#variables","text":"DATALINK_METADATA MSG_NOT_FOUND PIPELINE_CONFIG PIPELINE_SIZE_LIMIT PLATFORMS List of platform tags supported by AI Inference Server. PYTHON_PACKAGES PYTHON_PACKAGES_ZIP README_HTML REQUIREMENTS_TXT RUNTIME_CONFIG TELEMETRY_YAML supported_types List of input/output data types supported by AI Inference Server. Custom types can also be provided when specifying pipeline inputs/outputs, but AI SDK will raise a warning message in this case.","title":"Variables"},{"location":"reference/simaticai/packaging/python_dependencies.html","text":"Module simaticai.packaging.python_dependencies Python dependencies This class handles specifying and validating Python dependencies. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Python dependencies This class handles specifying and validating Python dependencies. \"\"\" import os import sys import logging import shutil import tempfile import zipfile import requests import urllib from pathlib import Path from typing import Union from simaticai.helpers import pep508 from .constants import REQUIREMENTS_TXT , PYTHON_PACKAGES from .wheelhouse import is_wheel_file , is_pure_python_source , get_wheel_name_version , get_sdist_name_version , _check_package_for_dependency_limitations logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _GPU_DEPENDENCIES = [ 'torch' , 'torchaudio' , 'torchvision' , 'ultralytics' , 'ultralyticsheadless' ] _PYTORCH_CPU_REPO_URL = 'https://download.pytorch.org/whl/cpu' _PYPI_REPO_URL = 'https://pypi.org/simple' REPO_MODIFICATION_WARNING_MSG = \"Pytorch GPU dependencies were replaced with CPU only Pytorch version. \" \\ f \"Using { _PYTORCH_CPU_REPO_URL } as the primary repository.\" ADDED_PYPI_WARNING_MSG = f \"Extra index url list was prepended with { _PYPI_REPO_URL } .\" INDEX_URL_MOVED_WARNING_MSG = \"User defined index url was moved to extra index url list.\" class PythonDependencies (): def __init__ ( self , python_version = '3.11' , dir : Union [ str , os . PathLike ] = None ): \"\"\" This class handles Python dependencies Dependencies from remote repositories can be added via a requirements.txt file, or by calling the add_dependencies method. Dependencies can also be added as a single wheel or source distribution file, or as a collection in a zip archive via the add_python_packages method. The class can be converted to string, which will contain the dependencies in PEP508 format. \"\"\" self . python_version = python_version self . dependencies = {} self . python_packages = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"dependencies_\" )) self . temp = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"temp_\" )) self . optimize_dependencies = True self . index_url = None self . extra_index = [] def __str__ ( self ): \"\"\" PEP508 representation of the dependencies. \"\"\" result = \"\" if self . index_url is not None : result += '# Index URL \\n ' result += f \" { self . index_url } \\n \" if len ( self . extra_index ) > 0 : result += \"# Extra index urls \\n \" for url in self . extra_index : result += f \" { url } \\n \" result += \"# Runtime dependencies \\n \" for spec in self . dependencies . values (): result += str ( spec ) + \" \\n \" return result def __repr__ ( self ): return self . __str__ () def clear ( self ): self . dependencies . clear () self . extra_index = [] self . index_url = None shutil . rmtree ( self . python_packages , ignore_errors = True ) shutil . rmtree ( self . temp , ignore_errors = True ) self . python_packages . mkdir ( mode = 0o700 , exist_ok = True ) self . temp . mkdir ( mode = 0o700 , exist_ok = True ) _logger . warning ( \"Previously added dependencies have been removed.\" ) def set_requirements ( self , requirements_path : Union [ str , os . PathLike ]): self . clear () dependencies , extra_index , index_url = pep508 . parse_requirements ( requirements_path ) for name , spec in dependencies . items (): _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): self . dependencies [ name ] = spec _logger . info ( f \"Runtime dependency added: { spec } \" ) else : _logger . warning ( f \"Dependency already exists: { spec } \" ) if index_url is not None : self . index_url = index_url _logger . info ( f \"Index url added: { index_url } \" ) for url in extra_index : self . extra_index . append ( url ) _logger . info ( f \"Extra index url added: { url } \" ) def add_dependencies ( self , packages : list ): for package in packages : if isinstance ( package , tuple ): name , version = package spec = pep508 . parse_line ( f \" { name } == { version } \" ) else : spec = pep508 . parse_line ( f \" { package } \" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: { spec } \" ) else : _logger . warning ( f \"Dependency already exists: { spec } \" ) def add_python_packages ( self , path : Union [ str , os . PathLike ]) -> None : path : Path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"The file must be available on path { path . resolve () } \" ) specs = [] tmp = None if is_wheel_file ( path ): name , version = get_wheel_name_version ( path ) specs . append (( name , version , path )) elif is_pure_python_source ( path ): name , version = get_sdist_name_version ( path ) specs . append (( name , version , path )) elif zipfile . is_zipfile ( path ): tmp = Path ( tempfile . mkdtemp ( dir = self . temp )) zip = zipfile . ZipFile ( path ) for pkg in zip . namelist (): zip . extract ( pkg , path = tmp ) file = tmp / Path ( pkg ) . name if is_wheel_file ( file ): name , version = get_wheel_name_version ( file ) specs . append (( name , version , file )) elif is_pure_python_source ( file ): name , version = get_sdist_name_version ( file ) specs . append (( name , version , file )) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source: { pkg } \" ) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source or a zip file: { path } \" ) for name , version , path in specs : if name is not None : if version is None : spec = pep508 . parse_line ( f \" { name } \" ) else : spec = pep508 . parse_line ( f \" { name } == { version } \" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): shutil . copy ( path , self . python_packages ) self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: { spec } \" ) else : _logger . warning ( f \"Dependency already exists: { spec } \" ) if tmp is not None : shutil . rmtree ( tmp , ignore_errors = True ) def _download_or_copy_dependency ( self , name , version ): dependency_url = urllib . parse . urlparse ( version ) # raises ValueError if the url is invalid dependency_path = Path ( urllib . parse . unquote ( dependency_url . path )) filename = dependency_path . name if \"file\" == dependency_url . scheme : # Possible Exceptions here: FileNotFoundError, PermissionError, OSError, IsADirectoryError, SameFileError if not dependency_path . is_file (): raise FileNotFoundError ( f \"The dependency ' { name } ' can not be found on path ' { dependency_path } '\" ) if ( self . python_packages / filename ) . is_file (): _logger . warning ( f \"Dependency ' { name } ' will not be copied because it already exists in ' { self . python_packages } ' folder.\" ) else : _logger . info ( f \"Dependency ' { name } ' will be copied to ' { self . python_packages } ' folder.\" ) shutil . copy ( dependency_path . resolve (), self . python_packages ) else : # Possible Exceptions here: requests.exceptions.RequestException _logger . info ( f \"Dependency ' { name } @ { version } ' will be downloaded from the repository.\" ) response = requests . get ( version ) response . raise_for_status () with open ( self . python_packages / filename , \"wb\" ) as f : f . write ( response . content ) return self . python_packages / filename def save ( self , folder_path ): # Downloads dependencies specified with url from remote repositories or copies them from local file system # Does not work with source distributed packages for name , dependency in self . dependencies . copy () . items (): if isinstance ( dependency . version , str ): try : path = self . _download_or_copy_dependency ( name , dependency . version ) _ , version = get_wheel_name_version ( path ) self . dependencies [ name ] = pep508 . parse_line ( f \" { name } == { version } \" ) except requests . exceptions . RequestException as request_error : raise RuntimeError ( f \"Failed to download dependency ' { dependency . name } == { dependency . version } ' from the repository.\" ) from request_error requirements_file_path = folder_path / REQUIREMENTS_TXT with open ( requirements_file_path , \"w\" ) as f : f . write ( str ( self )) shutil . make_archive ( base_name = folder_path / PYTHON_PACKAGES , root_dir = self . python_packages , format = 'zip' , verbose = True , logger = _logger ) def _check_if_index_url_is_set_to_pytorch_cpu ( self ): if self . index_url is None : return False if self . index_url . strip () . startswith ( \"--index-url\" ) and _PYTORCH_CPU_REPO_URL in self . index_url : return True return False def enable_dependency_optimization ( self ): self . optimize_dependencies = True def disable_dependency_optimization ( self ): self . optimize_dependencies = False def validate ( self ): for spec in self . dependencies . values (): _check_package_for_dependency_limitations ( spec . name ) found_gpu_dependency = any ( dep in _GPU_DEPENDENCIES for dep in self . dependencies ) if not found_gpu_dependency : return if self . optimize_dependencies : if self . index_url is None : self . index_url = f \"--index-url { _PYTORCH_CPU_REPO_URL } \" added_pypi_warning = \"\" if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url { _PYPI_REPO_URL } \" ) added_pypi_warning = ADDED_PYPI_WARNING_MSG _logger . warning ( f \"WARNING! { REPO_MODIFICATION_WARNING_MSG } { added_pypi_warning } \" ) elif self . _check_if_index_url_is_set_to_pytorch_cpu (): if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url { _PYPI_REPO_URL } \" ) _logger . warning ( f \"WARNING! { REPO_MODIFICATION_WARNING_MSG } { ADDED_PYPI_WARNING_MSG } \" ) else : user_defined_index_url = self . index_url . replace ( \"--index-url\" , \"--extra-index-url\" , 1 ) self . index_url = f \"--index-url { _PYTORCH_CPU_REPO_URL } \" self . extra_index . insert ( 0 , user_defined_index_url ) _logger . warning ( f \"WARNING! { REPO_MODIFICATION_WARNING_MSG } { INDEX_URL_MOVED_WARNING_MSG } \" ) else : if not self . _check_if_index_url_is_set_to_pytorch_cpu (): _logger . warning ( \"WARNING! The resulting package could contain unused GPU dependencies \" \"which considerably increase the file size.\" ) Variables ADDED_PYPI_WARNING_MSG INDEX_URL_MOVED_WARNING_MSG PYTHON_PACKAGES REPO_MODIFICATION_WARNING_MSG REQUIREMENTS_TXT Classes PythonDependencies class PythonDependencies ( python_version = '3.11' , dir : Union [ str , os . PathLike ] = None ) View Source class PythonDependencies () : def __init__ ( self , python_version = '3.11' , dir : Union [ str, os.PathLike ] = None ) : \"\"\" This class handles Python dependencies Dependencies from remote repositories can be added via a requirements.txt file, or by calling the add_dependencies method. Dependencies can also be added as a single wheel or source distribution file, or as a collection in a zip archive via the add_python_packages method. The class can be converted to string, which will contain the dependencies in PEP508 format. \"\"\" self . python_version = python_version self . dependencies = {} self . python_packages = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"dependencies_\" )) self . temp = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"temp_\" )) self . optimize_dependencies = True self . index_url = None self . extra_index = [] def __str__ ( self ) : \"\"\" PEP508 representation of the dependencies. \"\"\" result = \"\" if self . index_url is not None : result += '# Index URL\\n' result += f \"{self.index_url}\\n\" if len ( self . extra_index ) > 0 : result += \"# Extra index urls\\n\" for url in self . extra_index : result += f \"{url}\\n\" result += \"# Runtime dependencies\\n\" for spec in self . dependencies . values () : result += str ( spec ) + \"\\n\" return result def __repr__ ( self ) : return self . __str__ () def clear ( self ) : self . dependencies . clear () self . extra_index = [] self . index_url = None shutil . rmtree ( self . python_packages , ignore_errors = True ) shutil . rmtree ( self . temp , ignore_errors = True ) self . python_packages . mkdir ( mode = 0 o700 , exist_ok = True ) self . temp . mkdir ( mode = 0 o700 , exist_ok = True ) _logger . warning ( \"Previously added dependencies have been removed.\" ) def set_requirements ( self , requirements_path : Union [ str, os.PathLike ] ) : self . clear () dependencies , extra_index , index_url = pep508 . parse_requirements ( requirements_path ) for name , spec in dependencies . items () : _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : self . dependencies [ name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if index_url is not None : self . index_url = index_url _logger . info ( f \"Index url added: {index_url}\" ) for url in extra_index : self . extra_index . append ( url ) _logger . info ( f \"Extra index url added: {url}\" ) def add_dependencies ( self , packages : list ) : for package in packages : if isinstance ( package , tuple ) : name , version = package spec = pep508 . parse_line ( f \"{name}=={version}\" ) else : spec = pep508 . parse_line ( f \"{package}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : self . dependencies [ spec.name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) def add_python_packages ( self , path : Union [ str, os.PathLike ] ) -> None : path : Path = Path ( path ) if not path . is_file () : raise AssertionError ( f \"The file must be available on path {path.resolve()}\" ) specs = [] tmp = None if is_wheel_file ( path ) : name , version = get_wheel_name_version ( path ) specs . append (( name , version , path )) elif is_pure_python_source ( path ) : name , version = get_sdist_name_version ( path ) specs . append (( name , version , path )) elif zipfile . is_zipfile ( path ) : tmp = Path ( tempfile . mkdtemp ( dir = self . temp )) zip = zipfile . ZipFile ( path ) for pkg in zip . namelist () : zip . extract ( pkg , path = tmp ) file = tmp / Path ( pkg ). name if is_wheel_file ( file ) : name , version = get_wheel_name_version ( file ) specs . append (( name , version , file )) elif is_pure_python_source ( file ) : name , version = get_sdist_name_version ( file ) specs . append (( name , version , file )) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source: {pkg}\" ) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source or a zip file: {path}\" ) for name , version , path in specs : if name is not None : if version is None : spec = pep508 . parse_line ( f \"{name}\" ) else : spec = pep508 . parse_line ( f \"{name}=={version}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : shutil . copy ( path , self . python_packages ) self . dependencies [ spec.name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if tmp is not None : shutil . rmtree ( tmp , ignore_errors = True ) def _download_or_copy_dependency ( self , name , version ) : dependency_url = urllib . parse . urlparse ( version ) # raises ValueError if the url is invalid dependency_path = Path ( urllib . parse . unquote ( dependency_url . path )) filename = dependency_path . name if \"file\" == dependency_url . scheme : # Possible Exceptions here : FileNotFoundError , PermissionError , OSError , IsADirectoryError , SameFileError if not dependency_path . is_file () : raise FileNotFoundError ( f \"The dependency '{name}' can not be found on path '{dependency_path}'\" ) if ( self . python_packages / filename ). is_file () : _logger . warning ( f \"Dependency '{name}' will not be copied because it already exists in '{self.python_packages}' folder.\" ) else : _logger . info ( f \"Dependency '{name}' will be copied to '{self.python_packages}' folder.\" ) shutil . copy ( dependency_path . resolve (), self . python_packages ) else : # Possible Exceptions here : requests . exceptions . RequestException _logger . info ( f \"Dependency '{name}@{version}' will be downloaded from the repository.\" ) response = requests . get ( version ) response . raise_for_status () with open ( self . python_packages / filename , \"wb\" ) as f : f . write ( response . content ) return self . python_packages / filename def save ( self , folder_path ) : # Downloads dependencies specified with url from remote repositories or copies them from local file system # Does not work with source distributed packages for name , dependency in self . dependencies . copy (). items () : if isinstance ( dependency . version , str ) : try : path = self . _download_or_copy_dependency ( name , dependency . version ) _ , version = get_wheel_name_version ( path ) self . dependencies [ name ] = pep508 . parse_line ( f \"{name}=={version}\" ) except requests . exceptions . RequestException as request_error : raise RuntimeError ( f \"Failed to download dependency '{dependency.name}=={dependency.version}' from the repository.\" ) from request_error requirements_file_path = folder_path / REQUIREMENTS_TXT with open ( requirements_file_path , \"w\" ) as f : f . write ( str ( self )) shutil . make_archive ( base_name = folder_path / PYTHON_PACKAGES , root_dir = self . python_packages , format = 'zip' , verbose = True , logger = _logger ) def _check_if_index_url_is_set_to_pytorch_cpu ( self ) : if self . index_url is None : return False if self . index_url . strip (). startswith ( \"--index-url\" ) and _PYTORCH_CPU_REPO_URL in self . index_url : return True return False def enable_dependency_optimization ( self ) : self . optimize_dependencies = True def disable_dependency_optimization ( self ) : self . optimize_dependencies = False def validate ( self ) : for spec in self . dependencies . values () : _check_package_for_dependency_limitations ( spec . name ) found_gpu_dependency = any ( dep in _GPU_DEPENDENCIES for dep in self . dependencies ) if not found_gpu_dependency : return if self . optimize_dependencies : if self . index_url is None : self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" added_pypi_warning = \"\" if not any ( [ _PYPI_REPO_URL in item for item in self.extra_index ] ) : self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) added_pypi_warning = ADDED_PYPI_WARNING_MSG _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {added_pypi_warning}\" ) elif self . _check_if_index_url_is_set_to_pytorch_cpu () : if not any ( [ _PYPI_REPO_URL in item for item in self.extra_index ] ) : self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {ADDED_PYPI_WARNING_MSG}\" ) else : user_defined_index_url = self . index_url . replace ( \"--index-url\" , \"--extra-index-url\" , 1 ) self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" self . extra_index . insert ( 0 , user_defined_index_url ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {INDEX_URL_MOVED_WARNING_MSG}\" ) else : if not self . _check_if_index_url_is_set_to_pytorch_cpu () : _logger . warning ( \"WARNING! The resulting package could contain unused GPU dependencies \" \"which considerably increase the file size.\" ) Methods add_dependencies def add_dependencies ( self , packages : list ) View Source def add_dependencies ( self , packages : list ): for package in packages : if isinstance ( package , tuple ): name , version = package spec = pep508 . parse_line ( f \"{name}=={version}\" ) else : spec = pep508 . parse_line ( f \"{package}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) add_python_packages def add_python_packages ( self , path : Union [ str , os . PathLike ] ) -> None View Source def add_python_packages ( self , path : Union [ str , os . PathLike ]) -> None : path : Path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"The file must be available on path {path.resolve()}\" ) specs = [] tmp = None if is_wheel_file ( path ): name , version = get_wheel_name_version ( path ) specs . append (( name , version , path )) elif is_pure_python_source ( path ): name , version = get_sdist_name_version ( path ) specs . append (( name , version , path )) elif zipfile . is_zipfile ( path ): tmp = Path ( tempfile . mkdtemp ( dir = self . temp )) zip = zipfile . ZipFile ( path ) for pkg in zip . namelist (): zip . extract ( pkg , path = tmp ) file = tmp / Path ( pkg ). name if is_wheel_file ( file ): name , version = get_wheel_name_version ( file ) specs . append (( name , version , file )) elif is_pure_python_source ( file ): name , version = get_sdist_name_version ( file ) specs . append (( name , version , file )) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source: {pkg}\" ) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source or a zip file: {path}\" ) for name , version , path in specs : if name is not None : if version is None : spec = pep508 . parse_line ( f \"{name}\" ) else : spec = pep508 . parse_line ( f \"{name}=={version}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): shutil . copy ( path , self . python_packages ) self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if tmp is not None : shutil . rmtree ( tmp , ignore_errors = True ) clear def clear ( self ) View Source def clear ( self ): self . dependencies . clear () self . extra_index = [] self . index_url = None shutil . rmtree ( self . python_packages , ignore_errors = True ) shutil . rmtree ( self . temp , ignore_errors = True ) self . python_packages . mkdir ( mode = 0o700 , exist_ok = True ) self . temp . mkdir ( mode = 0o700 , exist_ok = True ) _logger . warning ( \"Previously added dependencies have been removed.\" ) disable_dependency_optimization def disable_dependency_optimization ( self ) View Source def disable_dependency_optimization(self): self.optimize_dependencies = False enable_dependency_optimization def enable_dependency_optimization ( self ) View Source def enable_dependency_optimization(self): self.optimize_dependencies = True save def save ( self , folder_path ) View Source def save ( self , folder_path ) : # Downloads dependencies specified with url from remote repositories or copies them from local file system # Does not work with source distributed packages for name , dependency in self . dependencies . copy (). items () : if isinstance ( dependency . version , str ) : try : path = self . _download_or_copy_dependency ( name , dependency . version ) _ , version = get_wheel_name_version ( path ) self . dependencies [ name ] = pep508 . parse_line ( f \"{name}=={version}\" ) except requests . exceptions . RequestException as request_error : raise RuntimeError ( f \"Failed to download dependency '{dependency.name}=={dependency.version}' from the repository.\" ) from request_error requirements_file_path = folder_path / REQUIREMENTS_TXT with open ( requirements_file_path , \"w\" ) as f : f . write ( str ( self )) shutil . make_archive ( base_name = folder_path / PYTHON_PACKAGES , root_dir = self . python_packages , format = 'zip' , verbose = True , logger = _logger ) set_requirements def set_requirements ( self , requirements_path : Union [ str , os . PathLike ] ) View Source def set_requirements ( self , requirements_path : Union [ str, os.PathLike ] ) : self . clear () dependencies , extra_index , index_url = pep508 . parse_requirements ( requirements_path ) for name , spec in dependencies . items () : _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : self . dependencies [ name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if index_url is not None : self . index_url = index_url _logger . info ( f \"Index url added: {index_url}\" ) for url in extra_index : self . extra_index . append ( url ) _logger . info ( f \"Extra index url added: {url}\" ) validate def validate ( self ) View Source def validate ( self ): for spec in self . dependencies . values (): _check_package_for_dependency_limitations ( spec . name ) found_gpu_dependency = any ( dep in _GPU_DEPENDENCIES for dep in self . dependencies ) if not found_gpu_dependency : return if self . optimize_dependencies : if self . index_url is None : self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" added_pypi_warning = \"\" if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) added_pypi_warning = ADDED_PYPI_WARNING_MSG _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {added_pypi_warning}\" ) elif self . _check_if_index_url_is_set_to_pytorch_cpu (): if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {ADDED_PYPI_WARNING_MSG}\" ) else : user_defined_index_url = self . index_url . replace ( \"--index-url\" , \"--extra-index-url\" , 1 ) self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" self . extra_index . insert ( 0 , user_defined_index_url ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {INDEX_URL_MOVED_WARNING_MSG}\" ) else : if not self . _check_if_index_url_is_set_to_pytorch_cpu (): _logger . warning ( \"WARNING! The resulting package could contain unused GPU dependencies \" \"which considerably increase the file size.\" )","title":"Python Dependencies"},{"location":"reference/simaticai/packaging/python_dependencies.html#module-simaticaipackagingpython_dependencies","text":"Python dependencies This class handles specifying and validating Python dependencies. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Python dependencies This class handles specifying and validating Python dependencies. \"\"\" import os import sys import logging import shutil import tempfile import zipfile import requests import urllib from pathlib import Path from typing import Union from simaticai.helpers import pep508 from .constants import REQUIREMENTS_TXT , PYTHON_PACKAGES from .wheelhouse import is_wheel_file , is_pure_python_source , get_wheel_name_version , get_sdist_name_version , _check_package_for_dependency_limitations logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _GPU_DEPENDENCIES = [ 'torch' , 'torchaudio' , 'torchvision' , 'ultralytics' , 'ultralyticsheadless' ] _PYTORCH_CPU_REPO_URL = 'https://download.pytorch.org/whl/cpu' _PYPI_REPO_URL = 'https://pypi.org/simple' REPO_MODIFICATION_WARNING_MSG = \"Pytorch GPU dependencies were replaced with CPU only Pytorch version. \" \\ f \"Using { _PYTORCH_CPU_REPO_URL } as the primary repository.\" ADDED_PYPI_WARNING_MSG = f \"Extra index url list was prepended with { _PYPI_REPO_URL } .\" INDEX_URL_MOVED_WARNING_MSG = \"User defined index url was moved to extra index url list.\" class PythonDependencies (): def __init__ ( self , python_version = '3.11' , dir : Union [ str , os . PathLike ] = None ): \"\"\" This class handles Python dependencies Dependencies from remote repositories can be added via a requirements.txt file, or by calling the add_dependencies method. Dependencies can also be added as a single wheel or source distribution file, or as a collection in a zip archive via the add_python_packages method. The class can be converted to string, which will contain the dependencies in PEP508 format. \"\"\" self . python_version = python_version self . dependencies = {} self . python_packages = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"dependencies_\" )) self . temp = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"temp_\" )) self . optimize_dependencies = True self . index_url = None self . extra_index = [] def __str__ ( self ): \"\"\" PEP508 representation of the dependencies. \"\"\" result = \"\" if self . index_url is not None : result += '# Index URL \\n ' result += f \" { self . index_url } \\n \" if len ( self . extra_index ) > 0 : result += \"# Extra index urls \\n \" for url in self . extra_index : result += f \" { url } \\n \" result += \"# Runtime dependencies \\n \" for spec in self . dependencies . values (): result += str ( spec ) + \" \\n \" return result def __repr__ ( self ): return self . __str__ () def clear ( self ): self . dependencies . clear () self . extra_index = [] self . index_url = None shutil . rmtree ( self . python_packages , ignore_errors = True ) shutil . rmtree ( self . temp , ignore_errors = True ) self . python_packages . mkdir ( mode = 0o700 , exist_ok = True ) self . temp . mkdir ( mode = 0o700 , exist_ok = True ) _logger . warning ( \"Previously added dependencies have been removed.\" ) def set_requirements ( self , requirements_path : Union [ str , os . PathLike ]): self . clear () dependencies , extra_index , index_url = pep508 . parse_requirements ( requirements_path ) for name , spec in dependencies . items (): _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): self . dependencies [ name ] = spec _logger . info ( f \"Runtime dependency added: { spec } \" ) else : _logger . warning ( f \"Dependency already exists: { spec } \" ) if index_url is not None : self . index_url = index_url _logger . info ( f \"Index url added: { index_url } \" ) for url in extra_index : self . extra_index . append ( url ) _logger . info ( f \"Extra index url added: { url } \" ) def add_dependencies ( self , packages : list ): for package in packages : if isinstance ( package , tuple ): name , version = package spec = pep508 . parse_line ( f \" { name } == { version } \" ) else : spec = pep508 . parse_line ( f \" { package } \" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: { spec } \" ) else : _logger . warning ( f \"Dependency already exists: { spec } \" ) def add_python_packages ( self , path : Union [ str , os . PathLike ]) -> None : path : Path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"The file must be available on path { path . resolve () } \" ) specs = [] tmp = None if is_wheel_file ( path ): name , version = get_wheel_name_version ( path ) specs . append (( name , version , path )) elif is_pure_python_source ( path ): name , version = get_sdist_name_version ( path ) specs . append (( name , version , path )) elif zipfile . is_zipfile ( path ): tmp = Path ( tempfile . mkdtemp ( dir = self . temp )) zip = zipfile . ZipFile ( path ) for pkg in zip . namelist (): zip . extract ( pkg , path = tmp ) file = tmp / Path ( pkg ) . name if is_wheel_file ( file ): name , version = get_wheel_name_version ( file ) specs . append (( name , version , file )) elif is_pure_python_source ( file ): name , version = get_sdist_name_version ( file ) specs . append (( name , version , file )) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source: { pkg } \" ) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source or a zip file: { path } \" ) for name , version , path in specs : if name is not None : if version is None : spec = pep508 . parse_line ( f \" { name } \" ) else : spec = pep508 . parse_line ( f \" { name } == { version } \" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): shutil . copy ( path , self . python_packages ) self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: { spec } \" ) else : _logger . warning ( f \"Dependency already exists: { spec } \" ) if tmp is not None : shutil . rmtree ( tmp , ignore_errors = True ) def _download_or_copy_dependency ( self , name , version ): dependency_url = urllib . parse . urlparse ( version ) # raises ValueError if the url is invalid dependency_path = Path ( urllib . parse . unquote ( dependency_url . path )) filename = dependency_path . name if \"file\" == dependency_url . scheme : # Possible Exceptions here: FileNotFoundError, PermissionError, OSError, IsADirectoryError, SameFileError if not dependency_path . is_file (): raise FileNotFoundError ( f \"The dependency ' { name } ' can not be found on path ' { dependency_path } '\" ) if ( self . python_packages / filename ) . is_file (): _logger . warning ( f \"Dependency ' { name } ' will not be copied because it already exists in ' { self . python_packages } ' folder.\" ) else : _logger . info ( f \"Dependency ' { name } ' will be copied to ' { self . python_packages } ' folder.\" ) shutil . copy ( dependency_path . resolve (), self . python_packages ) else : # Possible Exceptions here: requests.exceptions.RequestException _logger . info ( f \"Dependency ' { name } @ { version } ' will be downloaded from the repository.\" ) response = requests . get ( version ) response . raise_for_status () with open ( self . python_packages / filename , \"wb\" ) as f : f . write ( response . content ) return self . python_packages / filename def save ( self , folder_path ): # Downloads dependencies specified with url from remote repositories or copies them from local file system # Does not work with source distributed packages for name , dependency in self . dependencies . copy () . items (): if isinstance ( dependency . version , str ): try : path = self . _download_or_copy_dependency ( name , dependency . version ) _ , version = get_wheel_name_version ( path ) self . dependencies [ name ] = pep508 . parse_line ( f \" { name } == { version } \" ) except requests . exceptions . RequestException as request_error : raise RuntimeError ( f \"Failed to download dependency ' { dependency . name } == { dependency . version } ' from the repository.\" ) from request_error requirements_file_path = folder_path / REQUIREMENTS_TXT with open ( requirements_file_path , \"w\" ) as f : f . write ( str ( self )) shutil . make_archive ( base_name = folder_path / PYTHON_PACKAGES , root_dir = self . python_packages , format = 'zip' , verbose = True , logger = _logger ) def _check_if_index_url_is_set_to_pytorch_cpu ( self ): if self . index_url is None : return False if self . index_url . strip () . startswith ( \"--index-url\" ) and _PYTORCH_CPU_REPO_URL in self . index_url : return True return False def enable_dependency_optimization ( self ): self . optimize_dependencies = True def disable_dependency_optimization ( self ): self . optimize_dependencies = False def validate ( self ): for spec in self . dependencies . values (): _check_package_for_dependency_limitations ( spec . name ) found_gpu_dependency = any ( dep in _GPU_DEPENDENCIES for dep in self . dependencies ) if not found_gpu_dependency : return if self . optimize_dependencies : if self . index_url is None : self . index_url = f \"--index-url { _PYTORCH_CPU_REPO_URL } \" added_pypi_warning = \"\" if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url { _PYPI_REPO_URL } \" ) added_pypi_warning = ADDED_PYPI_WARNING_MSG _logger . warning ( f \"WARNING! { REPO_MODIFICATION_WARNING_MSG } { added_pypi_warning } \" ) elif self . _check_if_index_url_is_set_to_pytorch_cpu (): if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url { _PYPI_REPO_URL } \" ) _logger . warning ( f \"WARNING! { REPO_MODIFICATION_WARNING_MSG } { ADDED_PYPI_WARNING_MSG } \" ) else : user_defined_index_url = self . index_url . replace ( \"--index-url\" , \"--extra-index-url\" , 1 ) self . index_url = f \"--index-url { _PYTORCH_CPU_REPO_URL } \" self . extra_index . insert ( 0 , user_defined_index_url ) _logger . warning ( f \"WARNING! { REPO_MODIFICATION_WARNING_MSG } { INDEX_URL_MOVED_WARNING_MSG } \" ) else : if not self . _check_if_index_url_is_set_to_pytorch_cpu (): _logger . warning ( \"WARNING! The resulting package could contain unused GPU dependencies \" \"which considerably increase the file size.\" )","title":"Module simaticai.packaging.python_dependencies"},{"location":"reference/simaticai/packaging/python_dependencies.html#variables","text":"ADDED_PYPI_WARNING_MSG INDEX_URL_MOVED_WARNING_MSG PYTHON_PACKAGES REPO_MODIFICATION_WARNING_MSG REQUIREMENTS_TXT","title":"Variables"},{"location":"reference/simaticai/packaging/python_dependencies.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/packaging/python_dependencies.html#pythondependencies","text":"class PythonDependencies ( python_version = '3.11' , dir : Union [ str , os . PathLike ] = None ) View Source class PythonDependencies () : def __init__ ( self , python_version = '3.11' , dir : Union [ str, os.PathLike ] = None ) : \"\"\" This class handles Python dependencies Dependencies from remote repositories can be added via a requirements.txt file, or by calling the add_dependencies method. Dependencies can also be added as a single wheel or source distribution file, or as a collection in a zip archive via the add_python_packages method. The class can be converted to string, which will contain the dependencies in PEP508 format. \"\"\" self . python_version = python_version self . dependencies = {} self . python_packages = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"dependencies_\" )) self . temp = Path ( tempfile . mkdtemp ( dir = dir , prefix = \"temp_\" )) self . optimize_dependencies = True self . index_url = None self . extra_index = [] def __str__ ( self ) : \"\"\" PEP508 representation of the dependencies. \"\"\" result = \"\" if self . index_url is not None : result += '# Index URL\\n' result += f \"{self.index_url}\\n\" if len ( self . extra_index ) > 0 : result += \"# Extra index urls\\n\" for url in self . extra_index : result += f \"{url}\\n\" result += \"# Runtime dependencies\\n\" for spec in self . dependencies . values () : result += str ( spec ) + \"\\n\" return result def __repr__ ( self ) : return self . __str__ () def clear ( self ) : self . dependencies . clear () self . extra_index = [] self . index_url = None shutil . rmtree ( self . python_packages , ignore_errors = True ) shutil . rmtree ( self . temp , ignore_errors = True ) self . python_packages . mkdir ( mode = 0 o700 , exist_ok = True ) self . temp . mkdir ( mode = 0 o700 , exist_ok = True ) _logger . warning ( \"Previously added dependencies have been removed.\" ) def set_requirements ( self , requirements_path : Union [ str, os.PathLike ] ) : self . clear () dependencies , extra_index , index_url = pep508 . parse_requirements ( requirements_path ) for name , spec in dependencies . items () : _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : self . dependencies [ name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if index_url is not None : self . index_url = index_url _logger . info ( f \"Index url added: {index_url}\" ) for url in extra_index : self . extra_index . append ( url ) _logger . info ( f \"Extra index url added: {url}\" ) def add_dependencies ( self , packages : list ) : for package in packages : if isinstance ( package , tuple ) : name , version = package spec = pep508 . parse_line ( f \"{name}=={version}\" ) else : spec = pep508 . parse_line ( f \"{package}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : self . dependencies [ spec.name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) def add_python_packages ( self , path : Union [ str, os.PathLike ] ) -> None : path : Path = Path ( path ) if not path . is_file () : raise AssertionError ( f \"The file must be available on path {path.resolve()}\" ) specs = [] tmp = None if is_wheel_file ( path ) : name , version = get_wheel_name_version ( path ) specs . append (( name , version , path )) elif is_pure_python_source ( path ) : name , version = get_sdist_name_version ( path ) specs . append (( name , version , path )) elif zipfile . is_zipfile ( path ) : tmp = Path ( tempfile . mkdtemp ( dir = self . temp )) zip = zipfile . ZipFile ( path ) for pkg in zip . namelist () : zip . extract ( pkg , path = tmp ) file = tmp / Path ( pkg ). name if is_wheel_file ( file ) : name , version = get_wheel_name_version ( file ) specs . append (( name , version , file )) elif is_pure_python_source ( file ) : name , version = get_sdist_name_version ( file ) specs . append (( name , version , file )) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source: {pkg}\" ) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source or a zip file: {path}\" ) for name , version , path in specs : if name is not None : if version is None : spec = pep508 . parse_line ( f \"{name}\" ) else : spec = pep508 . parse_line ( f \"{name}=={version}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : shutil . copy ( path , self . python_packages ) self . dependencies [ spec.name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if tmp is not None : shutil . rmtree ( tmp , ignore_errors = True ) def _download_or_copy_dependency ( self , name , version ) : dependency_url = urllib . parse . urlparse ( version ) # raises ValueError if the url is invalid dependency_path = Path ( urllib . parse . unquote ( dependency_url . path )) filename = dependency_path . name if \"file\" == dependency_url . scheme : # Possible Exceptions here : FileNotFoundError , PermissionError , OSError , IsADirectoryError , SameFileError if not dependency_path . is_file () : raise FileNotFoundError ( f \"The dependency '{name}' can not be found on path '{dependency_path}'\" ) if ( self . python_packages / filename ). is_file () : _logger . warning ( f \"Dependency '{name}' will not be copied because it already exists in '{self.python_packages}' folder.\" ) else : _logger . info ( f \"Dependency '{name}' will be copied to '{self.python_packages}' folder.\" ) shutil . copy ( dependency_path . resolve (), self . python_packages ) else : # Possible Exceptions here : requests . exceptions . RequestException _logger . info ( f \"Dependency '{name}@{version}' will be downloaded from the repository.\" ) response = requests . get ( version ) response . raise_for_status () with open ( self . python_packages / filename , \"wb\" ) as f : f . write ( response . content ) return self . python_packages / filename def save ( self , folder_path ) : # Downloads dependencies specified with url from remote repositories or copies them from local file system # Does not work with source distributed packages for name , dependency in self . dependencies . copy (). items () : if isinstance ( dependency . version , str ) : try : path = self . _download_or_copy_dependency ( name , dependency . version ) _ , version = get_wheel_name_version ( path ) self . dependencies [ name ] = pep508 . parse_line ( f \"{name}=={version}\" ) except requests . exceptions . RequestException as request_error : raise RuntimeError ( f \"Failed to download dependency '{dependency.name}=={dependency.version}' from the repository.\" ) from request_error requirements_file_path = folder_path / REQUIREMENTS_TXT with open ( requirements_file_path , \"w\" ) as f : f . write ( str ( self )) shutil . make_archive ( base_name = folder_path / PYTHON_PACKAGES , root_dir = self . python_packages , format = 'zip' , verbose = True , logger = _logger ) def _check_if_index_url_is_set_to_pytorch_cpu ( self ) : if self . index_url is None : return False if self . index_url . strip (). startswith ( \"--index-url\" ) and _PYTORCH_CPU_REPO_URL in self . index_url : return True return False def enable_dependency_optimization ( self ) : self . optimize_dependencies = True def disable_dependency_optimization ( self ) : self . optimize_dependencies = False def validate ( self ) : for spec in self . dependencies . values () : _check_package_for_dependency_limitations ( spec . name ) found_gpu_dependency = any ( dep in _GPU_DEPENDENCIES for dep in self . dependencies ) if not found_gpu_dependency : return if self . optimize_dependencies : if self . index_url is None : self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" added_pypi_warning = \"\" if not any ( [ _PYPI_REPO_URL in item for item in self.extra_index ] ) : self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) added_pypi_warning = ADDED_PYPI_WARNING_MSG _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {added_pypi_warning}\" ) elif self . _check_if_index_url_is_set_to_pytorch_cpu () : if not any ( [ _PYPI_REPO_URL in item for item in self.extra_index ] ) : self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {ADDED_PYPI_WARNING_MSG}\" ) else : user_defined_index_url = self . index_url . replace ( \"--index-url\" , \"--extra-index-url\" , 1 ) self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" self . extra_index . insert ( 0 , user_defined_index_url ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {INDEX_URL_MOVED_WARNING_MSG}\" ) else : if not self . _check_if_index_url_is_set_to_pytorch_cpu () : _logger . warning ( \"WARNING! The resulting package could contain unused GPU dependencies \" \"which considerably increase the file size.\" )","title":"PythonDependencies"},{"location":"reference/simaticai/packaging/python_dependencies.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/packaging/python_dependencies.html#add_dependencies","text":"def add_dependencies ( self , packages : list ) View Source def add_dependencies ( self , packages : list ): for package in packages : if isinstance ( package , tuple ): name , version = package spec = pep508 . parse_line ( f \"{name}=={version}\" ) else : spec = pep508 . parse_line ( f \"{package}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" )","title":"add_dependencies"},{"location":"reference/simaticai/packaging/python_dependencies.html#add_python_packages","text":"def add_python_packages ( self , path : Union [ str , os . PathLike ] ) -> None View Source def add_python_packages ( self , path : Union [ str , os . PathLike ]) -> None : path : Path = Path ( path ) if not path . is_file (): raise AssertionError ( f \"The file must be available on path {path.resolve()}\" ) specs = [] tmp = None if is_wheel_file ( path ): name , version = get_wheel_name_version ( path ) specs . append (( name , version , path )) elif is_pure_python_source ( path ): name , version = get_sdist_name_version ( path ) specs . append (( name , version , path )) elif zipfile . is_zipfile ( path ): tmp = Path ( tempfile . mkdtemp ( dir = self . temp )) zip = zipfile . ZipFile ( path ) for pkg in zip . namelist (): zip . extract ( pkg , path = tmp ) file = tmp / Path ( pkg ). name if is_wheel_file ( file ): name , version = get_wheel_name_version ( file ) specs . append (( name , version , file )) elif is_pure_python_source ( file ): name , version = get_sdist_name_version ( file ) specs . append (( name , version , file )) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source: {pkg}\" ) else : _logger . warning ( f \"File skipped because it is not a wheel or pure python source or a zip file: {path}\" ) for name , version , path in specs : if name is not None : if version is None : spec = pep508 . parse_line ( f \"{name}\" ) else : spec = pep508 . parse_line ( f \"{name}=={version}\" ) _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ): shutil . copy ( path , self . python_packages ) self . dependencies [ spec . name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if tmp is not None : shutil . rmtree ( tmp , ignore_errors = True )","title":"add_python_packages"},{"location":"reference/simaticai/packaging/python_dependencies.html#clear","text":"def clear ( self ) View Source def clear ( self ): self . dependencies . clear () self . extra_index = [] self . index_url = None shutil . rmtree ( self . python_packages , ignore_errors = True ) shutil . rmtree ( self . temp , ignore_errors = True ) self . python_packages . mkdir ( mode = 0o700 , exist_ok = True ) self . temp . mkdir ( mode = 0o700 , exist_ok = True ) _logger . warning ( \"Previously added dependencies have been removed.\" )","title":"clear"},{"location":"reference/simaticai/packaging/python_dependencies.html#disable_dependency_optimization","text":"def disable_dependency_optimization ( self ) View Source def disable_dependency_optimization(self): self.optimize_dependencies = False","title":"disable_dependency_optimization"},{"location":"reference/simaticai/packaging/python_dependencies.html#enable_dependency_optimization","text":"def enable_dependency_optimization ( self ) View Source def enable_dependency_optimization(self): self.optimize_dependencies = True","title":"enable_dependency_optimization"},{"location":"reference/simaticai/packaging/python_dependencies.html#save","text":"def save ( self , folder_path ) View Source def save ( self , folder_path ) : # Downloads dependencies specified with url from remote repositories or copies them from local file system # Does not work with source distributed packages for name , dependency in self . dependencies . copy (). items () : if isinstance ( dependency . version , str ) : try : path = self . _download_or_copy_dependency ( name , dependency . version ) _ , version = get_wheel_name_version ( path ) self . dependencies [ name ] = pep508 . parse_line ( f \"{name}=={version}\" ) except requests . exceptions . RequestException as request_error : raise RuntimeError ( f \"Failed to download dependency '{dependency.name}=={dependency.version}' from the repository.\" ) from request_error requirements_file_path = folder_path / REQUIREMENTS_TXT with open ( requirements_file_path , \"w\" ) as f : f . write ( str ( self )) shutil . make_archive ( base_name = folder_path / PYTHON_PACKAGES , root_dir = self . python_packages , format = 'zip' , verbose = True , logger = _logger )","title":"save"},{"location":"reference/simaticai/packaging/python_dependencies.html#set_requirements","text":"def set_requirements ( self , requirements_path : Union [ str , os . PathLike ] ) View Source def set_requirements ( self , requirements_path : Union [ str, os.PathLike ] ) : self . clear () dependencies , extra_index , index_url = pep508 . parse_requirements ( requirements_path ) for name , spec in dependencies . items () : _check_package_for_dependency_limitations ( spec . name ) if not any ( spec . name . lower () == dep . lower () for dep in self . dependencies ) : self . dependencies [ name ] = spec _logger . info ( f \"Runtime dependency added: {spec}\" ) else : _logger . warning ( f \"Dependency already exists: {spec}\" ) if index_url is not None : self . index_url = index_url _logger . info ( f \"Index url added: {index_url}\" ) for url in extra_index : self . extra_index . append ( url ) _logger . info ( f \"Extra index url added: {url}\" )","title":"set_requirements"},{"location":"reference/simaticai/packaging/python_dependencies.html#validate","text":"def validate ( self ) View Source def validate ( self ): for spec in self . dependencies . values (): _check_package_for_dependency_limitations ( spec . name ) found_gpu_dependency = any ( dep in _GPU_DEPENDENCIES for dep in self . dependencies ) if not found_gpu_dependency : return if self . optimize_dependencies : if self . index_url is None : self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" added_pypi_warning = \"\" if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) added_pypi_warning = ADDED_PYPI_WARNING_MSG _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {added_pypi_warning}\" ) elif self . _check_if_index_url_is_set_to_pytorch_cpu (): if not any ([ _PYPI_REPO_URL in item for item in self . extra_index ]): self . extra_index . insert ( 0 , f \"--extra-index-url {_PYPI_REPO_URL}\" ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {ADDED_PYPI_WARNING_MSG}\" ) else : user_defined_index_url = self . index_url . replace ( \"--index-url\" , \"--extra-index-url\" , 1 ) self . index_url = f \"--index-url {_PYTORCH_CPU_REPO_URL}\" self . extra_index . insert ( 0 , user_defined_index_url ) _logger . warning ( f \"WARNING! {REPO_MODIFICATION_WARNING_MSG} {INDEX_URL_MOVED_WARNING_MSG}\" ) else : if not self . _check_if_index_url_is_set_to_pytorch_cpu (): _logger . warning ( \"WARNING! The resulting package could contain unused GPU dependencies \" \"which considerably increase the file size.\" )","title":"validate"},{"location":"reference/simaticai/packaging/wheelhouse.html","text":"Module simaticai.packaging.wheelhouse Methods for downloading and validating dependencies This module collects all the necessary methods for downloading wheel or source distributions, and validation methods for checking if the whole collection could be installed in the AI Inference Server's Python runtime environment. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Methods for downloading and validating dependencies This module collects all the necessary methods for downloading wheel or source distributions, and validation methods for checking if the whole collection could be installed in the AI Inference Server's Python runtime environment. \"\"\" import os import subprocess import sys import shutil import zipfile import tarfile import tempfile import json import logging from pathlib import Path from itertools import chain from typing import Union from textwrap import dedent from email.parser import Parser from simaticai.helpers import pep508 from .constants import PLATFORMS , REQUIREMENTS_TXT logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _ERROR_LINE = \"ERROR: No matching distribution found for \" report_json_path = lambda tmp_dir : Path ( tmp_dir ) / \"report.json\" LIMITED_PACKAGES = { \"tensorflow\" : \"WARNING: TensorFlow is imported. For better performance with an already \" \"trained model, consider using TensorFlow Lite (tflite) instead.\" , \"opencv_python\" : \"WARNING: opencv-python is currently not supported by AI Inference Server, \" \"please use opencv-python-headless instead.\" , } def assert_none_parameters ( ** kwargs ): \"\"\" Checks if any of the given parameters are None. Returns: True if all parameters are not None, Raises: AssertionError: otherwise. \"\"\" none_values = [ k for k , v in kwargs . items () if v is None ] if 0 < len ( none_values ): none_values = \", \" . join ( none_values ) raise AssertionError ( f \"Parameters can not be None: { none_values } \" ) return True def is_wheel_file ( path : os . PathLike ) -> bool : \"\"\" Checks whether the file on the given `path` is a wheel file. Args: path (path-like): The relative or absolute path of the wheel file. Returns: bool: True if the zipfile contains a WHEEL text file, False otherwise. \"\"\" if zipfile . is_zipfile ( path ): _wheel = zipfile . ZipFile ( path ) return 'WHEEL' in [ f . split ( \"/\" )[ - 1 ] for f in _wheel . namelist ()] return False def is_source_file ( path : os . PathLike ) -> bool : \"\"\" Checks whether the file on the given `path` is a python source distribtion file. Args: path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains a PKG-INFO text file, False otherwise. \"\"\" if zipfile . is_zipfile ( path ): _archive = zipfile . ZipFile ( path ) return 'PKG-INFO' in [ f . split ( \"/\" )[ - 1 ] for f in _archive . namelist ()] if tarfile . is_tarfile ( path ): with tarfile . open ( path ) as _archive : return 'PKG-INFO' in [ f . split ( \"/\" )[ - 1 ] for f in _archive . getnames ()] return False def _extract_pkg_info ( archive_path : Union [ str , os . PathLike ]): if tarfile . is_tarfile ( archive_path ): archive = tarfile . open ( archive_path , \"r\" ) files = archive . getnames () get_text = lambda filename : archive . extractfile ( filename ) . read () . decode ( \"utf-8\" ) elif zipfile . is_zipfile ( archive_path ): archive = zipfile . ZipFile ( archive_path , \"r\" ) files = archive . namelist () get_text = lambda filename : archive . read ( filename ) . decode ( \"utf-8\" ) else : return None PKG_INFO = list ( filter ( lambda filepath : filepath . endswith ( 'PKG-INFO' ), files )) headers = None if 0 < len ( PKG_INFO ): headers = map ( get_text , PKG_INFO ) headers = map ( lambda txt : Parser () . parsestr ( text = txt , headersonly = True ), headers ) headers = list ( headers ) archive . close () return headers def is_pure_python_source ( archive_path : Union [ str , os . PathLike ]): \"\"\" Checks whether the given source distribution contains only Python sources. This method handles source distributions in a unified way. It searches for 'PKG-INFO' files, collects the programming languages used in the source, and returns True if only the Python language was used. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains only Python sources. \"\"\" headers = _extract_pkg_info ( archive_path ) if headers is None : return False classifiers = map ( lambda header : header . get_all ( \"classifier\" ), headers ) classifiers = map ( lambda classifier : [] if classifier is None else classifier , classifiers ) classifiers = chain . from_iterable ( classifiers ) programming_languages = filter ( lambda line : line . startswith ( 'Programming Language ::' ), classifiers ) programming_languages = map ( lambda line : line . split ( \"::\" )[ 1 ] . strip () . lower (), programming_languages ) return all ( map ( lambda txt : txt == 'python' , programming_languages )) def get_wheel_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a wheel file. Args: archive_path (path-like): The relative or absolute path of the wheel archive file. Returns: (str, str): The name and version of the wheel if successful, (None,None) otherwise. \"\"\" with zipfile . ZipFile ( archive_path , \"r\" ) as archive : files = archive . namelist () METADATA = list ( filter ( lambda filepath : filepath . endswith ( 'METADATA' ), files )) if 0 < len ( METADATA ): headers = map ( lambda filename : archive . read ( filename ) . decode ( \"utf-8\" ), METADATA ) headers = map ( lambda txt : Parser () . parsestr ( text = txt , headersonly = True ), headers ) headers = list ( headers )[ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version ) return ( None , None ) def get_sdist_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a source distribution file. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: (str, str): The name and version of the source distribution if successful, (None,None) otherwise. \"\"\" headers = _extract_pkg_info ( archive_path ) if headers is None : return ( None , None ) headers = headers [ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version ) def _pip_download_platform_wheels ( requirements_file_path : Path , python_version : str , python_packages_folder : Path , extra_index = []): assert_none_parameters ( requirements_file_path = requirements_file_path , python_version = python_version , python_packages_folder = python_packages_folder ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"download\" , \"--no-color\" , \"-r\" , f \" { requirements_file_path . resolve () } \" , \"-d\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--python-version\" , f \" { python_version } \" , \"--only-binary=:all:\" , \"--no-binary=:none:\" ] for platform in PLATFORMS : command_line += [ \"--platform\" , platform ] for i in extra_index : parts = i . split ( \" \" ) command_line += parts return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def _pip_download_source_dist ( requirements_file_path : Path , python_packages_folder : Path , extra_index ): assert_none_parameters ( requirements_file_path = requirements_file_path , python_packages_folder = python_packages_folder ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"download\" , \"--no-color\" , \"-r\" , f \" { requirements_file_path . resolve () } \" , \"setuptools\" , \"-d\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--no-deps\" , ] for i in extra_index : parts = i . split ( \" \" ) command_line += parts return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def _pip_get_source_dependencies ( requirements_file_path : Path , python_packages_folder : Path , report_path : Path , extra_index ): assert_none_parameters ( requirements_file_path = requirements_file_path , python_packages_folder = python_packages_folder , report_path = report_path ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"install\" , \"-r\" , f \" { requirements_file_path . resolve () } \" , \"--no-color\" , \"--target\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--dry-run\" , \"--ignore-installed\" , \"--force-reinstall\" , \"--report\" , f \" { report_path . resolve () } \" , ] for i in extra_index : parts = i . split ( \" \" ) command_line += parts return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def _pip_dry_install_packages ( python_version : str , python_packages_folder : Path , report_path : Path ): assert_none_parameters ( python_version = python_version , python_packages_folder = python_packages_folder , ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"install\" , \"--no-color\" , \"--dry-run\" , \"--ignore-installed\" , \"--force-reinstall\" , \"--no-index\" , \"--target\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--python-version\" , f \" { python_version } \" , \"--only-binary=:all:\" , \"--no-binary=:none:\" , \"--report\" , f \" { report_path . resolve () } \" , ] for platform in PLATFORMS : command_line += [ \"--platform\" , platform ] command_line += [ str ( f . resolve ()) for f in Path ( python_packages_folder ) . iterdir () ] return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder : Union [ str , os . PathLike ]): \"\"\" Checks all files in a directory if they are wheel files or pure Python source distributions. Args: python_packages_folder (path-like): The relative or absolute path of the directory to be checked. Raises: AssertionError: If the directory contains other files than wheels or pure Python source distributions. \"\"\" not_pure = [] for file in list ( python_packages_folder . iterdir ()): if not ( is_wheel_file ( file ) or is_pure_python_source ( file )): not_pure . append ( file . name ) if 0 < len ( not_pure ): not_pure = \" \\n \" . join ( not_pure ) raise AssertionError ( dedent ( f \"\"\" One or more source dependencies are not pure Python sources. You need to convert them to wheel files for the target platform manually. List of not pure Python source distributions: { not_pure } \"\"\" )) def remove_setuptools ( python_packages_folder ): filename = [ f for f in Path ( python_packages_folder ) . glob ( \"setuptools*\" )] if len ( filename ): filename [ 0 ] . unlink () def consistency_check ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], report_path : Union [ str , os . PathLike ], check_pure : bool = True ): result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) _check_report_for_dependency_limitations ( report_path ) remove_setuptools ( python_packages_folder ) if 0 != result . returncode : raise AssertionError ( f \"Dependency checking failed for file ' { requirements_file_path } '. \\n { result . stderr } \" ) if check_pure : check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder ) def _extract_transitive_dependency_info ( requirements , sources , src_path , python_packages_folder , tmp , extra_index ): new_requirements = {} report_json = report_json_path ( tmp ) report_json . unlink ( missing_ok = True ) result = _pip_get_source_dependencies ( src_path , python_packages_folder , report_json , extra_index ) if 0 == result . returncode : report = json . loads ( report_json . read_text ()) for dep in report [ 'install' ]: name = dep [ 'metadata' ][ 'name' ] vers = dep [ 'metadata' ][ 'version' ] if ( name not in sources . keys ()) and ( name not in requirements . keys ()): new_requirements [ name ] = pep508 . parse_line ( f \" { name } == { vers } \" ) return new_requirements def _separate_wheels_and_sdists ( requirements : dict , sources : dict , python_version : str , python_packages_folder : Union [ str , os . PathLike ], req_path , src_path , extra_index ): has_src_dep = 0 < len ( requirements . values ()) counter = 100 while has_src_dep : if ( 0 == counter ): raise AssertionError ( \"It looks like the specified requirements have caused an infinite download loop. Terminating.\" ) counter -= 1 with open ( req_path , \"w\" ) as f : for spec in requirements . values (): f . write ( str ( spec ) + \" \\n \" ) with open ( src_path , \"w\" ) as f : for spec in sources . values (): f . write ( str ( spec ) + \" \\n \" ) # Try to download requirements for the target platform, and see what fails result = _pip_download_platform_wheels ( req_path , python_version , python_packages_folder , extra_index ) if 0 == result . returncode : has_src_dep = False else : for line in result . stderr . split ( \" \\n \" ): if _ERROR_LINE in line : spec_line = line . split ( _ERROR_LINE )[ 1 ] spec_line = spec_line . split ( \" \\x1b \" )[ 0 ] . strip () spec : pep508 . Spec = pep508 . parse_line ( spec_line ) sources [ spec . name ] = requirements . pop ( spec . name ) has_src_dep = 0 < len ( requirements . values ()) with open ( req_path , \"w\" ) as f : for spec in requirements . values (): f . write ( str ( spec ) + \" \\n \" ) with open ( src_path , \"w\" ) as f : for spec in sources . values (): f . write ( str ( spec ) + \" \\n \" ) return ( requirements , sources ) def _compose_dependencies ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], check_pure : bool = True ): \"\"\" Given a requirements.txt with some source distributions specified, this method - separates wheel and source dependencies - downloads platform specific wheels - downloads the source distributions without their dependencies - runs a dry install on the requirements collected so far - processes the report.json for further dependencies - repeats the whole process until all source and platform specific wheels are downloaded - runs a dry install on the package directory to check if it would install - checks if all the source distributions are pure python sources - raises an error if there are sources in other language \"\"\" tmp = Path ( tempfile . mkdtemp ()) req_path = tmp / REQUIREMENTS_TXT src_path = tmp / \"sources.txt\" report_path = report_json_path ( tmp ) req_path . touch ( exist_ok = True ) src_path . touch ( exist_ok = True ) report_path . touch ( exist_ok = True ) try : requirements , extra_index , index_url = pep508 . parse_requirements ( requirements_file_path ) if index_url is not None : extra_index . append ( index_url ) sources = {} has_src_transitive_dep = True while has_src_transitive_dep : requirements , sources = _separate_wheels_and_sdists ( requirements , sources , python_version , python_packages_folder , req_path , src_path , extra_index ) if 0 == len ( sources . values ()): has_src_transitive_dep = False else : result = _pip_download_source_dist ( src_path , python_packages_folder , extra_index ) if 0 != result . returncode : raise AssertionError ( f \"Requirements file ' { requirements_file_path } ' contains invalid dependency specifications: \\n { result . stderr } \" ) new_requirements = _extract_transitive_dependency_info ( requirements , sources , src_path , python_packages_folder , tmp , extra_index ) if 0 == len ( new_requirements . keys ()): has_src_transitive_dep = False else : for name , spec in new_requirements . items (): requirements [ name ] = spec consistency_check ( requirements_file_path , python_version , python_packages_folder , report_path , check_pure ) finally : shutil . rmtree ( tmp , ignore_errors = True ) def _download_only_wheels_if_possible ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ]): \"\"\" @Deprecated, reason: This method can be removed in SDK 2.0, since the `separate_wheels_and_sdists` method does the same thing, if there are no sdist dependencies. Kept only for preserving backward compatibility. \"\"\" result = _pip_download_platform_wheels ( requirements_file_path , python_version , python_packages_folder ) src_dep = False for line in result . stderr . split ( \" \\n \" ): if _ERROR_LINE in line : src_dep = True break if ( 0 != result . returncode ) and not src_dep : _logger . warning ( f \"Downloading wheels failed, reason: \\n { result . stderr } \" ) raise RuntimeError ( f \"Downloading wheels failed, reason: \\n { result . stderr } \" ) return 0 == result . returncode def _check_package_for_dependency_limitations ( package_name : str ): if package_name is None : return limited_package_message = LIMITED_PACKAGES . get ( package_name . replace ( '-' , '_' ) . lower ()) if limited_package_message : _logger . warning ( limited_package_message ) def _check_report_for_dependency_limitations ( report_path : Union [ str , os . PathLike ]): \"\"\" Checks the report.json file for dependency limitations. \"\"\" report = json . loads ( report_path . read_text ( encoding = 'utf-8' ) or \" {} \" ) for dep in report . get ( 'install' , []): name = dep [ 'metadata' ][ 'name' ] _check_package_for_dependency_limitations ( name ) def create_wheelhouse ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ]): dependency_set = set () # try the easy way success = _download_only_wheels_if_possible ( requirements_file_path , python_version , python_packages_folder ) if success : if not any ( python_packages_folder . iterdir ()): return dependency_set try : tmp = Path ( tempfile . mkdtemp ()) report_path = report_json_path ( tmp ) result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) if 0 != result . returncode : raise RuntimeError ( f \"Dry install failed, reason: \\n { result . stderr } \" ) _check_report_for_dependency_limitations ( report_path ) finally : shutil . rmtree ( tmp , ignore_errors = True ) else : # let's do this the hard way try : _compose_dependencies ( requirements_file_path , python_version , python_packages_folder ) except AssertionError as error : raise RuntimeError ( f \"Downloading wheels and source distributions failed, reason: \\n { str ( error ) } \" ) for package in python_packages_folder . iterdir (): if is_source_file ( package ): dependency_set . add ( get_sdist_name_version ( package )) elif is_wheel_file ( package ): dependency_set . add ( get_wheel_name_version ( package )) else : # TODO what to do when there is a different kind of file in the packages folder? pass return dependency_set Variables LIMITED_PACKAGES PLATFORMS REQUIREMENTS_TXT Functions assert_none_parameters def assert_none_parameters ( ** kwargs ) Checks if any of the given parameters are None. Returns: Type Description None True if all parameters are not None, Raises: Type Description AssertionError otherwise. View Source def assert_none_parameters(**kwargs): \"\"\" Checks if any of the given parameters are None. Returns: True if all parameters are not None, Raises: AssertionError: otherwise. \"\"\" none_values = [k for k, v in kwargs.items() if v is None] if 0 < len(none_values): none_values = \", \".join(none_values) raise AssertionError(f\"Parameters can not be None: {none_values}\") return True check_directory_has_only_wheels_and_pure_sdist def check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder : Union [ str , os . PathLike ] ) Checks all files in a directory if they are wheel files or pure Python source distributions. Parameters: Name Type Description Default python_packages_folder path-like The relative or absolute path of the directory to be checked. None Raises: Type Description AssertionError If the directory contains other files than wheels or pure Python source distributions. View Source def check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder : Union [ str , os . PathLike ]): \"\"\" Checks all files in a directory if they are wheel files or pure Python source distributions. Args: python_packages_folder (path-like): The relative or absolute path of the directory to be checked. Raises: AssertionError: If the directory contains other files than wheels or pure Python source distributions. \"\"\" not_pure = [] for file in list ( python_packages_folder . iterdir ()): if not ( is_wheel_file ( file ) or is_pure_python_source ( file )): not_pure . append ( file . name ) if 0 < len ( not_pure ): not_pure = \"\\n\" . join ( not_pure ) raise AssertionError ( dedent ( f \"\"\" One or more source dependencies are not pure Python sources. You need to convert them to wheel files for the target platform manually. List of not pure Python source distributions: {not_pure} \"\"\" )) consistency_check def consistency_check ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], report_path : Union [ str , os . PathLike ], check_pure : bool = True ) View Source def consistency_check ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], report_path : Union [ str , os . PathLike ], check_pure : bool = True ): result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) _check_report_for_dependency_limitations ( report_path ) remove_setuptools ( python_packages_folder ) if 0 != result . returncode : raise AssertionError ( f \"Dependency checking failed for file '{requirements_file_path}'. \\n {result.stderr}\" ) if check_pure : check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder ) create_wheelhouse def create_wheelhouse ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ] ) View Source def create_wheelhouse ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ]): dependency_set = set () # try the easy way success = _download_only_wheels_if_possible ( requirements_file_path , python_version , python_packages_folder ) if success : if not any ( python_packages_folder . iterdir ()): return dependency_set try : tmp = Path ( tempfile . mkdtemp ()) report_path = report_json_path ( tmp ) result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) if 0 != result . returncode : raise RuntimeError ( f \"Dry install failed, reason: \\n {result.stderr}\" ) _check_report_for_dependency_limitations ( report_path ) finally : shutil . rmtree ( tmp , ignore_errors = True ) else : # let's do this the hard way try : _compose_dependencies ( requirements_file_path , python_version , python_packages_folder ) except AssertionError as error : raise RuntimeError ( f \"Downloading wheels and source distributions failed, reason: \\n {str(error)}\" ) for package in python_packages_folder . iterdir (): if is_source_file ( package ): dependency_set . add ( get_sdist_name_version ( package )) elif is_wheel_file ( package ): dependency_set . add ( get_wheel_name_version ( package )) else : # TODO what to do when there is a different kind of file in the packages folder? pass return dependency_set get_sdist_name_version def get_sdist_name_version ( archive_path : Union [ str , os . PathLike ] ) Extracts the package name and version from a source distribution file. Parameters: Name Type Description Default archive_path path-like The relative or absolute path of the zip or tar.gz archive file. None Returns: Type Description None (str, str): The name and version of the source distribution if successful, (None,None) otherwise. View Source def get_sdist_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a source distribution file. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: (str, str): The name and version of the source distribution if successful, (None,None) otherwise. \"\"\" headers = _extract_pkg_info ( archive_path ) if headers is None : return ( None , None ) headers = headers [ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version ) get_wheel_name_version def get_wheel_name_version ( archive_path : Union [ str , os . PathLike ] ) Extracts the package name and version from a wheel file. Parameters: Name Type Description Default archive_path path-like The relative or absolute path of the wheel archive file. None Returns: Type Description None (str, str): The name and version of the wheel if successful, (None,None) otherwise. View Source def get_wheel_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a wheel file. Args: archive_path (path-like): The relative or absolute path of the wheel archive file. Returns: (str, str): The name and version of the wheel if successful, (None,None) otherwise. \"\"\" with zipfile . ZipFile ( archive_path , \"r\" ) as archive : files = archive . namelist () METADATA = list ( filter ( lambda filepath : filepath . endswith ( ' METADATA ' ), files )) if 0 < len ( METADATA ): headers = map ( lambda filename : archive . read ( filename ). decode ( \"utf-8\" ), METADATA ) headers = map ( lambda txt : Parser (). parsestr ( text = txt , headersonly = True ), headers ) headers = list ( headers )[ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version ) return ( None , None ) is_pure_python_source def is_pure_python_source ( archive_path : Union [ str , os . PathLike ] ) Checks whether the given source distribution contains only Python sources. This method handles source distributions in a unified way. It searches for 'PKG-INFO' files, collects the programming languages used in the source, and returns True if only the Python language was used. Parameters: Name Type Description Default archive_path path-like The relative or absolute path of the zip or tar.gz archive file. None Returns: Type Description bool True if the archive file contains only Python sources. View Source def is_pure_python_source(archive_path: Union[str, os.PathLike]): \"\"\" Checks whether the given source distribution contains only Python sources. This method handles source distributions in a unified way. It searches for 'PKG-INFO' files, collects the programming languages used in the source, and returns True if only the Python language was used. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains only Python sources. \"\"\" headers = _extract_pkg_info(archive_path) if headers is None: return False classifiers = map(lambda header: header.get_all(\"classifier\"), headers) classifiers = map(lambda classifier: [] if classifier is None else classifier, classifiers) classifiers = chain.from_iterable(classifiers) programming_languages = filter(lambda line: line.startswith('Programming Language ::'), classifiers) programming_languages = map(lambda line: line.split(\"::\")[1].strip().lower(), programming_languages) return all(map(lambda txt: txt == 'python', programming_languages)) is_source_file def is_source_file ( path : os . PathLike ) -> bool Checks whether the file on the given path is a python source distribtion file. Parameters: Name Type Description Default path path-like The relative or absolute path of the zip or tar.gz archive file. None Returns: Type Description bool True if the archive file contains a PKG-INFO text file, False otherwise. View Source def is_source_file ( path : os . PathLike ) -> bool : \" \"\" Checks whether the file on the given `path` is a python source distribtion file. Args: path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains a PKG-INFO text file, False otherwise. \"\" \" if zipfile . is_zipfile ( path ) : _archive = zipfile . ZipFile ( path ) return 'PKG-INFO' in [ f . split ( \"/\" ) [ - 1 ] for f in _archive . namelist () ] if tarfile . is_tarfile ( path ) : with tarfile . open ( path ) as _archive : return 'PKG-INFO' in [ f . split ( \"/\" ) [ - 1 ] for f in _archive . getnames () ] return False is_wheel_file def is_wheel_file ( path : os . PathLike ) -> bool Checks whether the file on the given path is a wheel file. Parameters: Name Type Description Default path path-like The relative or absolute path of the wheel file. None Returns: Type Description bool True if the zipfile contains a WHEEL text file, False otherwise. View Source def is_wheel_file ( path : os . PathLike ) -> bool : \" \"\" Checks whether the file on the given `path` is a wheel file. Args: path (path-like): The relative or absolute path of the wheel file. Returns: bool: True if the zipfile contains a WHEEL text file, False otherwise. \"\" \" if zipfile . is_zipfile ( path ) : _wheel = zipfile . ZipFile ( path ) return 'WHEEL' in [ f . split ( \"/\" ) [ - 1 ] for f in _wheel . namelist () ] return False remove_setuptools def remove_setuptools ( python_packages_folder ) View Source def remove_setuptools ( python_packages_folder ): filename = [ f for f in Path ( python_packages_folder ) . glob ( \"setuptools*\" )] if len ( filename ): filename [ 0 ] . unlink () report_json_path def report_json_path ( tmp_dir ) View Source report_json_path = lambda tmp_dir: Path(tmp_dir) / \"report.json\"","title":"Wheelhouse"},{"location":"reference/simaticai/packaging/wheelhouse.html#module-simaticaipackagingwheelhouse","text":"Methods for downloading and validating dependencies This module collects all the necessary methods for downloading wheel or source distributions, and validation methods for checking if the whole collection could be installed in the AI Inference Server's Python runtime environment. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Methods for downloading and validating dependencies This module collects all the necessary methods for downloading wheel or source distributions, and validation methods for checking if the whole collection could be installed in the AI Inference Server's Python runtime environment. \"\"\" import os import subprocess import sys import shutil import zipfile import tarfile import tempfile import json import logging from pathlib import Path from itertools import chain from typing import Union from textwrap import dedent from email.parser import Parser from simaticai.helpers import pep508 from .constants import PLATFORMS , REQUIREMENTS_TXT logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _ERROR_LINE = \"ERROR: No matching distribution found for \" report_json_path = lambda tmp_dir : Path ( tmp_dir ) / \"report.json\" LIMITED_PACKAGES = { \"tensorflow\" : \"WARNING: TensorFlow is imported. For better performance with an already \" \"trained model, consider using TensorFlow Lite (tflite) instead.\" , \"opencv_python\" : \"WARNING: opencv-python is currently not supported by AI Inference Server, \" \"please use opencv-python-headless instead.\" , } def assert_none_parameters ( ** kwargs ): \"\"\" Checks if any of the given parameters are None. Returns: True if all parameters are not None, Raises: AssertionError: otherwise. \"\"\" none_values = [ k for k , v in kwargs . items () if v is None ] if 0 < len ( none_values ): none_values = \", \" . join ( none_values ) raise AssertionError ( f \"Parameters can not be None: { none_values } \" ) return True def is_wheel_file ( path : os . PathLike ) -> bool : \"\"\" Checks whether the file on the given `path` is a wheel file. Args: path (path-like): The relative or absolute path of the wheel file. Returns: bool: True if the zipfile contains a WHEEL text file, False otherwise. \"\"\" if zipfile . is_zipfile ( path ): _wheel = zipfile . ZipFile ( path ) return 'WHEEL' in [ f . split ( \"/\" )[ - 1 ] for f in _wheel . namelist ()] return False def is_source_file ( path : os . PathLike ) -> bool : \"\"\" Checks whether the file on the given `path` is a python source distribtion file. Args: path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains a PKG-INFO text file, False otherwise. \"\"\" if zipfile . is_zipfile ( path ): _archive = zipfile . ZipFile ( path ) return 'PKG-INFO' in [ f . split ( \"/\" )[ - 1 ] for f in _archive . namelist ()] if tarfile . is_tarfile ( path ): with tarfile . open ( path ) as _archive : return 'PKG-INFO' in [ f . split ( \"/\" )[ - 1 ] for f in _archive . getnames ()] return False def _extract_pkg_info ( archive_path : Union [ str , os . PathLike ]): if tarfile . is_tarfile ( archive_path ): archive = tarfile . open ( archive_path , \"r\" ) files = archive . getnames () get_text = lambda filename : archive . extractfile ( filename ) . read () . decode ( \"utf-8\" ) elif zipfile . is_zipfile ( archive_path ): archive = zipfile . ZipFile ( archive_path , \"r\" ) files = archive . namelist () get_text = lambda filename : archive . read ( filename ) . decode ( \"utf-8\" ) else : return None PKG_INFO = list ( filter ( lambda filepath : filepath . endswith ( 'PKG-INFO' ), files )) headers = None if 0 < len ( PKG_INFO ): headers = map ( get_text , PKG_INFO ) headers = map ( lambda txt : Parser () . parsestr ( text = txt , headersonly = True ), headers ) headers = list ( headers ) archive . close () return headers def is_pure_python_source ( archive_path : Union [ str , os . PathLike ]): \"\"\" Checks whether the given source distribution contains only Python sources. This method handles source distributions in a unified way. It searches for 'PKG-INFO' files, collects the programming languages used in the source, and returns True if only the Python language was used. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains only Python sources. \"\"\" headers = _extract_pkg_info ( archive_path ) if headers is None : return False classifiers = map ( lambda header : header . get_all ( \"classifier\" ), headers ) classifiers = map ( lambda classifier : [] if classifier is None else classifier , classifiers ) classifiers = chain . from_iterable ( classifiers ) programming_languages = filter ( lambda line : line . startswith ( 'Programming Language ::' ), classifiers ) programming_languages = map ( lambda line : line . split ( \"::\" )[ 1 ] . strip () . lower (), programming_languages ) return all ( map ( lambda txt : txt == 'python' , programming_languages )) def get_wheel_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a wheel file. Args: archive_path (path-like): The relative or absolute path of the wheel archive file. Returns: (str, str): The name and version of the wheel if successful, (None,None) otherwise. \"\"\" with zipfile . ZipFile ( archive_path , \"r\" ) as archive : files = archive . namelist () METADATA = list ( filter ( lambda filepath : filepath . endswith ( 'METADATA' ), files )) if 0 < len ( METADATA ): headers = map ( lambda filename : archive . read ( filename ) . decode ( \"utf-8\" ), METADATA ) headers = map ( lambda txt : Parser () . parsestr ( text = txt , headersonly = True ), headers ) headers = list ( headers )[ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version ) return ( None , None ) def get_sdist_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a source distribution file. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: (str, str): The name and version of the source distribution if successful, (None,None) otherwise. \"\"\" headers = _extract_pkg_info ( archive_path ) if headers is None : return ( None , None ) headers = headers [ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version ) def _pip_download_platform_wheels ( requirements_file_path : Path , python_version : str , python_packages_folder : Path , extra_index = []): assert_none_parameters ( requirements_file_path = requirements_file_path , python_version = python_version , python_packages_folder = python_packages_folder ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"download\" , \"--no-color\" , \"-r\" , f \" { requirements_file_path . resolve () } \" , \"-d\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--python-version\" , f \" { python_version } \" , \"--only-binary=:all:\" , \"--no-binary=:none:\" ] for platform in PLATFORMS : command_line += [ \"--platform\" , platform ] for i in extra_index : parts = i . split ( \" \" ) command_line += parts return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def _pip_download_source_dist ( requirements_file_path : Path , python_packages_folder : Path , extra_index ): assert_none_parameters ( requirements_file_path = requirements_file_path , python_packages_folder = python_packages_folder ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"download\" , \"--no-color\" , \"-r\" , f \" { requirements_file_path . resolve () } \" , \"setuptools\" , \"-d\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--no-deps\" , ] for i in extra_index : parts = i . split ( \" \" ) command_line += parts return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def _pip_get_source_dependencies ( requirements_file_path : Path , python_packages_folder : Path , report_path : Path , extra_index ): assert_none_parameters ( requirements_file_path = requirements_file_path , python_packages_folder = python_packages_folder , report_path = report_path ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"install\" , \"-r\" , f \" { requirements_file_path . resolve () } \" , \"--no-color\" , \"--target\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--dry-run\" , \"--ignore-installed\" , \"--force-reinstall\" , \"--report\" , f \" { report_path . resolve () } \" , ] for i in extra_index : parts = i . split ( \" \" ) command_line += parts return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def _pip_dry_install_packages ( python_version : str , python_packages_folder : Path , report_path : Path ): assert_none_parameters ( python_version = python_version , python_packages_folder = python_packages_folder , ) command_line = [ sys . executable , \"-m\" , \"pip\" , \"install\" , \"--no-color\" , \"--dry-run\" , \"--ignore-installed\" , \"--force-reinstall\" , \"--no-index\" , \"--target\" , f \" { python_packages_folder . resolve () } \" , \"--find-links\" , f \" { python_packages_folder . resolve () } \" , \"--python-version\" , f \" { python_version } \" , \"--only-binary=:all:\" , \"--no-binary=:none:\" , \"--report\" , f \" { report_path . resolve () } \" , ] for platform in PLATFORMS : command_line += [ \"--platform\" , platform ] command_line += [ str ( f . resolve ()) for f in Path ( python_packages_folder ) . iterdir () ] return subprocess . run ( command_line , stderr = subprocess . PIPE , text = True ) def check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder : Union [ str , os . PathLike ]): \"\"\" Checks all files in a directory if they are wheel files or pure Python source distributions. Args: python_packages_folder (path-like): The relative or absolute path of the directory to be checked. Raises: AssertionError: If the directory contains other files than wheels or pure Python source distributions. \"\"\" not_pure = [] for file in list ( python_packages_folder . iterdir ()): if not ( is_wheel_file ( file ) or is_pure_python_source ( file )): not_pure . append ( file . name ) if 0 < len ( not_pure ): not_pure = \" \\n \" . join ( not_pure ) raise AssertionError ( dedent ( f \"\"\" One or more source dependencies are not pure Python sources. You need to convert them to wheel files for the target platform manually. List of not pure Python source distributions: { not_pure } \"\"\" )) def remove_setuptools ( python_packages_folder ): filename = [ f for f in Path ( python_packages_folder ) . glob ( \"setuptools*\" )] if len ( filename ): filename [ 0 ] . unlink () def consistency_check ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], report_path : Union [ str , os . PathLike ], check_pure : bool = True ): result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) _check_report_for_dependency_limitations ( report_path ) remove_setuptools ( python_packages_folder ) if 0 != result . returncode : raise AssertionError ( f \"Dependency checking failed for file ' { requirements_file_path } '. \\n { result . stderr } \" ) if check_pure : check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder ) def _extract_transitive_dependency_info ( requirements , sources , src_path , python_packages_folder , tmp , extra_index ): new_requirements = {} report_json = report_json_path ( tmp ) report_json . unlink ( missing_ok = True ) result = _pip_get_source_dependencies ( src_path , python_packages_folder , report_json , extra_index ) if 0 == result . returncode : report = json . loads ( report_json . read_text ()) for dep in report [ 'install' ]: name = dep [ 'metadata' ][ 'name' ] vers = dep [ 'metadata' ][ 'version' ] if ( name not in sources . keys ()) and ( name not in requirements . keys ()): new_requirements [ name ] = pep508 . parse_line ( f \" { name } == { vers } \" ) return new_requirements def _separate_wheels_and_sdists ( requirements : dict , sources : dict , python_version : str , python_packages_folder : Union [ str , os . PathLike ], req_path , src_path , extra_index ): has_src_dep = 0 < len ( requirements . values ()) counter = 100 while has_src_dep : if ( 0 == counter ): raise AssertionError ( \"It looks like the specified requirements have caused an infinite download loop. Terminating.\" ) counter -= 1 with open ( req_path , \"w\" ) as f : for spec in requirements . values (): f . write ( str ( spec ) + \" \\n \" ) with open ( src_path , \"w\" ) as f : for spec in sources . values (): f . write ( str ( spec ) + \" \\n \" ) # Try to download requirements for the target platform, and see what fails result = _pip_download_platform_wheels ( req_path , python_version , python_packages_folder , extra_index ) if 0 == result . returncode : has_src_dep = False else : for line in result . stderr . split ( \" \\n \" ): if _ERROR_LINE in line : spec_line = line . split ( _ERROR_LINE )[ 1 ] spec_line = spec_line . split ( \" \\x1b \" )[ 0 ] . strip () spec : pep508 . Spec = pep508 . parse_line ( spec_line ) sources [ spec . name ] = requirements . pop ( spec . name ) has_src_dep = 0 < len ( requirements . values ()) with open ( req_path , \"w\" ) as f : for spec in requirements . values (): f . write ( str ( spec ) + \" \\n \" ) with open ( src_path , \"w\" ) as f : for spec in sources . values (): f . write ( str ( spec ) + \" \\n \" ) return ( requirements , sources ) def _compose_dependencies ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], check_pure : bool = True ): \"\"\" Given a requirements.txt with some source distributions specified, this method - separates wheel and source dependencies - downloads platform specific wheels - downloads the source distributions without their dependencies - runs a dry install on the requirements collected so far - processes the report.json for further dependencies - repeats the whole process until all source and platform specific wheels are downloaded - runs a dry install on the package directory to check if it would install - checks if all the source distributions are pure python sources - raises an error if there are sources in other language \"\"\" tmp = Path ( tempfile . mkdtemp ()) req_path = tmp / REQUIREMENTS_TXT src_path = tmp / \"sources.txt\" report_path = report_json_path ( tmp ) req_path . touch ( exist_ok = True ) src_path . touch ( exist_ok = True ) report_path . touch ( exist_ok = True ) try : requirements , extra_index , index_url = pep508 . parse_requirements ( requirements_file_path ) if index_url is not None : extra_index . append ( index_url ) sources = {} has_src_transitive_dep = True while has_src_transitive_dep : requirements , sources = _separate_wheels_and_sdists ( requirements , sources , python_version , python_packages_folder , req_path , src_path , extra_index ) if 0 == len ( sources . values ()): has_src_transitive_dep = False else : result = _pip_download_source_dist ( src_path , python_packages_folder , extra_index ) if 0 != result . returncode : raise AssertionError ( f \"Requirements file ' { requirements_file_path } ' contains invalid dependency specifications: \\n { result . stderr } \" ) new_requirements = _extract_transitive_dependency_info ( requirements , sources , src_path , python_packages_folder , tmp , extra_index ) if 0 == len ( new_requirements . keys ()): has_src_transitive_dep = False else : for name , spec in new_requirements . items (): requirements [ name ] = spec consistency_check ( requirements_file_path , python_version , python_packages_folder , report_path , check_pure ) finally : shutil . rmtree ( tmp , ignore_errors = True ) def _download_only_wheels_if_possible ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ]): \"\"\" @Deprecated, reason: This method can be removed in SDK 2.0, since the `separate_wheels_and_sdists` method does the same thing, if there are no sdist dependencies. Kept only for preserving backward compatibility. \"\"\" result = _pip_download_platform_wheels ( requirements_file_path , python_version , python_packages_folder ) src_dep = False for line in result . stderr . split ( \" \\n \" ): if _ERROR_LINE in line : src_dep = True break if ( 0 != result . returncode ) and not src_dep : _logger . warning ( f \"Downloading wheels failed, reason: \\n { result . stderr } \" ) raise RuntimeError ( f \"Downloading wheels failed, reason: \\n { result . stderr } \" ) return 0 == result . returncode def _check_package_for_dependency_limitations ( package_name : str ): if package_name is None : return limited_package_message = LIMITED_PACKAGES . get ( package_name . replace ( '-' , '_' ) . lower ()) if limited_package_message : _logger . warning ( limited_package_message ) def _check_report_for_dependency_limitations ( report_path : Union [ str , os . PathLike ]): \"\"\" Checks the report.json file for dependency limitations. \"\"\" report = json . loads ( report_path . read_text ( encoding = 'utf-8' ) or \" {} \" ) for dep in report . get ( 'install' , []): name = dep [ 'metadata' ][ 'name' ] _check_package_for_dependency_limitations ( name ) def create_wheelhouse ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ]): dependency_set = set () # try the easy way success = _download_only_wheels_if_possible ( requirements_file_path , python_version , python_packages_folder ) if success : if not any ( python_packages_folder . iterdir ()): return dependency_set try : tmp = Path ( tempfile . mkdtemp ()) report_path = report_json_path ( tmp ) result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) if 0 != result . returncode : raise RuntimeError ( f \"Dry install failed, reason: \\n { result . stderr } \" ) _check_report_for_dependency_limitations ( report_path ) finally : shutil . rmtree ( tmp , ignore_errors = True ) else : # let's do this the hard way try : _compose_dependencies ( requirements_file_path , python_version , python_packages_folder ) except AssertionError as error : raise RuntimeError ( f \"Downloading wheels and source distributions failed, reason: \\n { str ( error ) } \" ) for package in python_packages_folder . iterdir (): if is_source_file ( package ): dependency_set . add ( get_sdist_name_version ( package )) elif is_wheel_file ( package ): dependency_set . add ( get_wheel_name_version ( package )) else : # TODO what to do when there is a different kind of file in the packages folder? pass return dependency_set","title":"Module simaticai.packaging.wheelhouse"},{"location":"reference/simaticai/packaging/wheelhouse.html#variables","text":"LIMITED_PACKAGES PLATFORMS REQUIREMENTS_TXT","title":"Variables"},{"location":"reference/simaticai/packaging/wheelhouse.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/packaging/wheelhouse.html#assert_none_parameters","text":"def assert_none_parameters ( ** kwargs ) Checks if any of the given parameters are None. Returns: Type Description None True if all parameters are not None, Raises: Type Description AssertionError otherwise. View Source def assert_none_parameters(**kwargs): \"\"\" Checks if any of the given parameters are None. Returns: True if all parameters are not None, Raises: AssertionError: otherwise. \"\"\" none_values = [k for k, v in kwargs.items() if v is None] if 0 < len(none_values): none_values = \", \".join(none_values) raise AssertionError(f\"Parameters can not be None: {none_values}\") return True","title":"assert_none_parameters"},{"location":"reference/simaticai/packaging/wheelhouse.html#check_directory_has_only_wheels_and_pure_sdist","text":"def check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder : Union [ str , os . PathLike ] ) Checks all files in a directory if they are wheel files or pure Python source distributions. Parameters: Name Type Description Default python_packages_folder path-like The relative or absolute path of the directory to be checked. None Raises: Type Description AssertionError If the directory contains other files than wheels or pure Python source distributions. View Source def check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder : Union [ str , os . PathLike ]): \"\"\" Checks all files in a directory if they are wheel files or pure Python source distributions. Args: python_packages_folder (path-like): The relative or absolute path of the directory to be checked. Raises: AssertionError: If the directory contains other files than wheels or pure Python source distributions. \"\"\" not_pure = [] for file in list ( python_packages_folder . iterdir ()): if not ( is_wheel_file ( file ) or is_pure_python_source ( file )): not_pure . append ( file . name ) if 0 < len ( not_pure ): not_pure = \"\\n\" . join ( not_pure ) raise AssertionError ( dedent ( f \"\"\" One or more source dependencies are not pure Python sources. You need to convert them to wheel files for the target platform manually. List of not pure Python source distributions: {not_pure} \"\"\" ))","title":"check_directory_has_only_wheels_and_pure_sdist"},{"location":"reference/simaticai/packaging/wheelhouse.html#consistency_check","text":"def consistency_check ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], report_path : Union [ str , os . PathLike ], check_pure : bool = True ) View Source def consistency_check ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ], report_path : Union [ str , os . PathLike ], check_pure : bool = True ): result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) _check_report_for_dependency_limitations ( report_path ) remove_setuptools ( python_packages_folder ) if 0 != result . returncode : raise AssertionError ( f \"Dependency checking failed for file '{requirements_file_path}'. \\n {result.stderr}\" ) if check_pure : check_directory_has_only_wheels_and_pure_sdist ( python_packages_folder )","title":"consistency_check"},{"location":"reference/simaticai/packaging/wheelhouse.html#create_wheelhouse","text":"def create_wheelhouse ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ] ) View Source def create_wheelhouse ( requirements_file_path : Union [ str , os . PathLike ], python_version : str , python_packages_folder : Union [ str , os . PathLike ]): dependency_set = set () # try the easy way success = _download_only_wheels_if_possible ( requirements_file_path , python_version , python_packages_folder ) if success : if not any ( python_packages_folder . iterdir ()): return dependency_set try : tmp = Path ( tempfile . mkdtemp ()) report_path = report_json_path ( tmp ) result = _pip_dry_install_packages ( python_version , python_packages_folder , report_path ) if 0 != result . returncode : raise RuntimeError ( f \"Dry install failed, reason: \\n {result.stderr}\" ) _check_report_for_dependency_limitations ( report_path ) finally : shutil . rmtree ( tmp , ignore_errors = True ) else : # let's do this the hard way try : _compose_dependencies ( requirements_file_path , python_version , python_packages_folder ) except AssertionError as error : raise RuntimeError ( f \"Downloading wheels and source distributions failed, reason: \\n {str(error)}\" ) for package in python_packages_folder . iterdir (): if is_source_file ( package ): dependency_set . add ( get_sdist_name_version ( package )) elif is_wheel_file ( package ): dependency_set . add ( get_wheel_name_version ( package )) else : # TODO what to do when there is a different kind of file in the packages folder? pass return dependency_set","title":"create_wheelhouse"},{"location":"reference/simaticai/packaging/wheelhouse.html#get_sdist_name_version","text":"def get_sdist_name_version ( archive_path : Union [ str , os . PathLike ] ) Extracts the package name and version from a source distribution file. Parameters: Name Type Description Default archive_path path-like The relative or absolute path of the zip or tar.gz archive file. None Returns: Type Description None (str, str): The name and version of the source distribution if successful, (None,None) otherwise. View Source def get_sdist_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a source distribution file. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: (str, str): The name and version of the source distribution if successful, (None,None) otherwise. \"\"\" headers = _extract_pkg_info ( archive_path ) if headers is None : return ( None , None ) headers = headers [ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version )","title":"get_sdist_name_version"},{"location":"reference/simaticai/packaging/wheelhouse.html#get_wheel_name_version","text":"def get_wheel_name_version ( archive_path : Union [ str , os . PathLike ] ) Extracts the package name and version from a wheel file. Parameters: Name Type Description Default archive_path path-like The relative or absolute path of the wheel archive file. None Returns: Type Description None (str, str): The name and version of the wheel if successful, (None,None) otherwise. View Source def get_wheel_name_version ( archive_path : Union [ str , os . PathLike ]): \"\"\" Extracts the package name and version from a wheel file. Args: archive_path (path-like): The relative or absolute path of the wheel archive file. Returns: (str, str): The name and version of the wheel if successful, (None,None) otherwise. \"\"\" with zipfile . ZipFile ( archive_path , \"r\" ) as archive : files = archive . namelist () METADATA = list ( filter ( lambda filepath : filepath . endswith ( ' METADATA ' ), files )) if 0 < len ( METADATA ): headers = map ( lambda filename : archive . read ( filename ). decode ( \"utf-8\" ), METADATA ) headers = map ( lambda txt : Parser (). parsestr ( text = txt , headersonly = True ), headers ) headers = list ( headers )[ 0 ] name = headers . get ( \"name\" ) version = headers . get ( \"version\" ) return ( name , version ) return ( None , None )","title":"get_wheel_name_version"},{"location":"reference/simaticai/packaging/wheelhouse.html#is_pure_python_source","text":"def is_pure_python_source ( archive_path : Union [ str , os . PathLike ] ) Checks whether the given source distribution contains only Python sources. This method handles source distributions in a unified way. It searches for 'PKG-INFO' files, collects the programming languages used in the source, and returns True if only the Python language was used. Parameters: Name Type Description Default archive_path path-like The relative or absolute path of the zip or tar.gz archive file. None Returns: Type Description bool True if the archive file contains only Python sources. View Source def is_pure_python_source(archive_path: Union[str, os.PathLike]): \"\"\" Checks whether the given source distribution contains only Python sources. This method handles source distributions in a unified way. It searches for 'PKG-INFO' files, collects the programming languages used in the source, and returns True if only the Python language was used. Args: archive_path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains only Python sources. \"\"\" headers = _extract_pkg_info(archive_path) if headers is None: return False classifiers = map(lambda header: header.get_all(\"classifier\"), headers) classifiers = map(lambda classifier: [] if classifier is None else classifier, classifiers) classifiers = chain.from_iterable(classifiers) programming_languages = filter(lambda line: line.startswith('Programming Language ::'), classifiers) programming_languages = map(lambda line: line.split(\"::\")[1].strip().lower(), programming_languages) return all(map(lambda txt: txt == 'python', programming_languages))","title":"is_pure_python_source"},{"location":"reference/simaticai/packaging/wheelhouse.html#is_source_file","text":"def is_source_file ( path : os . PathLike ) -> bool Checks whether the file on the given path is a python source distribtion file. Parameters: Name Type Description Default path path-like The relative or absolute path of the zip or tar.gz archive file. None Returns: Type Description bool True if the archive file contains a PKG-INFO text file, False otherwise. View Source def is_source_file ( path : os . PathLike ) -> bool : \" \"\" Checks whether the file on the given `path` is a python source distribtion file. Args: path (path-like): The relative or absolute path of the zip or tar.gz archive file. Returns: bool: True if the archive file contains a PKG-INFO text file, False otherwise. \"\" \" if zipfile . is_zipfile ( path ) : _archive = zipfile . ZipFile ( path ) return 'PKG-INFO' in [ f . split ( \"/\" ) [ - 1 ] for f in _archive . namelist () ] if tarfile . is_tarfile ( path ) : with tarfile . open ( path ) as _archive : return 'PKG-INFO' in [ f . split ( \"/\" ) [ - 1 ] for f in _archive . getnames () ] return False","title":"is_source_file"},{"location":"reference/simaticai/packaging/wheelhouse.html#is_wheel_file","text":"def is_wheel_file ( path : os . PathLike ) -> bool Checks whether the file on the given path is a wheel file. Parameters: Name Type Description Default path path-like The relative or absolute path of the wheel file. None Returns: Type Description bool True if the zipfile contains a WHEEL text file, False otherwise. View Source def is_wheel_file ( path : os . PathLike ) -> bool : \" \"\" Checks whether the file on the given `path` is a wheel file. Args: path (path-like): The relative or absolute path of the wheel file. Returns: bool: True if the zipfile contains a WHEEL text file, False otherwise. \"\" \" if zipfile . is_zipfile ( path ) : _wheel = zipfile . ZipFile ( path ) return 'WHEEL' in [ f . split ( \"/\" ) [ - 1 ] for f in _wheel . namelist () ] return False","title":"is_wheel_file"},{"location":"reference/simaticai/packaging/wheelhouse.html#remove_setuptools","text":"def remove_setuptools ( python_packages_folder ) View Source def remove_setuptools ( python_packages_folder ): filename = [ f for f in Path ( python_packages_folder ) . glob ( \"setuptools*\" )] if len ( filename ): filename [ 0 ] . unlink ()","title":"remove_setuptools"},{"location":"reference/simaticai/packaging/wheelhouse.html#report_json_path","text":"def report_json_path ( tmp_dir ) View Source report_json_path = lambda tmp_dir: Path(tmp_dir) / \"report.json\"","title":"report_json_path"},{"location":"reference/simaticai/testing/index.html","text":"Module simaticai.testing Test pipeline configuration package locally When you have created your inference pipeline package, you could go straight on with deploying it to the AI Inference Server. However, we strongly recommend that you test your package before you deploy it. The benefits of local testing are the following: You can figure out many potential problems quicker, as you don't have to go through a deployment cycle. You can diagnose and troubleshoot issues more easily, as you can inspect artifacts in your development environment. You can validate your fixes quicker and move on to further issues that have not surfaced yet due to earlier issues. You can easily include the local pipeline tests into the test automation in your build process. In general, we encourage you to apply state-of-the-art software engineering practices, such as unit testing and test driven development. This means that ideally you already have automated unit or even integration tests in place that make sure that the Python code and the saved models work according to expectations in isolation. This helps you localize errors when you put these pieces together and integrate them as a pipeline configuration package. AI SDK package simaticai.testing provides two tools for local testing: A pipeline validator, that performs a static validation of the package concerning the availability of required Python packages. A pipeline runner, that lets you simulate the execution of your pipeline in your Python environment. Please note that all this functionality applies to pipeline configuration packages, not edge configuration packages. In other words, you must use them before you convert your pipeline configuration package to an edge configuration package using the convert_package function. As the conversion itself is done in an automated way, most potential problems are already present in the package before the conversion, so a verification after conversion would only delay identifying these problems. For more comprehensive guidance on how to test pipelines before deployment, we recommend you refer to the AI SDK User Manual, especially the chapter concerning local testing of pipeline configuration packages. We also recommend you study the project templates for the AI SDK, which provide concrete code examples that show how to feed a pipeline with different kinds of inputs in a local test. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \" \"\" ## Test pipeline configuration package locally When you have created your inference pipeline package, you could go straight on with deploying it to the AI Inference Server. **However, we strongly recommend that you test your package before you deploy it.** The benefits of local testing are the following: - You can figure out many potential problems quicker, as you don't have to go through a deployment cycle. - You can diagnose and troubleshoot issues more easily, as you can inspect artifacts in your development environment. - You can validate your fixes quicker and move on to further issues that have not surfaced yet due to earlier issues. - You can easily include the local pipeline tests into the test automation in your build process. In general, we encourage you to apply state-of-the-art software engineering practices, such as unit testing and test driven development. This means that ideally you already have automated unit or even integration tests in place that make sure that the Python code and the saved models work according to expectations in isolation. This helps you localize errors when you put these pieces together and integrate them as a pipeline configuration package. AI SDK package `simaticai.testing` provides two tools for local testing: - A pipeline validator, that performs a static validation of the package concerning the availability of required Python packages. - A pipeline runner, that lets you simulate the execution of your pipeline in your Python environment. Please note that all this functionality applies to pipeline configuration packages, not edge configuration packages. In other words, you must use them before you convert your pipeline configuration package to an edge configuration package using the `convert_package` function. As the conversion itself is done in an automated way, most potential problems are already present in the package before the conversion, so a verification after conversion would only delay identifying these problems. For more comprehensive guidance on how to test pipelines before deployment, we recommend you refer to the AI SDK User Manual, especially the chapter concerning local testing of pipeline configuration packages. We also recommend you study the project templates for the AI SDK, which provide concrete code examples that show how to feed a pipeline with different kinds of inputs in a local test. \"\" \" from . pipeline_runner import LocalPipelineRunner from . component_runner import ComponentRunner __all__ = [ \"LocalPipelineRunner\" , \"ComponentRunner\" ] Sub-modules simaticai.testing.component_runner simaticai.testing.data_stream simaticai.testing.pipeline_runner simaticai.testing.pipeline_validator simaticai.testing.run_component simaticai.testing.run_gpuruntime_component simaticai.testing.timeseries_stream simaticai.testing.vca_stream","title":"Index"},{"location":"reference/simaticai/testing/index.html#module-simaticaitesting","text":"","title":"Module simaticai.testing"},{"location":"reference/simaticai/testing/index.html#test-pipeline-configuration-package-locally","text":"When you have created your inference pipeline package, you could go straight on with deploying it to the AI Inference Server. However, we strongly recommend that you test your package before you deploy it. The benefits of local testing are the following: You can figure out many potential problems quicker, as you don't have to go through a deployment cycle. You can diagnose and troubleshoot issues more easily, as you can inspect artifacts in your development environment. You can validate your fixes quicker and move on to further issues that have not surfaced yet due to earlier issues. You can easily include the local pipeline tests into the test automation in your build process. In general, we encourage you to apply state-of-the-art software engineering practices, such as unit testing and test driven development. This means that ideally you already have automated unit or even integration tests in place that make sure that the Python code and the saved models work according to expectations in isolation. This helps you localize errors when you put these pieces together and integrate them as a pipeline configuration package. AI SDK package simaticai.testing provides two tools for local testing: A pipeline validator, that performs a static validation of the package concerning the availability of required Python packages. A pipeline runner, that lets you simulate the execution of your pipeline in your Python environment. Please note that all this functionality applies to pipeline configuration packages, not edge configuration packages. In other words, you must use them before you convert your pipeline configuration package to an edge configuration package using the convert_package function. As the conversion itself is done in an automated way, most potential problems are already present in the package before the conversion, so a verification after conversion would only delay identifying these problems. For more comprehensive guidance on how to test pipelines before deployment, we recommend you refer to the AI SDK User Manual, especially the chapter concerning local testing of pipeline configuration packages. We also recommend you study the project templates for the AI SDK, which provide concrete code examples that show how to feed a pipeline with different kinds of inputs in a local test. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \" \"\" ## Test pipeline configuration package locally When you have created your inference pipeline package, you could go straight on with deploying it to the AI Inference Server. **However, we strongly recommend that you test your package before you deploy it.** The benefits of local testing are the following: - You can figure out many potential problems quicker, as you don't have to go through a deployment cycle. - You can diagnose and troubleshoot issues more easily, as you can inspect artifacts in your development environment. - You can validate your fixes quicker and move on to further issues that have not surfaced yet due to earlier issues. - You can easily include the local pipeline tests into the test automation in your build process. In general, we encourage you to apply state-of-the-art software engineering practices, such as unit testing and test driven development. This means that ideally you already have automated unit or even integration tests in place that make sure that the Python code and the saved models work according to expectations in isolation. This helps you localize errors when you put these pieces together and integrate them as a pipeline configuration package. AI SDK package `simaticai.testing` provides two tools for local testing: - A pipeline validator, that performs a static validation of the package concerning the availability of required Python packages. - A pipeline runner, that lets you simulate the execution of your pipeline in your Python environment. Please note that all this functionality applies to pipeline configuration packages, not edge configuration packages. In other words, you must use them before you convert your pipeline configuration package to an edge configuration package using the `convert_package` function. As the conversion itself is done in an automated way, most potential problems are already present in the package before the conversion, so a verification after conversion would only delay identifying these problems. For more comprehensive guidance on how to test pipelines before deployment, we recommend you refer to the AI SDK User Manual, especially the chapter concerning local testing of pipeline configuration packages. We also recommend you study the project templates for the AI SDK, which provide concrete code examples that show how to feed a pipeline with different kinds of inputs in a local test. \"\" \" from . pipeline_runner import LocalPipelineRunner from . component_runner import ComponentRunner __all__ = [ \"LocalPipelineRunner\" , \"ComponentRunner\" ]","title":"Test pipeline configuration package locally"},{"location":"reference/simaticai/testing/index.html#sub-modules","text":"simaticai.testing.component_runner simaticai.testing.data_stream simaticai.testing.pipeline_runner simaticai.testing.pipeline_validator simaticai.testing.run_component simaticai.testing.run_gpuruntime_component simaticai.testing.timeseries_stream simaticai.testing.vca_stream","title":"Sub-modules"},{"location":"reference/simaticai/testing/component_runner.html","text":"Module simaticai.testing.component_runner None None View Source # Copyright (C) Siemens AG 2025. All Rights Reserved. Confidential. import json import logging import os from pathlib import Path import shutil import subprocess from typing import Optional , Union import venv import joblib from simaticai.deployment import PythonComponent , GPURuntimeComponent from simaticai.testing.data_stream import DataStream PYTHON_RUNNER_PATH = Path ( __file__ ) . parent / 'run_component.py' GPU_RUNNER_PATH = Path ( __file__ ) . parent / 'run_gpuruntime_component.py' GPU_REQUIREMENTS_PATH = Path ( __file__ ) . parent / 'gpuruntime_requirements.txt' GPU_CONFIG_PATH = Path ( __file__ ) . parents [ 1 ] / 'model_config_pb2.py' class ComponentRunner (): \"\"\" Class to run a Pipeline Component in a virtual environment. Supported Component Types: - PythonComponent - GPURuntimeComponent Args: component: PythonComponent or GPURuntimeComponent workdir: Path to the directory where the component should be run. If None, the current working directory is used. cleanup: If True, the workdir is deleted after the context manager exits. \"\"\" def __init__ ( self , component , workdir = None , cleanup = False ): self . component = component self . parameters = {} self . cleanup = cleanup self . _logger = self . _set_logger () self . _create_workdir ( component , workdir ) self . _copy_resources () self . _create_venv () self . _install_requirements () def __enter__ ( self ): return self def __exit__ ( self , exception_type , exception_value , traceback ): if exception_type is not None : self . _logger . error ( f \"Exception occurred: { exception_type . __name__ } : { exception_value } \" ) return False if self . cleanup : shutil . rmtree ( self . workdir ) def _set_logger ( self ): \"\"\" Set the logger for the ComponentRunner. \"\"\" logger = logging . getLogger ( __name__ ) log_level = os . environ . get ( \"loglevel\" , \"INFO\" ) . upper () logger . setLevel ( log_level ) handler = logging . StreamHandler () handler . setLevel ( log_level ) formatter = logging . Formatter ( ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger def _create_workdir ( self , component , workdir ): \"\"\" Create a workdir for the component. \"\"\" if workdir is None : self . workdir = Path . cwd () / component . name else : self . workdir = Path ( workdir ) / component . name self . workdir . mkdir ( parents = True , exist_ok = True ) self . _logger . info ( f \"Created workdir: { self . workdir } \" ) def _copy_resources ( self ): \"\"\" Copy the required resources of the component to the workdir. In case of a PythonComponent, also creates the dependencies into requirements.txt. In case of a GPURuntimeComponent, also creates a requirements.txt with the required onnx and onnxruntime dependencies. Also adds the runner script to the workdir. In case of the source folder same as the workdir, the resources are not copied, so any changes in source will affect the workdir. \"\"\" if isinstance ( self . component , PythonComponent ): same_resources = [] for from_path in self . component . resources . keys (): try : to_path = self . workdir / self . component . resources [ from_path ] shutil . copy ( from_path , to_path ) except shutil . SameFileError : same_resources . append ( from_path . name ) if same_resources : self . _logger . info ( \"Resources are already in workdir, which are not copied:\" ) for resource in same_resources : self . _logger . debug ( f \" - { resource } \" ) self . component . python_dependencies . save ( self . workdir ) # saves manually added Python packages and requirements.txt shutil . copy ( PYTHON_RUNNER_PATH , self . workdir ) else : model_dir = Path ( self . workdir / \"1\" ) model_dir . mkdir ( parents = True , exist_ok = True ) shutil . copy ( self . component . model_path , model_dir / \"model.onnx\" ) Path ( self . workdir / \"config.pbtxt\" ) . write_text ( f \" { self . component . auto_config } \" ) shutil . copy ( GPU_RUNNER_PATH , self . workdir / \"run_component.py\" ) shutil . copy ( GPU_REQUIREMENTS_PATH , self . workdir / \"requirements.txt\" ) shutil . copy ( GPU_CONFIG_PATH , self . workdir ) self . _logger . info ( \"Resources are copied into the workdir.\" ) def _create_venv ( self ): \"\"\" Create a Python virtual environment in the workdir. The created virtual environment is stored in the context_dir and has the same version as the current Python interpreter. \"\"\" context_dir = self . workdir / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( context_dir ) self . context_dir = builder . ensure_directories ( context_dir ) self . bin_path = Path ( self . context_dir . bin_path ) . resolve () self . _logger . info ( f \"Python virtualenv created in folder ' { self . bin_path . parent } '\" ) def _install_requirements ( self ): \"\"\" Installs the required Python dependencies. \"\"\" # TODO: Add log_module install # TODO: Add install from PythonPackages.zip cmd = [ str ( self . bin_path / 'pip' ), 'install' , 'joblib' , '-r' , 'requirements.txt' ] result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) result_lines = result . stdout . decode () . split ( \" \\n \" ) installed_message = \" \" . join ([ line for line in result_lines if \"Successfully installed\" in line ]) self . _logger . info ( installed_message ) def set_parameters ( self , parameter_name , parameter_value ): \"\"\" Set the parameters for the component. \"\"\" self . parameters [ parameter_name ] = parameter_value def run ( self , input_payload : Optional [ Union [ dict , list ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. \"\"\" input_payload_path : Path = self . workdir / \"input.joblib\" output_payload_path : Path = self . workdir / \"output.joblib\" batch_input : bool = self . component . batch . inputBatch if isinstance ( input_payload , DataStream ): input_payload = [ item for item in input_payload ] elif isinstance ( input_payload , dict ): input_payload = [ input_payload ] input_variables = [{ 'name' : name , 'type' : input [ 'type' ]} for name , input in self . component . inputs . items ()] self . _validate_payload ( input_payload , input_variables , batch_input ) joblib . dump ( input_payload , input_payload_path ) cmd = self . _create_command ( input_payload_path , output_payload_path ) self . _logger . info ( \"Running command: \" + \" \" . join ( cmd )) result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) self . _logger . debug ( result . stderr . decode ()) output_payload = joblib . load ( output_payload_path ) batch_output : bool = self . component . batch . outputBatch output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] output_payload = [ output for output in output_payload if output is not None ] output_variables = [{ 'name' : name , 'type' : output [ 'type' ]} for name , output in self . component . outputs . items ()] if isinstance ( self . component , PythonComponent ): output_variables += [{ 'name' : name } for name in self . component . metrics ] self . _validate_payload ( output_payload , output_variables , batch_output ) return output_payload def _check_instance ( self , element , element_name : str , instance ): if not isinstance ( element , instance ): self . _logger . error ( f \" { element_name } must be an instance of { instance . __name__ } \" ) raise ValueError ( f \" { element_name } must be an instance of { instance . __name__ } \" ) def _check_variable_types ( self , variable : dict , value ): if variable [ \"type\" ] == \"String\" : self . _check_instance ( value , \"String value\" , str ) if variable [ \"type\" ] == \"StringArray\" : self . _check_instance ( value , \"StringArray value\" , list ) for i in value : self . _check_instance ( i , \"StringArray item\" , str ) if variable [ \"type\" ] == \"Object\" : self . _check_instance ( value , \"Object value\" , dict ) values = list ( value . values ()) if len ( values ) != 2 : self . _logger . error ( \"Object value must have exactly 2 items\" ) raise ValueError ( \"Object value must have exactly 2 items\" ) ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) if not ok : self . _logger . error ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) raise ValueError ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) def _validate_payload ( self , payload : list , variables : list , batch : bool ): self . _check_instance ( payload , \"Payload\" , list ) for payload_element in payload : if batch : self . _check_instance ( payload_element , \"Batch payload element\" , list ) else : payload_element = [ payload_element ] for item in payload_element : self . _validate_payload_item ( item , variables ) def _validate_payload_item ( self , item : dict , variables : list ): self . _check_instance ( item , \"Payload item\" , dict ) for variable in variables : name = variable [ \"name\" ] value = item . get ( name , None ) if value is None : self . _logger . warning ( f \"WARNING! Variable ' { name } ' is missing from input, output or metric\" ) continue self . _check_variable_types ( variable , value ) payload_names = set ( item . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): self . _logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: { extra_variables } \" ) def _create_command ( self , input_payload_path , output_payload_path ): \"\"\" Create the command to run the component. \"\"\" cmd = [ str ( self . bin_path / 'python' ), \"-m\" , 'run_component' ] if isinstance ( self . component , PythonComponent ): cmd += [ \"-m\" , Path ( self . component . entrypoint ) . stem , \"-p\" , json . dumps ( self . parameters ) ] if isinstance ( self . component , GPURuntimeComponent ): # TODO: check relative path to run_component.py model_path = Path ( self . workdir / \"1\" / \"model.onnx\" ) . absolute () . resolve () config_path = Path ( self . workdir / \"config.pbtxt\" ) . absolute () . resolve () cmd += [ \"-m\" , str ( model_path ), \"-c\" , str ( config_path ), ] cmd += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . _logger . getEffectiveLevel ()) ] return cmd Variables GPU_CONFIG_PATH GPU_REQUIREMENTS_PATH GPU_RUNNER_PATH PYTHON_RUNNER_PATH Classes ComponentRunner Class to run a Pipeline Component in a virtual environment. Supported Component Types: - PythonComponent - GPURuntimeComponent Args: component: PythonComponent or GPURuntimeComponent workdir: Path to the directory where the component should be run. If None, the current working directory is used. cleanup: If True, the workdir is deleted after the context manager exits. class ComponentRunner ( component , workdir = None , cleanup = False ) View Source class ComponentRunner (): \"\"\" Class to run a Pipeline Component in a virtual environment. Supported Component Types: - PythonComponent - GPURuntimeComponent Args: component: PythonComponent or GPURuntimeComponent workdir: Path to the directory where the component should be run. If None, the current working directory is used. cleanup: If True, the workdir is deleted after the context manager exits. \"\"\" def __init__ ( self , component , workdir = None , cleanup = False ): self . component = component self . parameters = {} self . cleanup = cleanup self . _logger = self . _set_logger () self . _create_workdir ( component , workdir ) self . _copy_resources () self . _create_venv () self . _install_requirements () def __enter__ ( self ): return self def __exit__ ( self , exception_type , exception_value , traceback ): if exception_type is not None : self . _logger . error ( f \"Exception occurred: {exception_type.__name__}: {exception_value}\" ) return False if self . cleanup : shutil . rmtree ( self . workdir ) def _set_logger ( self ): \"\"\" Set the logger for the ComponentRunner. \"\"\" logger = logging . getLogger ( __name__ ) log_level = os . environ . get ( \"loglevel\" , \"INFO\" ) . upper () logger . setLevel ( log_level ) handler = logging . StreamHandler () handler . setLevel ( log_level ) formatter = logging . Formatter ( ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger def _create_workdir ( self , component , workdir ): \"\"\" Create a workdir for the component. \"\"\" if workdir is None : self . workdir = Path . cwd () / component . name else : self . workdir = Path ( workdir ) / component . name self . workdir . mkdir ( parents = True , exist_ok = True ) self . _logger . info ( f \"Created workdir: {self.workdir}\" ) def _copy_resources ( self ): \"\"\" Copy the required resources of the component to the workdir. In case of a PythonComponent, also creates the dependencies into requirements.txt. In case of a GPURuntimeComponent, also creates a requirements.txt with the required onnx and onnxruntime dependencies. Also adds the runner script to the workdir. In case of the source folder same as the workdir, the resources are not copied, so any changes in source will affect the workdir. \"\"\" if isinstance ( self . component , PythonComponent ): same_resources = [] for from_path in self . component . resources . keys (): try : to_path = self . workdir / self . component . resources [ from_path ] shutil . copy ( from_path , to_path ) except shutil . SameFileError : same_resources . append ( from_path . name ) if same_resources : self . _logger . info ( \"Resources are already in workdir, which are not copied:\" ) for resource in same_resources : self . _logger . debug ( f \" - {resource}\" ) self . component . python_dependencies . save ( self . workdir ) # saves manually added Python packages and requirements.txt shutil . copy ( PYTHON_RUNNER_PATH , self . workdir ) else : model_dir = Path ( self . workdir / \"1\" ) model_dir . mkdir ( parents = True , exist_ok = True ) shutil . copy ( self . component . model_path , model_dir / \"model.onnx\" ) Path ( self . workdir / \"config.pbtxt\" ) . write_text ( f \"{self.component.auto_config}\" ) shutil . copy ( GPU_RUNNER_PATH , self . workdir / \"run_component.py\" ) shutil . copy ( GPU_REQUIREMENTS_PATH , self . workdir / \"requirements.txt\" ) shutil . copy ( GPU_CONFIG_PATH , self . workdir ) self . _logger . info ( \"Resources are copied into the workdir.\" ) def _create_venv ( self ): \"\"\" Create a Python virtual environment in the workdir. The created virtual environment is stored in the context_dir and has the same version as the current Python interpreter. \"\"\" context_dir = self . workdir / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( context_dir ) self . context_dir = builder . ensure_directories ( context_dir ) self . bin_path = Path ( self . context_dir . bin_path ) . resolve () self . _logger . info ( f \"Python virtualenv created in folder '{self.bin_path.parent}'\" ) def _install_requirements ( self ): \"\"\" Installs the required Python dependencies. \"\"\" # TODO: Add log_module install # TODO: Add install from PythonPackages.zip cmd = [ str ( self . bin_path / 'pip' ), 'install' , 'joblib' , '-r' , 'requirements.txt' ] result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) result_lines = result . stdout . decode () . split ( \" \\n \" ) installed_message = \" \" . join ([ line for line in result_lines if \"Successfully installed\" in line ]) self . _logger . info ( installed_message ) def set_parameters ( self , parameter_name , parameter_value ): \"\"\" Set the parameters for the component. \"\"\" self . parameters [ parameter_name ] = parameter_value def run ( self , input_payload : Optional [ Union [ dict , list ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. \"\"\" input_payload_path : Path = self . workdir / \"input.joblib\" output_payload_path : Path = self . workdir / \"output.joblib\" batch_input : bool = self . component . batch . inputBatch if isinstance ( input_payload , DataStream ): input_payload = [ item for item in input_payload ] elif isinstance ( input_payload , dict ): input_payload = [ input_payload ] input_variables = [{ 'name' : name , 'type' : input [ 'type' ]} for name , input in self . component . inputs . items ()] self . _validate_payload ( input_payload , input_variables , batch_input ) joblib . dump ( input_payload , input_payload_path ) cmd = self . _create_command ( input_payload_path , output_payload_path ) self . _logger . info ( \"Running command: \" + \" \" . join ( cmd )) result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) self . _logger . debug ( result . stderr . decode ()) output_payload = joblib . load ( output_payload_path ) batch_output : bool = self . component . batch . outputBatch output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] output_payload = [ output for output in output_payload if output is not None ] output_variables = [{ 'name' : name , 'type' : output [ 'type' ]} for name , output in self . component . outputs . items ()] if isinstance ( self . component , PythonComponent ): output_variables += [{ 'name' : name } for name in self . component . metrics ] self . _validate_payload ( output_payload , output_variables , batch_output ) return output_payload def _check_instance ( self , element , element_name : str , instance ): if not isinstance ( element , instance ): self . _logger . error ( f \"{element_name} must be an instance of {instance.__name__}\" ) raise ValueError ( f \"{element_name} must be an instance of {instance.__name__}\" ) def _check_variable_types ( self , variable : dict , value ): if variable [ \"type\" ] == \"String\" : self . _check_instance ( value , \"String value\" , str ) if variable [ \"type\" ] == \"StringArray\" : self . _check_instance ( value , \"StringArray value\" , list ) for i in value : self . _check_instance ( i , \"StringArray item\" , str ) if variable [ \"type\" ] == \"Object\" : self . _check_instance ( value , \"Object value\" , dict ) values = list ( value . values ()) if len ( values ) != 2 : self . _logger . error ( \"Object value must have exactly 2 items\" ) raise ValueError ( \"Object value must have exactly 2 items\" ) ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) if not ok : self . _logger . error ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) raise ValueError ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) def _validate_payload ( self , payload : list , variables : list , batch : bool ): self . _check_instance ( payload , \"Payload\" , list ) for payload_element in payload : if batch : self . _check_instance ( payload_element , \"Batch payload element\" , list ) else : payload_element = [ payload_element ] for item in payload_element : self . _validate_payload_item ( item , variables ) def _validate_payload_item ( self , item : dict , variables : list ): self . _check_instance ( item , \"Payload item\" , dict ) for variable in variables : name = variable [ \"name\" ] value = item . get ( name , None ) if value is None : self . _logger . warning ( f \"WARNING! Variable '{name}' is missing from input, output or metric\" ) continue self . _check_variable_types ( variable , value ) payload_names = set ( item . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): self . _logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: {extra_variables}\" ) def _create_command ( self , input_payload_path , output_payload_path ): \"\"\" Create the command to run the component. \"\"\" cmd = [ str ( self . bin_path / 'python' ), \"-m\" , 'run_component' ] if isinstance ( self . component , PythonComponent ): cmd += [ \"-m\" , Path ( self . component . entrypoint ) . stem , \"-p\" , json . dumps ( self . parameters ) ] if isinstance ( self . component , GPURuntimeComponent ): # TODO: check relative path to run_component.py model_path = Path ( self . workdir / \"1\" / \"model.onnx\" ) . absolute () . resolve () config_path = Path ( self . workdir / \"config.pbtxt\" ) . absolute () . resolve () cmd += [ \"-m\" , str ( model_path ), \"-c\" , str ( config_path ), ] cmd += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . _logger . getEffectiveLevel ()) ] return cmd Methods run def run ( self , input_payload : Union [ dict , list , NoneType ] ) -> Union [ dict , list , NoneType ] Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. View Source def run ( self , input_payload : Optional [ Union [ dict , list ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. \"\"\" input_payload_path : Path = self . workdir / \"input.joblib\" output_payload_path : Path = self . workdir / \"output.joblib\" batch_input : bool = self . component . batch . inputBatch if isinstance ( input_payload , DataStream ): input_payload = [ item for item in input_payload ] elif isinstance ( input_payload , dict ): input_payload = [ input_payload ] input_variables = [{ 'name' : name , 'type' : input [ 'type' ]} for name , input in self . component . inputs . items ()] self . _validate_payload ( input_payload , input_variables , batch_input ) joblib . dump ( input_payload , input_payload_path ) cmd = self . _create_command ( input_payload_path , output_payload_path ) self . _logger . info ( \"Running command: \" + \" \" . join ( cmd )) result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) self . _logger . debug ( result . stderr . decode ()) output_payload = joblib . load ( output_payload_path ) batch_output : bool = self . component . batch . outputBatch output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] output_payload = [ output for output in output_payload if output is not None ] output_variables = [{ 'name' : name , 'type' : output [ 'type' ]} for name , output in self . component . outputs . items ()] if isinstance ( self . component , PythonComponent ): output_variables += [{ 'name' : name } for name in self . component . metrics ] self . _validate_payload ( output_payload , output_variables , batch_output ) return output_payload set_parameters def set_parameters ( self , parameter_name , parameter_value ) Set the parameters for the component. View Source def set_parameters ( self , parameter_name , parameter_value ) : \"\"\" Set the parameters for the component. \"\"\" self . parameters [ parameter_name ] = parameter_value","title":"Component Runner"},{"location":"reference/simaticai/testing/component_runner.html#module-simaticaitestingcomponent_runner","text":"None None View Source # Copyright (C) Siemens AG 2025. All Rights Reserved. Confidential. import json import logging import os from pathlib import Path import shutil import subprocess from typing import Optional , Union import venv import joblib from simaticai.deployment import PythonComponent , GPURuntimeComponent from simaticai.testing.data_stream import DataStream PYTHON_RUNNER_PATH = Path ( __file__ ) . parent / 'run_component.py' GPU_RUNNER_PATH = Path ( __file__ ) . parent / 'run_gpuruntime_component.py' GPU_REQUIREMENTS_PATH = Path ( __file__ ) . parent / 'gpuruntime_requirements.txt' GPU_CONFIG_PATH = Path ( __file__ ) . parents [ 1 ] / 'model_config_pb2.py' class ComponentRunner (): \"\"\" Class to run a Pipeline Component in a virtual environment. Supported Component Types: - PythonComponent - GPURuntimeComponent Args: component: PythonComponent or GPURuntimeComponent workdir: Path to the directory where the component should be run. If None, the current working directory is used. cleanup: If True, the workdir is deleted after the context manager exits. \"\"\" def __init__ ( self , component , workdir = None , cleanup = False ): self . component = component self . parameters = {} self . cleanup = cleanup self . _logger = self . _set_logger () self . _create_workdir ( component , workdir ) self . _copy_resources () self . _create_venv () self . _install_requirements () def __enter__ ( self ): return self def __exit__ ( self , exception_type , exception_value , traceback ): if exception_type is not None : self . _logger . error ( f \"Exception occurred: { exception_type . __name__ } : { exception_value } \" ) return False if self . cleanup : shutil . rmtree ( self . workdir ) def _set_logger ( self ): \"\"\" Set the logger for the ComponentRunner. \"\"\" logger = logging . getLogger ( __name__ ) log_level = os . environ . get ( \"loglevel\" , \"INFO\" ) . upper () logger . setLevel ( log_level ) handler = logging . StreamHandler () handler . setLevel ( log_level ) formatter = logging . Formatter ( ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger def _create_workdir ( self , component , workdir ): \"\"\" Create a workdir for the component. \"\"\" if workdir is None : self . workdir = Path . cwd () / component . name else : self . workdir = Path ( workdir ) / component . name self . workdir . mkdir ( parents = True , exist_ok = True ) self . _logger . info ( f \"Created workdir: { self . workdir } \" ) def _copy_resources ( self ): \"\"\" Copy the required resources of the component to the workdir. In case of a PythonComponent, also creates the dependencies into requirements.txt. In case of a GPURuntimeComponent, also creates a requirements.txt with the required onnx and onnxruntime dependencies. Also adds the runner script to the workdir. In case of the source folder same as the workdir, the resources are not copied, so any changes in source will affect the workdir. \"\"\" if isinstance ( self . component , PythonComponent ): same_resources = [] for from_path in self . component . resources . keys (): try : to_path = self . workdir / self . component . resources [ from_path ] shutil . copy ( from_path , to_path ) except shutil . SameFileError : same_resources . append ( from_path . name ) if same_resources : self . _logger . info ( \"Resources are already in workdir, which are not copied:\" ) for resource in same_resources : self . _logger . debug ( f \" - { resource } \" ) self . component . python_dependencies . save ( self . workdir ) # saves manually added Python packages and requirements.txt shutil . copy ( PYTHON_RUNNER_PATH , self . workdir ) else : model_dir = Path ( self . workdir / \"1\" ) model_dir . mkdir ( parents = True , exist_ok = True ) shutil . copy ( self . component . model_path , model_dir / \"model.onnx\" ) Path ( self . workdir / \"config.pbtxt\" ) . write_text ( f \" { self . component . auto_config } \" ) shutil . copy ( GPU_RUNNER_PATH , self . workdir / \"run_component.py\" ) shutil . copy ( GPU_REQUIREMENTS_PATH , self . workdir / \"requirements.txt\" ) shutil . copy ( GPU_CONFIG_PATH , self . workdir ) self . _logger . info ( \"Resources are copied into the workdir.\" ) def _create_venv ( self ): \"\"\" Create a Python virtual environment in the workdir. The created virtual environment is stored in the context_dir and has the same version as the current Python interpreter. \"\"\" context_dir = self . workdir / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( context_dir ) self . context_dir = builder . ensure_directories ( context_dir ) self . bin_path = Path ( self . context_dir . bin_path ) . resolve () self . _logger . info ( f \"Python virtualenv created in folder ' { self . bin_path . parent } '\" ) def _install_requirements ( self ): \"\"\" Installs the required Python dependencies. \"\"\" # TODO: Add log_module install # TODO: Add install from PythonPackages.zip cmd = [ str ( self . bin_path / 'pip' ), 'install' , 'joblib' , '-r' , 'requirements.txt' ] result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) result_lines = result . stdout . decode () . split ( \" \\n \" ) installed_message = \" \" . join ([ line for line in result_lines if \"Successfully installed\" in line ]) self . _logger . info ( installed_message ) def set_parameters ( self , parameter_name , parameter_value ): \"\"\" Set the parameters for the component. \"\"\" self . parameters [ parameter_name ] = parameter_value def run ( self , input_payload : Optional [ Union [ dict , list ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. \"\"\" input_payload_path : Path = self . workdir / \"input.joblib\" output_payload_path : Path = self . workdir / \"output.joblib\" batch_input : bool = self . component . batch . inputBatch if isinstance ( input_payload , DataStream ): input_payload = [ item for item in input_payload ] elif isinstance ( input_payload , dict ): input_payload = [ input_payload ] input_variables = [{ 'name' : name , 'type' : input [ 'type' ]} for name , input in self . component . inputs . items ()] self . _validate_payload ( input_payload , input_variables , batch_input ) joblib . dump ( input_payload , input_payload_path ) cmd = self . _create_command ( input_payload_path , output_payload_path ) self . _logger . info ( \"Running command: \" + \" \" . join ( cmd )) result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) self . _logger . debug ( result . stderr . decode ()) output_payload = joblib . load ( output_payload_path ) batch_output : bool = self . component . batch . outputBatch output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] output_payload = [ output for output in output_payload if output is not None ] output_variables = [{ 'name' : name , 'type' : output [ 'type' ]} for name , output in self . component . outputs . items ()] if isinstance ( self . component , PythonComponent ): output_variables += [{ 'name' : name } for name in self . component . metrics ] self . _validate_payload ( output_payload , output_variables , batch_output ) return output_payload def _check_instance ( self , element , element_name : str , instance ): if not isinstance ( element , instance ): self . _logger . error ( f \" { element_name } must be an instance of { instance . __name__ } \" ) raise ValueError ( f \" { element_name } must be an instance of { instance . __name__ } \" ) def _check_variable_types ( self , variable : dict , value ): if variable [ \"type\" ] == \"String\" : self . _check_instance ( value , \"String value\" , str ) if variable [ \"type\" ] == \"StringArray\" : self . _check_instance ( value , \"StringArray value\" , list ) for i in value : self . _check_instance ( i , \"StringArray item\" , str ) if variable [ \"type\" ] == \"Object\" : self . _check_instance ( value , \"Object value\" , dict ) values = list ( value . values ()) if len ( values ) != 2 : self . _logger . error ( \"Object value must have exactly 2 items\" ) raise ValueError ( \"Object value must have exactly 2 items\" ) ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) if not ok : self . _logger . error ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) raise ValueError ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) def _validate_payload ( self , payload : list , variables : list , batch : bool ): self . _check_instance ( payload , \"Payload\" , list ) for payload_element in payload : if batch : self . _check_instance ( payload_element , \"Batch payload element\" , list ) else : payload_element = [ payload_element ] for item in payload_element : self . _validate_payload_item ( item , variables ) def _validate_payload_item ( self , item : dict , variables : list ): self . _check_instance ( item , \"Payload item\" , dict ) for variable in variables : name = variable [ \"name\" ] value = item . get ( name , None ) if value is None : self . _logger . warning ( f \"WARNING! Variable ' { name } ' is missing from input, output or metric\" ) continue self . _check_variable_types ( variable , value ) payload_names = set ( item . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): self . _logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: { extra_variables } \" ) def _create_command ( self , input_payload_path , output_payload_path ): \"\"\" Create the command to run the component. \"\"\" cmd = [ str ( self . bin_path / 'python' ), \"-m\" , 'run_component' ] if isinstance ( self . component , PythonComponent ): cmd += [ \"-m\" , Path ( self . component . entrypoint ) . stem , \"-p\" , json . dumps ( self . parameters ) ] if isinstance ( self . component , GPURuntimeComponent ): # TODO: check relative path to run_component.py model_path = Path ( self . workdir / \"1\" / \"model.onnx\" ) . absolute () . resolve () config_path = Path ( self . workdir / \"config.pbtxt\" ) . absolute () . resolve () cmd += [ \"-m\" , str ( model_path ), \"-c\" , str ( config_path ), ] cmd += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . _logger . getEffectiveLevel ()) ] return cmd","title":"Module simaticai.testing.component_runner"},{"location":"reference/simaticai/testing/component_runner.html#variables","text":"GPU_CONFIG_PATH GPU_REQUIREMENTS_PATH GPU_RUNNER_PATH PYTHON_RUNNER_PATH","title":"Variables"},{"location":"reference/simaticai/testing/component_runner.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/testing/component_runner.html#componentrunner","text":"Class to run a Pipeline Component in a virtual environment. Supported Component Types: - PythonComponent - GPURuntimeComponent Args: component: PythonComponent or GPURuntimeComponent workdir: Path to the directory where the component should be run. If None, the current working directory is used. cleanup: If True, the workdir is deleted after the context manager exits. class ComponentRunner ( component , workdir = None , cleanup = False ) View Source class ComponentRunner (): \"\"\" Class to run a Pipeline Component in a virtual environment. Supported Component Types: - PythonComponent - GPURuntimeComponent Args: component: PythonComponent or GPURuntimeComponent workdir: Path to the directory where the component should be run. If None, the current working directory is used. cleanup: If True, the workdir is deleted after the context manager exits. \"\"\" def __init__ ( self , component , workdir = None , cleanup = False ): self . component = component self . parameters = {} self . cleanup = cleanup self . _logger = self . _set_logger () self . _create_workdir ( component , workdir ) self . _copy_resources () self . _create_venv () self . _install_requirements () def __enter__ ( self ): return self def __exit__ ( self , exception_type , exception_value , traceback ): if exception_type is not None : self . _logger . error ( f \"Exception occurred: {exception_type.__name__}: {exception_value}\" ) return False if self . cleanup : shutil . rmtree ( self . workdir ) def _set_logger ( self ): \"\"\" Set the logger for the ComponentRunner. \"\"\" logger = logging . getLogger ( __name__ ) log_level = os . environ . get ( \"loglevel\" , \"INFO\" ) . upper () logger . setLevel ( log_level ) handler = logging . StreamHandler () handler . setLevel ( log_level ) formatter = logging . Formatter ( ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger def _create_workdir ( self , component , workdir ): \"\"\" Create a workdir for the component. \"\"\" if workdir is None : self . workdir = Path . cwd () / component . name else : self . workdir = Path ( workdir ) / component . name self . workdir . mkdir ( parents = True , exist_ok = True ) self . _logger . info ( f \"Created workdir: {self.workdir}\" ) def _copy_resources ( self ): \"\"\" Copy the required resources of the component to the workdir. In case of a PythonComponent, also creates the dependencies into requirements.txt. In case of a GPURuntimeComponent, also creates a requirements.txt with the required onnx and onnxruntime dependencies. Also adds the runner script to the workdir. In case of the source folder same as the workdir, the resources are not copied, so any changes in source will affect the workdir. \"\"\" if isinstance ( self . component , PythonComponent ): same_resources = [] for from_path in self . component . resources . keys (): try : to_path = self . workdir / self . component . resources [ from_path ] shutil . copy ( from_path , to_path ) except shutil . SameFileError : same_resources . append ( from_path . name ) if same_resources : self . _logger . info ( \"Resources are already in workdir, which are not copied:\" ) for resource in same_resources : self . _logger . debug ( f \" - {resource}\" ) self . component . python_dependencies . save ( self . workdir ) # saves manually added Python packages and requirements.txt shutil . copy ( PYTHON_RUNNER_PATH , self . workdir ) else : model_dir = Path ( self . workdir / \"1\" ) model_dir . mkdir ( parents = True , exist_ok = True ) shutil . copy ( self . component . model_path , model_dir / \"model.onnx\" ) Path ( self . workdir / \"config.pbtxt\" ) . write_text ( f \"{self.component.auto_config}\" ) shutil . copy ( GPU_RUNNER_PATH , self . workdir / \"run_component.py\" ) shutil . copy ( GPU_REQUIREMENTS_PATH , self . workdir / \"requirements.txt\" ) shutil . copy ( GPU_CONFIG_PATH , self . workdir ) self . _logger . info ( \"Resources are copied into the workdir.\" ) def _create_venv ( self ): \"\"\" Create a Python virtual environment in the workdir. The created virtual environment is stored in the context_dir and has the same version as the current Python interpreter. \"\"\" context_dir = self . workdir / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( context_dir ) self . context_dir = builder . ensure_directories ( context_dir ) self . bin_path = Path ( self . context_dir . bin_path ) . resolve () self . _logger . info ( f \"Python virtualenv created in folder '{self.bin_path.parent}'\" ) def _install_requirements ( self ): \"\"\" Installs the required Python dependencies. \"\"\" # TODO: Add log_module install # TODO: Add install from PythonPackages.zip cmd = [ str ( self . bin_path / 'pip' ), 'install' , 'joblib' , '-r' , 'requirements.txt' ] result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) result_lines = result . stdout . decode () . split ( \" \\n \" ) installed_message = \" \" . join ([ line for line in result_lines if \"Successfully installed\" in line ]) self . _logger . info ( installed_message ) def set_parameters ( self , parameter_name , parameter_value ): \"\"\" Set the parameters for the component. \"\"\" self . parameters [ parameter_name ] = parameter_value def run ( self , input_payload : Optional [ Union [ dict , list ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. \"\"\" input_payload_path : Path = self . workdir / \"input.joblib\" output_payload_path : Path = self . workdir / \"output.joblib\" batch_input : bool = self . component . batch . inputBatch if isinstance ( input_payload , DataStream ): input_payload = [ item for item in input_payload ] elif isinstance ( input_payload , dict ): input_payload = [ input_payload ] input_variables = [{ 'name' : name , 'type' : input [ 'type' ]} for name , input in self . component . inputs . items ()] self . _validate_payload ( input_payload , input_variables , batch_input ) joblib . dump ( input_payload , input_payload_path ) cmd = self . _create_command ( input_payload_path , output_payload_path ) self . _logger . info ( \"Running command: \" + \" \" . join ( cmd )) result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) self . _logger . debug ( result . stderr . decode ()) output_payload = joblib . load ( output_payload_path ) batch_output : bool = self . component . batch . outputBatch output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] output_payload = [ output for output in output_payload if output is not None ] output_variables = [{ 'name' : name , 'type' : output [ 'type' ]} for name , output in self . component . outputs . items ()] if isinstance ( self . component , PythonComponent ): output_variables += [{ 'name' : name } for name in self . component . metrics ] self . _validate_payload ( output_payload , output_variables , batch_output ) return output_payload def _check_instance ( self , element , element_name : str , instance ): if not isinstance ( element , instance ): self . _logger . error ( f \"{element_name} must be an instance of {instance.__name__}\" ) raise ValueError ( f \"{element_name} must be an instance of {instance.__name__}\" ) def _check_variable_types ( self , variable : dict , value ): if variable [ \"type\" ] == \"String\" : self . _check_instance ( value , \"String value\" , str ) if variable [ \"type\" ] == \"StringArray\" : self . _check_instance ( value , \"StringArray value\" , list ) for i in value : self . _check_instance ( i , \"StringArray item\" , str ) if variable [ \"type\" ] == \"Object\" : self . _check_instance ( value , \"Object value\" , dict ) values = list ( value . values ()) if len ( values ) != 2 : self . _logger . error ( \"Object value must have exactly 2 items\" ) raise ValueError ( \"Object value must have exactly 2 items\" ) ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) if not ok : self . _logger . error ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) raise ValueError ( \"Object value must have exactly one 'str' and one 'bytes' field\" ) def _validate_payload ( self , payload : list , variables : list , batch : bool ): self . _check_instance ( payload , \"Payload\" , list ) for payload_element in payload : if batch : self . _check_instance ( payload_element , \"Batch payload element\" , list ) else : payload_element = [ payload_element ] for item in payload_element : self . _validate_payload_item ( item , variables ) def _validate_payload_item ( self , item : dict , variables : list ): self . _check_instance ( item , \"Payload item\" , dict ) for variable in variables : name = variable [ \"name\" ] value = item . get ( name , None ) if value is None : self . _logger . warning ( f \"WARNING! Variable '{name}' is missing from input, output or metric\" ) continue self . _check_variable_types ( variable , value ) payload_names = set ( item . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): self . _logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: {extra_variables}\" ) def _create_command ( self , input_payload_path , output_payload_path ): \"\"\" Create the command to run the component. \"\"\" cmd = [ str ( self . bin_path / 'python' ), \"-m\" , 'run_component' ] if isinstance ( self . component , PythonComponent ): cmd += [ \"-m\" , Path ( self . component . entrypoint ) . stem , \"-p\" , json . dumps ( self . parameters ) ] if isinstance ( self . component , GPURuntimeComponent ): # TODO: check relative path to run_component.py model_path = Path ( self . workdir / \"1\" / \"model.onnx\" ) . absolute () . resolve () config_path = Path ( self . workdir / \"config.pbtxt\" ) . absolute () . resolve () cmd += [ \"-m\" , str ( model_path ), \"-c\" , str ( config_path ), ] cmd += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . _logger . getEffectiveLevel ()) ] return cmd","title":"ComponentRunner"},{"location":"reference/simaticai/testing/component_runner.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/testing/component_runner.html#run","text":"def run ( self , input_payload : Union [ dict , list , NoneType ] ) -> Union [ dict , list , NoneType ] Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. View Source def run ( self , input_payload : Optional [ Union [ dict , list ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Run the component with the input payload. Parameters: input_payload: Input payload for the component. Returns: Output payload from the component. Side Effects: - The input payload is saved in the workdir. - The output payload is loaded from the workdir. \"\"\" input_payload_path : Path = self . workdir / \"input.joblib\" output_payload_path : Path = self . workdir / \"output.joblib\" batch_input : bool = self . component . batch . inputBatch if isinstance ( input_payload , DataStream ): input_payload = [ item for item in input_payload ] elif isinstance ( input_payload , dict ): input_payload = [ input_payload ] input_variables = [{ 'name' : name , 'type' : input [ 'type' ]} for name , input in self . component . inputs . items ()] self . _validate_payload ( input_payload , input_variables , batch_input ) joblib . dump ( input_payload , input_payload_path ) cmd = self . _create_command ( input_payload_path , output_payload_path ) self . _logger . info ( \"Running command: \" + \" \" . join ( cmd )) result = subprocess . run ( cmd , cwd = self . workdir , capture_output = True ) if result . returncode != 0 : raise RuntimeError ( result . stderr . decode ()) self . _logger . debug ( result . stderr . decode ()) output_payload = joblib . load ( output_payload_path ) batch_output : bool = self . component . batch . outputBatch output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] output_payload = [ output for output in output_payload if output is not None ] output_variables = [{ 'name' : name , 'type' : output [ 'type' ]} for name , output in self . component . outputs . items ()] if isinstance ( self . component , PythonComponent ): output_variables += [{ 'name' : name } for name in self . component . metrics ] self . _validate_payload ( output_payload , output_variables , batch_output ) return output_payload","title":"run"},{"location":"reference/simaticai/testing/component_runner.html#set_parameters","text":"def set_parameters ( self , parameter_name , parameter_value ) Set the parameters for the component. View Source def set_parameters ( self , parameter_name , parameter_value ) : \"\"\" Set the parameters for the component. \"\"\" self . parameters [ parameter_name ] = parameter_value","title":"set_parameters"},{"location":"reference/simaticai/testing/data_stream.html","text":"Module simaticai.testing.data_stream None None View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . class DataStream : \"\"\" Base class for datastream generators \"\"\" def __iter__ ( self ): \"\"\" Empty generator method for child classess to implement. \"\"\" raise NotImplementedError ( ' Child classes must implement this method . ' ) Classes DataStream Base class for datastream generators class DataStream ( / , * args , ** kwargs ) View Source class DataStream : \"\"\" Base class for datastream generators \"\"\" def __iter__ ( self ): \"\"\" Empty generator method for child classess to implement. \"\"\" raise NotImplementedError ( ' Child classes must implement this method . ' ) Descendants simaticai.testing.timeseries_stream.TimeSeriesStream simaticai.testing.vca_stream.VCAStream","title":"Data Stream"},{"location":"reference/simaticai/testing/data_stream.html#module-simaticaitestingdata_stream","text":"None None View Source # Copyright ( C ) Siemens AG 2021 . All Rights Reserved . Confidential . class DataStream : \"\"\" Base class for datastream generators \"\"\" def __iter__ ( self ): \"\"\" Empty generator method for child classess to implement. \"\"\" raise NotImplementedError ( ' Child classes must implement this method . ' )","title":"Module simaticai.testing.data_stream"},{"location":"reference/simaticai/testing/data_stream.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/testing/data_stream.html#datastream","text":"Base class for datastream generators class DataStream ( / , * args , ** kwargs ) View Source class DataStream : \"\"\" Base class for datastream generators \"\"\" def __iter__ ( self ): \"\"\" Empty generator method for child classess to implement. \"\"\" raise NotImplementedError ( ' Child classes must implement this method . ' )","title":"DataStream"},{"location":"reference/simaticai/testing/data_stream.html#descendants","text":"simaticai.testing.timeseries_stream.TimeSeriesStream simaticai.testing.vca_stream.VCAStream","title":"Descendants"},{"location":"reference/simaticai/testing/pipeline_runner.html","text":"Module simaticai.testing.pipeline_runner A pipeline runner that lets you simulate the execution of a pipeline in a local Python environment. It can be used to locally mimic the behavior of the AI Inference Server concerning loading and running inference pipelines. This is a quick and easy way to find programming or configuration errors before deploying the package. The local pipeline runner also lets you exercise your pipeline component by component. In other words, you can feed single components with inputs and verify the output produced. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" A pipeline runner that lets you simulate the execution of a pipeline in a local Python environment. It can be used to locally mimic the behavior of the AI Inference Server concerning loading and running inference pipelines. This is a quick and easy way to find programming or configuration errors before deploying the package. The local pipeline runner also lets you exercise your pipeline component by component. In other words, you can feed single components with inputs and verify the output produced. \"\"\" import platform import tempfile import zipfile import pkg_resources import yaml import joblib import json import venv import subprocess import logging import datetime import sys import os import re import shutil from pathlib import Path from importlib import metadata as importlib_metadata from typing import Union , Optional from simaticai.helpers import calc_sha from simaticai.helpers.pep508 import parse_requirements from simaticai.helpers.reporter import PipelineRunnerReportWriter , ReportWriterHandler from simaticai.packaging.constants import ( REQUIREMENTS_TXT , TELEMETRY_YAML , PYTHON_PACKAGES , PYTHON_PACKAGES_ZIP , MSG_NOT_FOUND ) from simaticai.testing.pipeline_validator import INVALID_PIPELINE_PACKAGE_MESSAGE from simaticai.testing.data_stream import DataStream RETURN_CODE_OK = 0b0 RETURN_CODE_DEPRECATED = 0b10001 # equals to 17 logging . basicConfig () _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _runner_path = Path ( __file__ ) . parent type_map = { 'int' : 'Integer' , 'float' : 'Double' , 'bool' : 'Boolean' , 'str' : 'String' } def _relative_to ( path1 , path2 ): path1 = Path ( path1 ) . resolve () . absolute () path2 = Path ( path2 ) . resolve () . absolute () return path1 . relative_to ( path2 ) class LocalPipelineRunner : \"\"\" Simulates the execution of a pipeline in a local Python environment. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Restriction: only linear pipelines are supported where the pipeline input variables are only used by one component, each component uses only the outputs of the previous components, and the pipeline output only consists of variables from the last component. If the caller specifies no `path`, the working directory is temporary and is removed unless an error occurs. If the caller specifies a working directory with a `path` argument, the working directory is kept. This behavior can be overridden using boolean parameter `cleanup`. Currently, the pipeline runner supports both the current `process_input(data: dict)` entrypoint signature and the legacy `run(data: str)` signature. If both entrypoints are present, `process_input()` takes precedence. Please note however that `run()` is deprecated, and support for it will be removed in future versions of the pipeline runner. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset, a temporary directory is created. cleanup (bool): If set, the working directory is kept when True, and deleted when False. \\ If unset, a temporary working directory is removed, and an explicit working directory is kept. \\ When an error occurs in a component, the working directory is kept regardless of this value. \"\"\" def __init__ ( self , packageZip : os . PathLike , path : Optional [ os . PathLike ] = None , cleanup : Optional [ bool ] = None , loglevel = logging . INFO ): \"\"\" Creates a new component LocalPipelineRunner for the provided pipeline configuration package. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset a temporary directory will be created. cleanup (bool): If set, the working directory will be kept when True, and deleted when False. \\ If unset, a temporary working directory will be removed, and an explicit working directory will be kept. \\ When an error occurs in a component, the working directory will be kept regardless of this value. \"\"\" self . package_zip : Path = Path ( packageZip ) self . path = path self . components = {} self . parameters = {} self . cleanup = cleanup self . log_level = loglevel self . workdir : Path self . report_writer = PipelineRunnerReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) def __enter__ ( self ): self . report_writer . set_package_zip_path ( self . package_zip ) timestamp = re . sub ( r \"[-:]\" , \"\" , datetime . datetime . utcnow () . isoformat ( sep = \"_\" , timespec = \"seconds\" )) if self . path is not None : self . workdir = Path ( self . path ) self . workdir . mkdir ( parents = True , exist_ok = True ) self . cleanup = self . cleanup if self . cleanup is not None else False else : self . workdir = Path ( tempfile . mkdtemp ( prefix = f \"LocalPipelineRunner_ { timestamp } _\" )) self . cleanup = self . cleanup if self . cleanup is not None else True unzip_components = False with zipfile . ZipFile ( self . package_zip ) as zf : if 'runtime_config.yml' in zf . namelist (): self . workdir = self . workdir / self . package_zip . stem zf . extractall ( path = self . workdir ) unzip_components = True else : zf . extractall ( path = self . workdir ) self . workdir = self . workdir / zf . namelist ()[ 0 ] try : with open ( self . workdir / \"pipeline_config.yml\" ) as cf : config = yaml . load ( cf , Loader = yaml . FullLoader ) components = config [ \"dataFlowPipeline\" ] . get ( \"components\" , []) for component in components : component [ \"context\" ] = None component [ \"env_dir\" ] = self . workdir / component [ 'name' ] self . components [ component [ \"name\" ]] = component if unzip_components : for component in components : component_zip = f \" { component [ 'name' ] } _ { component [ 'version' ] } .zip\" with zipfile . ZipFile ( self . workdir / 'components' / component_zip ) as zf : zf . extractall ( path = self . workdir / component [ \"name\" ]) for parameter in config [ \"dataFlowPipeline\" ] . get ( \"pipelineParameters\" , {}): self . parameters [ parameter [ \"name\" ]] = parameter except Exception : raise RuntimeError ( INVALID_PIPELINE_PACKAGE_MESSAGE ) return self def __exit__ ( self , exception_type , value , traceback ): self . update_telemetry_data () self . report_writer . write_report () if self . cleanup : _logger . info ( \"Removing local pipeline runner environment...\" ) shutil . rmtree ( self . workdir . parent ) else : _logger . info ( f \"Leaving local pipeline runner environment in its final state at ' { self . workdir } '\" ) def collect_telemetry_data ( self ): \"\"\" Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: dict: A dictionary containing the telemetry data. \"\"\" telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: { locals () } \" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ][ 'runtime' ][ 'version' ] for component in self . components if self . components [ component ][ 'runtime' ][ 'type' ] == 'python' )) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = [] for component_dir in [ Path ( self . workdir ) / c for c in Path ( self . workdir ) . rglob ( \"*\" ) if c . name in self . components . keys ()]: excluded_dirs = set ([ component_dir / '.venv' , component_dir / '__pyache__' ]) suffixes = list ( set ( f . suffix for f in component_dir . rglob ( \"*\" ) if not ( any ( excluded_dirs . intersection ( f . parents )) or f . suffix in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ]))) for suffix in suffixes : if suffix not in telemetry_data [ \"pipeline\" ][ \"file_extensions\" ]: telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] . append ( suffix ) return telemetry_data def update_telemetry_data ( self ): \"\"\" Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: None \"\"\" _logger . info ( \"Updating telemetry data and the package\" ) telemetry_path = self . workdir / \"telemetry_data.yml\" if telemetry_path . is_file (): telemetry_data = yaml . safe_load ( telemetry_path . read_text ()) else : telemetry_data = self . collect_telemetry_data () telemetry_data [ \"pipeline\" ][ \"last_run\" ] = datetime . datetime . now () . isoformat () telemetry_path . write_text ( yaml . dump ( telemetry_data )) config_package = False with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : for file in zip_read . namelist (): if TELEMETRY_YAML in file and file != TELEMETRY_YAML : config_package = True break new_zip_path = Path ( self . package_zip ) . parent / f \" { Path ( self . package_zip ) . stem } _tested.zip\" with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : with zipfile . ZipFile ( new_zip_path , 'w' ) as zip_write : for file in zip_read . namelist (): if TELEMETRY_YAML not in file : filepath = zip_read . extract ( file , path = self . workdir ) zip_write . write ( filepath , arcname = file ) else : zip_write . write ( telemetry_path , arcname = file ) if config_package : shutil . copy ( new_zip_path , self . package_zip ) else : new_sha_path = Path ( self . package_zip ) . parent / f \" { Path ( self . package_zip ) . stem } _tested.sha256\" new_sha_path . write_text ( calc_sha ( new_zip_path )) def _install_requirements ( self , component , package_dir , no_index : bool = True ): _logger . info ( \"Installing requirements...\" ) pip_report_file = component [ \"env_dir\" ] / \"pip_report.json\" executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) package_dir_path = _relative_to ( package_dir , component [ \"env_dir\" ]) pip_report_file_path = _relative_to ( pip_report_file , component [ \"env_dir\" ]) cmd = [ f \" { executable_path } \" , \"-m\" , \"pip\" , \"install\" , \"--no-warn-script-location\" , \"-f\" , f \" { package_dir_path } \" , \"-r\" , REQUIREMENTS_TXT , \"--report\" , f \" { pip_report_file_path } \" ] if no_index : cmd += [ \"--no-index\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if 0 == result . returncode and pip_report_file . is_file (): self . report_writer . add_installed_packages ( component [ \"name\" ], pip_report_file ) return result def _install_from_packages_zip ( self , component , package_dir ): result = self . _install_requirements ( component , package_dir , True ) return 0 == result . returncode def _install_from_pypi_org ( self , component , package_dir ): return self . _install_requirements ( component , package_dir , False ) def _init_component_venv ( self , component : dict ): \"\"\" Creates a virtual environment in which the given component can run. Args: component (str): name of the selected component. \"\"\" _logger . info ( f \"Creating virtual environment for component ' { component [ 'name' ] } '...\" ) context_dir = component [ \"env_dir\" ] / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( str ( context_dir )) component [ \"context\" ] = builder . ensure_directories ( context_dir ) component [ \"bin_path\" ] = Path ( component [ \"context\" ] . bin_path ) . absolute () _logger . debug ( f \"Component bin path: { component [ 'bin_path' ] } \" ) _logger . info ( \"Upgrading pip...\" ) executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \" { executable_path } \" , \"-m\" , \"pip\" , \"install\" , \"pip\" , \"--upgrade\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if result . returncode != 0 : self . cleanup = False _logger . warning ( f \"Error upgrading pip: \\n { result . stderr } \" ) try : result = self . _install_logmodule ( component [ \"bin_path\" ], component [ \"env_dir\" ]) except Exception as err : _logger . error ( err ) self . cleanup = False raise RuntimeError ( \"The 'simaticai' Python package is either not installed or does not contain package 'log_module'.\" ) from None if result . returncode != 0 : self . cleanup = False raise RuntimeError ( f \"Error installing log_module: \\n { result . stderr } \" ) req_list = Path ( component [ \"env_dir\" ] / \"requirements.list\" ) req_list . touch ( exist_ok = True ) if Path ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) . is_file (): dependencies , extra_index , index_url = parse_requirements ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) requirements = \"#\" . join ( dependencies . keys ()) req_list . write_text ( requirements ) else : _logger . info ( f \"' { REQUIREMENTS_TXT } ' was not found. No additional dependencies were installed.\" ) return package_dir = component [ \"env_dir\" ] / PYTHON_PACKAGES package_zip = component [ \"env_dir\" ] / PYTHON_PACKAGES_ZIP _logger . info ( f \"Extracting { PYTHON_PACKAGES_ZIP } \" ) if package_zip . is_file (): with zipfile . ZipFile ( package_zip ) as zf : zf . extractall ( path = package_dir . absolute ()) else : _logger . info ( f \"There is no { PYTHON_PACKAGES_ZIP } to extract.\" ) package_dir . mkdir ( parents = True , exist_ok = True ) success = self . _install_from_packages_zip ( component , package_dir ) if not success : msg = f \"Warning! Could not install dependencies from { PYTHON_PACKAGES_ZIP } . \" msg += \"Trying to install them from pypi.org. The resulting Python environment \" msg += \"may be significantly different than the targeted Python environment on the Edge Device!\" _logger . warning ( msg ) second_install_result = self . _install_from_pypi_org ( component , package_dir ) if 0 != second_install_result . returncode : self . cleanup = False raise RuntimeError ( f \"Error installing requirements: \\n { second_install_result . stderr } \" ) @staticmethod def _install_logmodule ( bin_path , env_dir ): _logger . info ( \"Installing LogModule...\" ) try : package_paths = importlib_metadata . files ( \"simaticai\" ) assert package_paths is not None logger_wheel = [ p for p in package_paths if 'log_module' in str ( p )][ 0 ] . locate () except Exception : from importlib.metadata import Distribution direct_url = Distribution . from_name ( \"simaticai\" ) . read_text ( \"direct_url.json\" ) assert direct_url is not None direct_url = json . loads ( direct_url )[ 'url' ] direct_url = direct_url . replace ( 'file://' , '' ) direct_url = Path ( direct_url ) / 'simaticai' / 'data' paths = list ( direct_url . rglob ( '*.whl' )) logger_wheel = [ p for p in paths if 'log_module' in str ( p )][ 0 ] . resolve () cmd = [ f \" { bin_path / 'pip' } \" , \"install\" , \"--no-warn-script-location\" , logger_wheel , \"joblib\" ] return subprocess . run ( cmd , cwd = env_dir , text = True , stderr = subprocess . PIPE ) def run_component ( self , name : str , data : Optional [ Union [ dict , list , DataStream ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Runs the component in its virtual environment with the given input. This environment is created according to `requirements.txt` in the package. Additionally 'joblib' and the mock `log_module` is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as `inputs.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: name (str): The name of the component to be executed. data (dict or list): One or more input records for the component. Returns: dict / list: A list of dictionaries for outputs if there were no errors and field `ready` is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. \"\"\" assert name in self . components , f \"Invalid component name: { name } \" component = self . components [ name ] assert component [ \"runtime\" ][ \"type\" ] in [ \"python\" , \"gpuruntime\" ], f \"Can not run component ' { name } ': Runtime type is nor 'python' or 'gpuruntime'\" input_payload_path : Path = component [ \"env_dir\" ] / \"input.joblib\" output_payload_path : Path = component [ \"env_dir\" ] / \"output.joblib\" batch_input : bool = component [ \"batch\" ][ \"inputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False batch_output : bool = component [ \"batch\" ][ \"outputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False # Validate and serialize input payload assert data is not None , f \"Can not run component ' { name } ' without input.\" result_is_list = True if isinstance ( data , list ): input_payload = data elif isinstance ( data , DataStream ): input_payload = [ item for item in data ] else : result_is_list = False input_payload = [ data ] validate_payload ( input_payload , component [ \"inputType\" ], batch_input ) joblib . dump ( input_payload , input_payload_path ) self . report_writer . set_input_payload_length ( name , len ( input_payload )) _logger . info ( f \"Input payload saved as ' { input_payload_path } '\" ) # Assemble command for runnig component if component [ 'runtime' ][ 'type' ] == 'python' : # Version check for Python e_major , e_minor , _ , _ , _ = sys . version_info c_major , c_minor , * _ = tuple ( str ( component [ \"runtime\" ][ \"version\" ]) . split ( '.' )) if f \" { e_major } . { e_minor } \" != f \" { c_major } . { c_minor } \" : msg = f \"The local python version ( { e_major } . { e_minor } ) and the python version defined for the component ( { c_major } . { c_minor } ) are different.\" msg += \" Testing will be done using dependencies that corresponds to the python version of your development environment.\" msg += \" Pipeline behavior on AI Inference Server might be different.\" _logger . warning ( msg ) if component [ \"context\" ] is None : self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_component.py' , component [ \"env_dir\" ]) req_list_path = _relative_to ( component [ \"env_dir\" ] / \"requirements.list\" , component [ \"env_dir\" ]) json_params = json . dumps ({ param [ \"name\" ]: param [ \"defaultValue\" ] for param in self . parameters . values ()}) args = [ \"-m\" , 'run_component' , \"-m\" , Path ( component [ \"entrypoint\" ]) . stem , \"-p\" , f \" { json_params } \" , \"-r\" , f \" { req_list_path } \" , ] else : # gpuruntime step requires Python environment with onnxruntime installed # TODO: check gpuruntime version if needed if component [ \"context\" ] is None : shutil . copy ( _runner_path / 'gpuruntime_requirements.txt' , component [ \"env_dir\" ] / REQUIREMENTS_TXT ) self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_gpuruntime_component.py' , component [ \"env_dir\" ]) shutil . copy ( _runner_path . parent / 'model_config_pb2.py' , component [ \"env_dir\" ]) args = [ \"-m\" , 'run_gpuruntime_component' , \"-m\" , component [ \"entrypoint\" ], \"-c\" , \"config.pbtxt\" , ] args += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . log_level ) ] # Run the component in the created Python environment executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \" { executable_path } \" ] + args _logger . info ( f \"Running command: { subprocess . list2cmdline ( cmd ) } \" ) p = subprocess . Popen ( cmd , cwd = component [ \"env_dir\" ], stdout = subprocess . PIPE , stderr = subprocess . STDOUT , text = True ) if p . stdout is not None : for line in p . stdout : _logger . info ( line . strip ()) returncode = p . wait () if returncode not in [ RETURN_CODE_OK , RETURN_CODE_DEPRECATED ]: self . cleanup = False raise RuntimeError ( f \"\"\" \\ There was an error while running component ' { name } '. You can check the test results in directory ' { component [ 'env_dir' ] } ' \"\"\" ) else : response_wrapped = True if returncode == RETURN_CODE_DEPRECATED else False # Deserialize and validate output payload _logger . info ( f \"Loading output payload from ' { output_payload_path } '\" ) output_payload = joblib . load ( output_payload_path ) output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] self . report_writer . set_output_payload_length ( name , len ( output_payload )) if response_wrapped : output_payload = [ json . loads ( item [ 'output' ]) for item in output_payload if item [ 'ready' ]] else : output_payload = [ output for output in output_payload if output is not None ] validate_payload ( output_payload , component [ \"outputType\" ], batch_output ) # Only return the first item if the input was a single item, or None if there are no valid results. if result_is_list : return output_payload return output_payload [ 0 ] if len ( output_payload ) > 0 else None def update_parameters ( self , parameters : dict ): \"\"\" Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Args: parameters (dict): names and values of parameters to update Raises: AssertionError: When: - either `name` is not in the configured parameters, - or `defaultValue` type is different from the configured one \"\"\" for key , value in parameters . items (): if key not in self . parameters . keys () or self . parameters [ key ][ \"type\" ] != type_map . get ( type ( value ) . __name__ ): raise AssertionError ( f \"Pipeline has no parameters with the name ' { key } ' and type ' { type_map . get ( type ( value ) . __name__ ) } '\" ) for key , value in parameters . items (): self . parameters [ key ][ \"defaultValue\" ] = value def run_pipeline ( self , payload : Optional [ Union [ dict , list , DataStream ]] = {}) -> Optional [ Union [ dict , list ]]: \"\"\" Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as `input.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: payload (dict or list): One or more input records for the pipeline. Returns: The output of the last component. \"\"\" for name in self . components . keys (): payload = self . run_component ( name , payload ) return payload def validate_payload ( data : list , variables : list , batch : bool , logger = _logger ): \"\"\" Validates that data is a valid list of input or output payload items. Variables list what variables each playload item has. Batch indicates if the payload items are themselves batches of items or not. \"\"\" assert isinstance ( data , list ), \"payload data must be a 'list'\" for i in data : if batch : assert isinstance ( i , list ), \"batch payload items must be 'list' instances\" else : i = [ i ] for j in i : validate_payload_item ( j , variables , logger ) def validate_payload_item ( data : dict , variables : list , logger ): \"\"\" Validates that data is a valid payload item. Variables listed must have a corresponding field in data. The types of the values must match their declared type. \"\"\" assert isinstance ( data , dict ), \"payload items must be 'dict' isntances\" for variable in variables : name = variable [ \"name\" ] value = data . get ( name , None ) if value is None : logger . warning ( f \"WARNING! Variable ' { name } ' is missing from input, output or metric\" ) continue if variable [ \"type\" ] == \"String\" : assert isinstance ( value , str ), \"'String' value must be an 'str'\" if variable [ \"type\" ] == \"StringArray\" : assert isinstance ( value , list ), \"'StringArray' value must be a 'list'\" assert all ( isinstance ( i , str ) for i in value ), \"'StringArray' items must be 'str' isntances\" if variable [ \"type\" ] == \"Object\" : assert isinstance ( value , dict ), \"'Object' value must be a 'dict'\" values = list ( value . values ()) assert len ( values ) == 2 , \"'Object' value must have exactly 2 items\" ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) assert ok , \"'Object' value must have exactly one 'str' and one 'bytes' field\" payload_names = set ( data . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: { extra_variables } \" ) Variables INVALID_PIPELINE_PACKAGE_MESSAGE MSG_NOT_FOUND PYTHON_PACKAGES PYTHON_PACKAGES_ZIP REQUIREMENTS_TXT RETURN_CODE_DEPRECATED RETURN_CODE_OK TELEMETRY_YAML type_map Functions validate_payload def validate_payload ( data : list , variables : list , batch : bool , logger =< Logger simaticai . testing . pipeline_runner ( INFO ) > ) Validates that data is a valid list of input or output payload items. Variables list what variables each playload item has. Batch indicates if the payload items are themselves batches of items or not. View Source def validate_payload ( data : list , variables : list , batch : bool , logger = _logger ): \"\"\" Validates that data is a valid list of input or output payload items. Variables list what variables each playload item has. Batch indicates if the payload items are themselves batches of items or not. \"\"\" assert isinstance ( data , list ), \"payload data must be a 'list'\" for i in data : if batch : assert isinstance ( i , list ), \"batch payload items must be 'list' instances\" else : i = [ i ] for j in i : validate_payload_item ( j , variables , logger ) validate_payload_item def validate_payload_item ( data : dict , variables : list , logger ) Validates that data is a valid payload item. Variables listed must have a corresponding field in data. The types of the values must match their declared type. View Source def validate_payload_item ( data : dict , variables : list , logger ): \"\"\" Validates that data is a valid payload item. Variables listed must have a corresponding field in data. The types of the values must match their declared type. \"\"\" assert isinstance ( data , dict ), \"payload items must be 'dict' isntances\" for variable in variables : name = variable [ \"name\" ] value = data . get ( name , None ) if value is None : logger . warning ( f \"WARNING! Variable '{name}' is missing from input, output or metric\" ) continue if variable [ \"type\" ] == \"String\" : assert isinstance ( value , str ), \"'String' value must be an 'str'\" if variable [ \"type\" ] == \"StringArray\" : assert isinstance ( value , list ), \"'StringArray' value must be a 'list'\" assert all ( isinstance ( i , str ) for i in value ), \"'StringArray' items must be 'str' isntances\" if variable [ \"type\" ] == \"Object\" : assert isinstance ( value , dict ), \"'Object' value must be a 'dict'\" values = list ( value . values ()) assert len ( values ) == 2 , \"'Object' value must have exactly 2 items\" ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) assert ok , \"'Object' value must have exactly one 'str' and one 'bytes' field\" payload_names = set ( data . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: {extra_variables}\" ) Classes LocalPipelineRunner Simulates the execution of a pipeline in a local Python environment. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Restriction: only linear pipelines are supported where the pipeline input variables are only used by one component, each component uses only the outputs of the previous components, and the pipeline output only consists of variables from the last component. If the caller specifies no path , the working directory is temporary and is removed unless an error occurs. If the caller specifies a working directory with a path argument, the working directory is kept. This behavior can be overridden using boolean parameter cleanup . Currently, the pipeline runner supports both the current process_input(data: dict) entrypoint signature and the legacy run(data: str) signature. If both entrypoints are present, process_input() takes precedence. Please note however that run() is deprecated, and support for it will be removed in future versions of the pipeline runner. class LocalPipelineRunner ( packageZip : os . PathLike , path : Optional [ os . PathLike ] = None , cleanup : Optional [ bool ] = None , loglevel = 20 ) Attributes Name Type Description Default packageZip path-like Path to the pipeline configuration package. None path path-like Path to the working directory. If unset, a temporary directory is created. None cleanup bool If set, the working directory is kept when True, and deleted when False. If unset, a temporary working directory is removed, and an explicit working directory is kept. When an error occurs in a component, the working directory is kept regardless of this value. None View Source class LocalPipelineRunner : \"\"\" Simulates the execution of a pipeline in a local Python environment. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Restriction: only linear pipelines are supported where the pipeline input variables are only used by one component, each component uses only the outputs of the previous components, and the pipeline output only consists of variables from the last component. If the caller specifies no `path`, the working directory is temporary and is removed unless an error occurs. If the caller specifies a working directory with a `path` argument, the working directory is kept. This behavior can be overridden using boolean parameter `cleanup`. Currently, the pipeline runner supports both the current `process_input(data: dict)` entrypoint signature and the legacy `run(data: str)` signature. If both entrypoints are present, `process_input()` takes precedence. Please note however that `run()` is deprecated, and support for it will be removed in future versions of the pipeline runner. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset, a temporary directory is created. cleanup (bool): If set, the working directory is kept when True, and deleted when False. \\ If unset, a temporary working directory is removed, and an explicit working directory is kept. \\ When an error occurs in a component, the working directory is kept regardless of this value. \"\"\" def __init__ ( self , packageZip : os . PathLike , path : Optional [ os . PathLike ] = None , cleanup : Optional [ bool ] = None , loglevel = logging . INFO ): \"\"\" Creates a new component LocalPipelineRunner for the provided pipeline configuration package. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset a temporary directory will be created. cleanup (bool): If set, the working directory will be kept when True, and deleted when False. \\ If unset, a temporary working directory will be removed, and an explicit working directory will be kept. \\ When an error occurs in a component, the working directory will be kept regardless of this value. \"\"\" self . package_zip : Path = Path ( packageZip ) self . path = path self . components = {} self . parameters = {} self . cleanup = cleanup self . log_level = loglevel self . workdir : Path self . report_writer = PipelineRunnerReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) def __enter__ ( self ): self . report_writer . set_package_zip_path ( self . package_zip ) timestamp = re . sub ( r \"[-:]\" , \"\" , datetime . datetime . utcnow () . isoformat ( sep = \"_\" , timespec = \"seconds\" )) if self . path is not None : self . workdir = Path ( self . path ) self . workdir . mkdir ( parents = True , exist_ok = True ) self . cleanup = self . cleanup if self . cleanup is not None else False else : self . workdir = Path ( tempfile . mkdtemp ( prefix = f \"LocalPipelineRunner_{timestamp}_\" )) self . cleanup = self . cleanup if self . cleanup is not None else True unzip_components = False with zipfile . ZipFile ( self . package_zip ) as zf : if 'runtime_config.yml' in zf . namelist (): self . workdir = self . workdir / self . package_zip . stem zf . extractall ( path = self . workdir ) unzip_components = True else : zf . extractall ( path = self . workdir ) self . workdir = self . workdir / zf . namelist ()[ 0 ] try : with open ( self . workdir / \"pipeline_config.yml\" ) as cf : config = yaml . load ( cf , Loader = yaml . FullLoader ) components = config [ \"dataFlowPipeline\" ] . get ( \"components\" , []) for component in components : component [ \"context\" ] = None component [ \"env_dir\" ] = self . workdir / component [ 'name' ] self . components [ component [ \"name\" ]] = component if unzip_components : for component in components : component_zip = f \"{component['name']}_{component['version']}.zip\" with zipfile . ZipFile ( self . workdir / 'components' / component_zip ) as zf : zf . extractall ( path = self . workdir / component [ \"name\" ]) for parameter in config [ \"dataFlowPipeline\" ] . get ( \"pipelineParameters\" , {}): self . parameters [ parameter [ \"name\" ]] = parameter except Exception : raise RuntimeError ( INVALID_PIPELINE_PACKAGE_MESSAGE ) return self def __exit__ ( self , exception_type , value , traceback ): self . update_telemetry_data () self . report_writer . write_report () if self . cleanup : _logger . info ( \"Removing local pipeline runner environment...\" ) shutil . rmtree ( self . workdir . parent ) else : _logger . info ( f \"Leaving local pipeline runner environment in its final state at '{self.workdir}'\" ) def collect_telemetry_data ( self ): \"\"\" Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: dict: A dictionary containing the telemetry data. \"\"\" telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ][ 'runtime' ][ 'version' ] for component in self . components if self . components [ component ][ 'runtime' ][ 'type' ] == 'python' )) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = [] for component_dir in [ Path ( self . workdir ) / c for c in Path ( self . workdir ) . rglob ( \"*\" ) if c . name in self . components . keys ()]: excluded_dirs = set ([ component_dir / '.venv' , component_dir / '__pyache__' ]) suffixes = list ( set ( f . suffix for f in component_dir . rglob ( \"*\" ) if not ( any ( excluded_dirs . intersection ( f . parents )) or f . suffix in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ]))) for suffix in suffixes : if suffix not in telemetry_data [ \"pipeline\" ][ \"file_extensions\" ]: telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] . append ( suffix ) return telemetry_data def update_telemetry_data ( self ): \"\"\" Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: None \"\"\" _logger . info ( \"Updating telemetry data and the package\" ) telemetry_path = self . workdir / \"telemetry_data.yml\" if telemetry_path . is_file (): telemetry_data = yaml . safe_load ( telemetry_path . read_text ()) else : telemetry_data = self . collect_telemetry_data () telemetry_data [ \"pipeline\" ][ \"last_run\" ] = datetime . datetime . now () . isoformat () telemetry_path . write_text ( yaml . dump ( telemetry_data )) config_package = False with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : for file in zip_read . namelist (): if TELEMETRY_YAML in file and file != TELEMETRY_YAML : config_package = True break new_zip_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.zip\" with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : with zipfile . ZipFile ( new_zip_path , 'w' ) as zip_write : for file in zip_read . namelist (): if TELEMETRY_YAML not in file : filepath = zip_read . extract ( file , path = self . workdir ) zip_write . write ( filepath , arcname = file ) else : zip_write . write ( telemetry_path , arcname = file ) if config_package : shutil . copy ( new_zip_path , self . package_zip ) else : new_sha_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.sha256\" new_sha_path . write_text ( calc_sha ( new_zip_path )) def _install_requirements ( self , component , package_dir , no_index : bool = True ): _logger . info ( \"Installing requirements...\" ) pip_report_file = component [ \"env_dir\" ] / \"pip_report.json\" executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) package_dir_path = _relative_to ( package_dir , component [ \"env_dir\" ]) pip_report_file_path = _relative_to ( pip_report_file , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" , \"-m\" , \"pip\" , \"install\" , \"--no-warn-script-location\" , \"-f\" , f \"{package_dir_path}\" , \"-r\" , REQUIREMENTS_TXT , \"--report\" , f \"{pip_report_file_path}\" ] if no_index : cmd += [ \"--no-index\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if 0 == result . returncode and pip_report_file . is_file (): self . report_writer . add_installed_packages ( component [ \"name\" ], pip_report_file ) return result def _install_from_packages_zip ( self , component , package_dir ): result = self . _install_requirements ( component , package_dir , True ) return 0 == result . returncode def _install_from_pypi_org ( self , component , package_dir ): return self . _install_requirements ( component , package_dir , False ) def _init_component_venv ( self , component : dict ): \"\"\" Creates a virtual environment in which the given component can run. Args: component (str): name of the selected component. \"\"\" _logger . info ( f \"Creating virtual environment for component '{component['name']}'...\" ) context_dir = component [ \"env_dir\" ] / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( str ( context_dir )) component [ \"context\" ] = builder . ensure_directories ( context_dir ) component [ \"bin_path\" ] = Path ( component [ \"context\" ] . bin_path ) . absolute () _logger . debug ( f \"Component bin path: {component['bin_path']}\" ) _logger . info ( \"Upgrading pip...\" ) executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" , \"-m\" , \"pip\" , \"install\" , \"pip\" , \"--upgrade\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if result . returncode != 0 : self . cleanup = False _logger . warning ( f \"Error upgrading pip: \\n {result.stderr}\" ) try : result = self . _install_logmodule ( component [ \"bin_path\" ], component [ \"env_dir\" ]) except Exception as err : _logger . error ( err ) self . cleanup = False raise RuntimeError ( \"The 'simaticai' Python package is either not installed or does not contain package 'log_module'.\" ) from None if result . returncode != 0 : self . cleanup = False raise RuntimeError ( f \"Error installing log_module: \\n {result.stderr}\" ) req_list = Path ( component [ \"env_dir\" ] / \"requirements.list\" ) req_list . touch ( exist_ok = True ) if Path ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) . is_file (): dependencies , extra_index , index_url = parse_requirements ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) requirements = \"#\" . join ( dependencies . keys ()) req_list . write_text ( requirements ) else : _logger . info ( f \"'{REQUIREMENTS_TXT}' was not found. No additional dependencies were installed.\" ) return package_dir = component [ \"env_dir\" ] / PYTHON_PACKAGES package_zip = component [ \"env_dir\" ] / PYTHON_PACKAGES_ZIP _logger . info ( f \"Extracting {PYTHON_PACKAGES_ZIP}\" ) if package_zip . is_file (): with zipfile . ZipFile ( package_zip ) as zf : zf . extractall ( path = package_dir . absolute ()) else : _logger . info ( f \"There is no {PYTHON_PACKAGES_ZIP} to extract.\" ) package_dir . mkdir ( parents = True , exist_ok = True ) success = self . _install_from_packages_zip ( component , package_dir ) if not success : msg = f \"Warning! Could not install dependencies from {PYTHON_PACKAGES_ZIP}. \" msg += \"Trying to install them from pypi.org. The resulting Python environment \" msg += \"may be significantly different than the targeted Python environment on the Edge Device!\" _logger . warning ( msg ) second_install_result = self . _install_from_pypi_org ( component , package_dir ) if 0 != second_install_result . returncode : self . cleanup = False raise RuntimeError ( f \"Error installing requirements: \\n {second_install_result.stderr}\" ) @ staticmethod def _install_logmodule ( bin_path , env_dir ): _logger . info ( \"Installing LogModule...\" ) try : package_paths = importlib_metadata . files ( \"simaticai\" ) assert package_paths is not None logger_wheel = [ p for p in package_paths if 'log_module' in str ( p )][ 0 ] . locate () except Exception : from importlib . metadata import Distribution direct_url = Distribution . from_name ( \"simaticai\" ) . read_text ( \"direct_url.json\" ) assert direct_url is not None direct_url = json . loads ( direct_url )[ 'url' ] direct_url = direct_url . replace ( 'file://' , '' ) direct_url = Path ( direct_url ) / 'simaticai' / 'data' paths = list ( direct_url . rglob ( '*.whl' )) logger_wheel = [ p for p in paths if 'log_module' in str ( p )][ 0 ] . resolve () cmd = [ f \"{bin_path / 'pip'}\" , \"install\" , \"--no-warn-script-location\" , logger_wheel , \"joblib\" ] return subprocess . run ( cmd , cwd = env_dir , text = True , stderr = subprocess . PIPE ) def run_component ( self , name : str , data : Optional [ Union [ dict , list , DataStream ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Runs the component in its virtual environment with the given input. This environment is created according to `requirements.txt` in the package. Additionally 'joblib' and the mock `log_module` is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as `inputs.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: name (str): The name of the component to be executed. data (dict or list): One or more input records for the component. Returns: dict / list: A list of dictionaries for outputs if there were no errors and field `ready` is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. \"\"\" assert name in self . components , f \"Invalid component name: {name}\" component = self . components [ name ] assert component [ \"runtime\" ][ \"type\" ] in [ \"python\" , \"gpuruntime\" ], f \"Can not run component '{name}': Runtime type is nor 'python' or 'gpuruntime'\" input_payload_path : Path = component [ \"env_dir\" ] / \"input.joblib\" output_payload_path : Path = component [ \"env_dir\" ] / \"output.joblib\" batch_input : bool = component [ \"batch\" ][ \"inputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False batch_output : bool = component [ \"batch\" ][ \"outputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False # Validate and serialize input payload assert data is not None , f \"Can not run component '{name}' without input.\" result_is_list = True if isinstance ( data , list ): input_payload = data elif isinstance ( data , DataStream ): input_payload = [ item for item in data ] else : result_is_list = False input_payload = [ data ] validate_payload ( input_payload , component [ \"inputType\" ], batch_input ) joblib . dump ( input_payload , input_payload_path ) self . report_writer . set_input_payload_length ( name , len ( input_payload )) _logger . info ( f \"Input payload saved as '{input_payload_path}'\" ) # Assemble command for runnig component if component [ 'runtime' ][ 'type' ] == 'python' : # Version check for Python e_major , e_minor , _ , _ , _ = sys . version_info c_major , c_minor , * _ = tuple ( str ( component [ \"runtime\" ][ \"version\" ]) . split ( '.' )) if f \"{e_major}.{e_minor}\" != f \"{c_major}.{c_minor}\" : msg = f \"The local python version ({e_major}.{e_minor}) and the python version defined for the component ({c_major}.{c_minor}) are different.\" msg += \" Testing will be done using dependencies that corresponds to the python version of your development environment.\" msg += \" Pipeline behavior on AI Inference Server might be different.\" _logger . warning ( msg ) if component [ \"context\" ] is None : self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_component.py' , component [ \"env_dir\" ]) req_list_path = _relative_to ( component [ \"env_dir\" ] / \"requirements.list\" , component [ \"env_dir\" ]) json_params = json . dumps ({ param [ \"name\" ]: param [ \"defaultValue\" ] for param in self . parameters . values ()}) args = [ \"-m\" , 'run_component' , \"-m\" , Path ( component [ \"entrypoint\" ]) . stem , \"-p\" , f \"{json_params}\" , \"-r\" , f \"{req_list_path}\" , ] else : # gpuruntime step requires Python environment with onnxruntime installed # TODO: check gpuruntime version if needed if component [ \"context\" ] is None : shutil . copy ( _runner_path / 'gpuruntime_requirements.txt' , component [ \"env_dir\" ] / REQUIREMENTS_TXT ) self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_gpuruntime_component.py' , component [ \"env_dir\" ]) shutil . copy ( _runner_path . parent / 'model_config_pb2.py' , component [ \"env_dir\" ]) args = [ \"-m\" , 'run_gpuruntime_component' , \"-m\" , component [ \"entrypoint\" ], \"-c\" , \"config.pbtxt\" , ] args += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . log_level ) ] # Run the component in the created Python environment executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" ] + args _logger . info ( f \"Running command: {subprocess.list2cmdline(cmd)}\" ) p = subprocess . Popen ( cmd , cwd = component [ \"env_dir\" ], stdout = subprocess . PIPE , stderr = subprocess . STDOUT , text = True ) if p . stdout is not None : for line in p . stdout : _logger . info ( line . strip ()) returncode = p . wait () if returncode not in [ RETURN_CODE_OK , RETURN_CODE_DEPRECATED ]: self . cleanup = False raise RuntimeError ( f \"\"\" \\ There was an error while running component '{name}'. You can check the test results in directory '{component['env_dir']}' \"\"\" ) else : response_wrapped = True if returncode == RETURN_CODE_DEPRECATED else False # Deserialize and validate output payload _logger . info ( f \"Loading output payload from '{output_payload_path}'\" ) output_payload = joblib . load ( output_payload_path ) output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] self . report_writer . set_output_payload_length ( name , len ( output_payload )) if response_wrapped : output_payload = [ json . loads ( item [ 'output' ]) for item in output_payload if item [ 'ready' ]] else : output_payload = [ output for output in output_payload if output is not None ] validate_payload ( output_payload , component [ \"outputType\" ], batch_output ) # Only return the first item if the input was a single item, or None if there are no valid results. if result_is_list : return output_payload return output_payload [ 0 ] if len ( output_payload ) > 0 else None def update_parameters ( self , parameters : dict ): \"\"\" Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Args: parameters (dict): names and values of parameters to update Raises: AssertionError: When: - either `name` is not in the configured parameters, - or `defaultValue` type is different from the configured one \"\"\" for key , value in parameters . items (): if key not in self . parameters . keys () or self . parameters [ key ][ \"type\" ] != type_map . get ( type ( value ) . __name__ ): raise AssertionError ( f \"Pipeline has no parameters with the name '{key}' and type '{type_map.get(type(value).__name__)}'\" ) for key , value in parameters . items (): self . parameters [ key ][ \"defaultValue\" ] = value def run_pipeline ( self , payload : Optional [ Union [ dict , list , DataStream ]] = {}) -> Optional [ Union [ dict , list ]]: \"\"\" Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as `input.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: payload (dict or list): One or more input records for the pipeline. Returns: The output of the last component. \"\"\" for name in self . components . keys (): payload = self . run_component ( name , payload ) return payload Methods collect_telemetry_data def collect_telemetry_data ( self ) Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: Type Description dict A dictionary containing the telemetry data. View Source def collect_telemetry_data ( self ) : \"\"\" Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: dict: A dictionary containing the telemetry data. \"\"\" telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\", \"get_ipython\" ] ) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ). version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ). version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ][ 'runtime' ][ 'version' ] for component in self . components if self . components [ component ][ 'runtime' ][ 'type' ] == 'python' )) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = [] for component_dir in [ Path(self.workdir) / c for c in Path(self.workdir).rglob(\"*\") if c.name in self.components.keys() ] : excluded_dirs = set ( [ component_dir / '.venv', component_dir / '__pyache__' ] ) suffixes = list ( set ( f . suffix for f in component_dir . rglob ( \"*\" ) if not ( any ( excluded_dirs . intersection ( f . parents )) or f . suffix in [ \"\", \".zip\", \".yml\", \".yaml\", \".html\" ] ))) for suffix in suffixes : if suffix not in telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] : telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] . append ( suffix ) return telemetry_data run_component def run_component ( self , name : str , data : Union [ dict , list , simaticai . testing . data_stream . DataStream , NoneType ] ) -> Union [ dict , list , NoneType ] Runs the component in its virtual environment with the given input. This environment is created according to requirements.txt in the package. Additionally 'joblib' and the mock log_module is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as inputs.joblib in the component runtime directory, and the output is saved as output.joblib . Parameters: Name Type Description Default name str The name of the component to be executed. None data dict or list One or more input records for the component. None Returns: Type Description None dict / list: A list of dictionaries for outputs if there were no errors and field ready is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. View Source def run_component ( self , name : str , data : Optional [ Union [ dict , list , DataStream ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Runs the component in its virtual environment with the given input. This environment is created according to `requirements.txt` in the package. Additionally 'joblib' and the mock `log_module` is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as `inputs.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: name (str): The name of the component to be executed. data (dict or list): One or more input records for the component. Returns: dict / list: A list of dictionaries for outputs if there were no errors and field `ready` is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. \"\"\" assert name in self . components , f \"Invalid component name: {name}\" component = self . components [ name ] assert component [ \"runtime\" ][ \"type\" ] in [ \"python\" , \"gpuruntime\" ], f \"Can not run component '{name}': Runtime type is nor 'python' or 'gpuruntime'\" input_payload_path : Path = component [ \"env_dir\" ] / \"input.joblib\" output_payload_path : Path = component [ \"env_dir\" ] / \"output.joblib\" batch_input : bool = component [ \"batch\" ][ \"inputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False batch_output : bool = component [ \"batch\" ][ \"outputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False # Validate and serialize input payload assert data is not None , f \"Can not run component '{name}' without input.\" result_is_list = True if isinstance ( data , list ): input_payload = data elif isinstance ( data , DataStream ): input_payload = [ item for item in data ] else : result_is_list = False input_payload = [ data ] validate_payload ( input_payload , component [ \"inputType\" ], batch_input ) joblib . dump ( input_payload , input_payload_path ) self . report_writer . set_input_payload_length ( name , len ( input_payload )) _logger . info ( f \"Input payload saved as '{input_payload_path}'\" ) # Assemble command for runnig component if component [ 'runtime' ][ 'type' ] == 'python' : # Version check for Python e_major , e_minor , _ , _ , _ = sys . version_info c_major , c_minor , * _ = tuple ( str ( component [ \"runtime\" ][ \"version\" ]) . split ( '.' )) if f \"{e_major}.{e_minor}\" != f \"{c_major}.{c_minor}\" : msg = f \"The local python version ({e_major}.{e_minor}) and the python version defined for the component ({c_major}.{c_minor}) are different.\" msg += \" Testing will be done using dependencies that corresponds to the python version of your development environment.\" msg += \" Pipeline behavior on AI Inference Server might be different.\" _logger . warning ( msg ) if component [ \"context\" ] is None : self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_component.py' , component [ \"env_dir\" ]) req_list_path = _relative_to ( component [ \"env_dir\" ] / \"requirements.list\" , component [ \"env_dir\" ]) json_params = json . dumps ({ param [ \"name\" ]: param [ \"defaultValue\" ] for param in self . parameters . values ()}) args = [ \"-m\" , 'run_component' , \"-m\" , Path ( component [ \"entrypoint\" ]) . stem , \"-p\" , f \"{json_params}\" , \"-r\" , f \"{req_list_path}\" , ] else : # gpuruntime step requires Python environment with onnxruntime installed # TODO: check gpuruntime version if needed if component [ \"context\" ] is None : shutil . copy ( _runner_path / 'gpuruntime_requirements.txt' , component [ \"env_dir\" ] / REQUIREMENTS_TXT ) self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_gpuruntime_component.py' , component [ \"env_dir\" ]) shutil . copy ( _runner_path . parent / 'model_config_pb2.py' , component [ \"env_dir\" ]) args = [ \"-m\" , 'run_gpuruntime_component' , \"-m\" , component [ \"entrypoint\" ], \"-c\" , \"config.pbtxt\" , ] args += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . log_level ) ] # Run the component in the created Python environment executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" ] + args _logger . info ( f \"Running command: {subprocess.list2cmdline(cmd)}\" ) p = subprocess . Popen ( cmd , cwd = component [ \"env_dir\" ], stdout = subprocess . PIPE , stderr = subprocess . STDOUT , text = True ) if p . stdout is not None : for line in p . stdout : _logger . info ( line . strip ()) returncode = p . wait () if returncode not in [ RETURN_CODE_OK , RETURN_CODE_DEPRECATED ]: self . cleanup = False raise RuntimeError ( f \"\"\" \\ There was an error while running component '{name}'. You can check the test results in directory '{component['env_dir']}' \"\"\" ) else : response_wrapped = True if returncode == RETURN_CODE_DEPRECATED else False # Deserialize and validate output payload _logger . info ( f \"Loading output payload from '{output_payload_path}'\" ) output_payload = joblib . load ( output_payload_path ) output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] self . report_writer . set_output_payload_length ( name , len ( output_payload )) if response_wrapped : output_payload = [ json . loads ( item [ 'output' ]) for item in output_payload if item [ 'ready' ]] else : output_payload = [ output for output in output_payload if output is not None ] validate_payload ( output_payload , component [ \"outputType\" ], batch_output ) # Only return the first item if the input was a single item, or None if there are no valid results. if result_is_list : return output_payload return output_payload [ 0 ] if len ( output_payload ) > 0 else None run_pipeline def run_pipeline ( self , payload : Union [ dict , list , simaticai . testing . data_stream . DataStream , NoneType ] = {} ) -> Union [ dict , list , NoneType ] Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as input.joblib in the component runtime directory, and the output is saved as output.joblib . Parameters: Name Type Description Default payload dict or list One or more input records for the pipeline. None Returns: Type Description None The output of the last component. View Source def run_pipeline ( self , payload : Optional [ Union [ dict , list , DataStream ]] = {}) -> Optional [ Union [ dict , list ]]: \"\"\" Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as `input.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: payload (dict or list): One or more input records for the pipeline. Returns: The output of the last component. \"\"\" for name in self . components . keys (): payload = self . run_component ( name , payload ) return payload update_parameters def update_parameters ( self , parameters : dict ) Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Parameters: Name Type Description Default parameters dict names and values of parameters to update None Raises: Type Description AssertionError When: - either name is not in the configured parameters, - or defaultValue type is different from the configured one View Source def update_parameters ( self , parameters : dict ): \"\"\" Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Args: parameters (dict): names and values of parameters to update Raises: AssertionError: When: - either `name` is not in the configured parameters, - or `defaultValue` type is different from the configured one \"\"\" for key , value in parameters . items (): if key not in self . parameters . keys () or self . parameters [ key ][ \"type\" ] != type_map . get ( type ( value ). __name__ ): raise AssertionError ( f \"Pipeline has no parameters with the name '{key}' and type '{type_map.get(type(value).__name__)}'\" ) for key , value in parameters . items (): self . parameters [ key ][ \"defaultValue\" ] = value update_telemetry_data def update_telemetry_data ( self ) Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: Type Description None None View Source def update_telemetry_data ( self ): \"\"\" Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: None \"\"\" _logger . info ( \"Updating telemetry data and the package\" ) telemetry_path = self . workdir / \"telemetry_data.yml\" if telemetry_path . is_file (): telemetry_data = yaml . safe_load ( telemetry_path . read_text ()) else : telemetry_data = self . collect_telemetry_data () telemetry_data [ \"pipeline\" ][ \"last_run\" ] = datetime . datetime . now () . isoformat () telemetry_path . write_text ( yaml . dump ( telemetry_data )) config_package = False with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : for file in zip_read . namelist (): if TELEMETRY_YAML in file and file != TELEMETRY_YAML : config_package = True break new_zip_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.zip\" with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : with zipfile . ZipFile ( new_zip_path , 'w' ) as zip_write : for file in zip_read . namelist (): if TELEMETRY_YAML not in file : filepath = zip_read . extract ( file , path = self . workdir ) zip_write . write ( filepath , arcname = file ) else : zip_write . write ( telemetry_path , arcname = file ) if config_package : shutil . copy ( new_zip_path , self . package_zip ) else : new_sha_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.sha256\" new_sha_path . write_text ( calc_sha ( new_zip_path ))","title":"Pipeline Runner"},{"location":"reference/simaticai/testing/pipeline_runner.html#module-simaticaitestingpipeline_runner","text":"A pipeline runner that lets you simulate the execution of a pipeline in a local Python environment. It can be used to locally mimic the behavior of the AI Inference Server concerning loading and running inference pipelines. This is a quick and easy way to find programming or configuration errors before deploying the package. The local pipeline runner also lets you exercise your pipeline component by component. In other words, you can feed single components with inputs and verify the output produced. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" A pipeline runner that lets you simulate the execution of a pipeline in a local Python environment. It can be used to locally mimic the behavior of the AI Inference Server concerning loading and running inference pipelines. This is a quick and easy way to find programming or configuration errors before deploying the package. The local pipeline runner also lets you exercise your pipeline component by component. In other words, you can feed single components with inputs and verify the output produced. \"\"\" import platform import tempfile import zipfile import pkg_resources import yaml import joblib import json import venv import subprocess import logging import datetime import sys import os import re import shutil from pathlib import Path from importlib import metadata as importlib_metadata from typing import Union , Optional from simaticai.helpers import calc_sha from simaticai.helpers.pep508 import parse_requirements from simaticai.helpers.reporter import PipelineRunnerReportWriter , ReportWriterHandler from simaticai.packaging.constants import ( REQUIREMENTS_TXT , TELEMETRY_YAML , PYTHON_PACKAGES , PYTHON_PACKAGES_ZIP , MSG_NOT_FOUND ) from simaticai.testing.pipeline_validator import INVALID_PIPELINE_PACKAGE_MESSAGE from simaticai.testing.data_stream import DataStream RETURN_CODE_OK = 0b0 RETURN_CODE_DEPRECATED = 0b10001 # equals to 17 logging . basicConfig () _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) _runner_path = Path ( __file__ ) . parent type_map = { 'int' : 'Integer' , 'float' : 'Double' , 'bool' : 'Boolean' , 'str' : 'String' } def _relative_to ( path1 , path2 ): path1 = Path ( path1 ) . resolve () . absolute () path2 = Path ( path2 ) . resolve () . absolute () return path1 . relative_to ( path2 ) class LocalPipelineRunner : \"\"\" Simulates the execution of a pipeline in a local Python environment. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Restriction: only linear pipelines are supported where the pipeline input variables are only used by one component, each component uses only the outputs of the previous components, and the pipeline output only consists of variables from the last component. If the caller specifies no `path`, the working directory is temporary and is removed unless an error occurs. If the caller specifies a working directory with a `path` argument, the working directory is kept. This behavior can be overridden using boolean parameter `cleanup`. Currently, the pipeline runner supports both the current `process_input(data: dict)` entrypoint signature and the legacy `run(data: str)` signature. If both entrypoints are present, `process_input()` takes precedence. Please note however that `run()` is deprecated, and support for it will be removed in future versions of the pipeline runner. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset, a temporary directory is created. cleanup (bool): If set, the working directory is kept when True, and deleted when False. \\ If unset, a temporary working directory is removed, and an explicit working directory is kept. \\ When an error occurs in a component, the working directory is kept regardless of this value. \"\"\" def __init__ ( self , packageZip : os . PathLike , path : Optional [ os . PathLike ] = None , cleanup : Optional [ bool ] = None , loglevel = logging . INFO ): \"\"\" Creates a new component LocalPipelineRunner for the provided pipeline configuration package. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset a temporary directory will be created. cleanup (bool): If set, the working directory will be kept when True, and deleted when False. \\ If unset, a temporary working directory will be removed, and an explicit working directory will be kept. \\ When an error occurs in a component, the working directory will be kept regardless of this value. \"\"\" self . package_zip : Path = Path ( packageZip ) self . path = path self . components = {} self . parameters = {} self . cleanup = cleanup self . log_level = loglevel self . workdir : Path self . report_writer = PipelineRunnerReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) def __enter__ ( self ): self . report_writer . set_package_zip_path ( self . package_zip ) timestamp = re . sub ( r \"[-:]\" , \"\" , datetime . datetime . utcnow () . isoformat ( sep = \"_\" , timespec = \"seconds\" )) if self . path is not None : self . workdir = Path ( self . path ) self . workdir . mkdir ( parents = True , exist_ok = True ) self . cleanup = self . cleanup if self . cleanup is not None else False else : self . workdir = Path ( tempfile . mkdtemp ( prefix = f \"LocalPipelineRunner_ { timestamp } _\" )) self . cleanup = self . cleanup if self . cleanup is not None else True unzip_components = False with zipfile . ZipFile ( self . package_zip ) as zf : if 'runtime_config.yml' in zf . namelist (): self . workdir = self . workdir / self . package_zip . stem zf . extractall ( path = self . workdir ) unzip_components = True else : zf . extractall ( path = self . workdir ) self . workdir = self . workdir / zf . namelist ()[ 0 ] try : with open ( self . workdir / \"pipeline_config.yml\" ) as cf : config = yaml . load ( cf , Loader = yaml . FullLoader ) components = config [ \"dataFlowPipeline\" ] . get ( \"components\" , []) for component in components : component [ \"context\" ] = None component [ \"env_dir\" ] = self . workdir / component [ 'name' ] self . components [ component [ \"name\" ]] = component if unzip_components : for component in components : component_zip = f \" { component [ 'name' ] } _ { component [ 'version' ] } .zip\" with zipfile . ZipFile ( self . workdir / 'components' / component_zip ) as zf : zf . extractall ( path = self . workdir / component [ \"name\" ]) for parameter in config [ \"dataFlowPipeline\" ] . get ( \"pipelineParameters\" , {}): self . parameters [ parameter [ \"name\" ]] = parameter except Exception : raise RuntimeError ( INVALID_PIPELINE_PACKAGE_MESSAGE ) return self def __exit__ ( self , exception_type , value , traceback ): self . update_telemetry_data () self . report_writer . write_report () if self . cleanup : _logger . info ( \"Removing local pipeline runner environment...\" ) shutil . rmtree ( self . workdir . parent ) else : _logger . info ( f \"Leaving local pipeline runner environment in its final state at ' { self . workdir } '\" ) def collect_telemetry_data ( self ): \"\"\" Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: dict: A dictionary containing the telemetry data. \"\"\" telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: { locals () } \" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ][ 'runtime' ][ 'version' ] for component in self . components if self . components [ component ][ 'runtime' ][ 'type' ] == 'python' )) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = [] for component_dir in [ Path ( self . workdir ) / c for c in Path ( self . workdir ) . rglob ( \"*\" ) if c . name in self . components . keys ()]: excluded_dirs = set ([ component_dir / '.venv' , component_dir / '__pyache__' ]) suffixes = list ( set ( f . suffix for f in component_dir . rglob ( \"*\" ) if not ( any ( excluded_dirs . intersection ( f . parents )) or f . suffix in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ]))) for suffix in suffixes : if suffix not in telemetry_data [ \"pipeline\" ][ \"file_extensions\" ]: telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] . append ( suffix ) return telemetry_data def update_telemetry_data ( self ): \"\"\" Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: None \"\"\" _logger . info ( \"Updating telemetry data and the package\" ) telemetry_path = self . workdir / \"telemetry_data.yml\" if telemetry_path . is_file (): telemetry_data = yaml . safe_load ( telemetry_path . read_text ()) else : telemetry_data = self . collect_telemetry_data () telemetry_data [ \"pipeline\" ][ \"last_run\" ] = datetime . datetime . now () . isoformat () telemetry_path . write_text ( yaml . dump ( telemetry_data )) config_package = False with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : for file in zip_read . namelist (): if TELEMETRY_YAML in file and file != TELEMETRY_YAML : config_package = True break new_zip_path = Path ( self . package_zip ) . parent / f \" { Path ( self . package_zip ) . stem } _tested.zip\" with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : with zipfile . ZipFile ( new_zip_path , 'w' ) as zip_write : for file in zip_read . namelist (): if TELEMETRY_YAML not in file : filepath = zip_read . extract ( file , path = self . workdir ) zip_write . write ( filepath , arcname = file ) else : zip_write . write ( telemetry_path , arcname = file ) if config_package : shutil . copy ( new_zip_path , self . package_zip ) else : new_sha_path = Path ( self . package_zip ) . parent / f \" { Path ( self . package_zip ) . stem } _tested.sha256\" new_sha_path . write_text ( calc_sha ( new_zip_path )) def _install_requirements ( self , component , package_dir , no_index : bool = True ): _logger . info ( \"Installing requirements...\" ) pip_report_file = component [ \"env_dir\" ] / \"pip_report.json\" executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) package_dir_path = _relative_to ( package_dir , component [ \"env_dir\" ]) pip_report_file_path = _relative_to ( pip_report_file , component [ \"env_dir\" ]) cmd = [ f \" { executable_path } \" , \"-m\" , \"pip\" , \"install\" , \"--no-warn-script-location\" , \"-f\" , f \" { package_dir_path } \" , \"-r\" , REQUIREMENTS_TXT , \"--report\" , f \" { pip_report_file_path } \" ] if no_index : cmd += [ \"--no-index\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if 0 == result . returncode and pip_report_file . is_file (): self . report_writer . add_installed_packages ( component [ \"name\" ], pip_report_file ) return result def _install_from_packages_zip ( self , component , package_dir ): result = self . _install_requirements ( component , package_dir , True ) return 0 == result . returncode def _install_from_pypi_org ( self , component , package_dir ): return self . _install_requirements ( component , package_dir , False ) def _init_component_venv ( self , component : dict ): \"\"\" Creates a virtual environment in which the given component can run. Args: component (str): name of the selected component. \"\"\" _logger . info ( f \"Creating virtual environment for component ' { component [ 'name' ] } '...\" ) context_dir = component [ \"env_dir\" ] / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( str ( context_dir )) component [ \"context\" ] = builder . ensure_directories ( context_dir ) component [ \"bin_path\" ] = Path ( component [ \"context\" ] . bin_path ) . absolute () _logger . debug ( f \"Component bin path: { component [ 'bin_path' ] } \" ) _logger . info ( \"Upgrading pip...\" ) executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \" { executable_path } \" , \"-m\" , \"pip\" , \"install\" , \"pip\" , \"--upgrade\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if result . returncode != 0 : self . cleanup = False _logger . warning ( f \"Error upgrading pip: \\n { result . stderr } \" ) try : result = self . _install_logmodule ( component [ \"bin_path\" ], component [ \"env_dir\" ]) except Exception as err : _logger . error ( err ) self . cleanup = False raise RuntimeError ( \"The 'simaticai' Python package is either not installed or does not contain package 'log_module'.\" ) from None if result . returncode != 0 : self . cleanup = False raise RuntimeError ( f \"Error installing log_module: \\n { result . stderr } \" ) req_list = Path ( component [ \"env_dir\" ] / \"requirements.list\" ) req_list . touch ( exist_ok = True ) if Path ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) . is_file (): dependencies , extra_index , index_url = parse_requirements ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) requirements = \"#\" . join ( dependencies . keys ()) req_list . write_text ( requirements ) else : _logger . info ( f \"' { REQUIREMENTS_TXT } ' was not found. No additional dependencies were installed.\" ) return package_dir = component [ \"env_dir\" ] / PYTHON_PACKAGES package_zip = component [ \"env_dir\" ] / PYTHON_PACKAGES_ZIP _logger . info ( f \"Extracting { PYTHON_PACKAGES_ZIP } \" ) if package_zip . is_file (): with zipfile . ZipFile ( package_zip ) as zf : zf . extractall ( path = package_dir . absolute ()) else : _logger . info ( f \"There is no { PYTHON_PACKAGES_ZIP } to extract.\" ) package_dir . mkdir ( parents = True , exist_ok = True ) success = self . _install_from_packages_zip ( component , package_dir ) if not success : msg = f \"Warning! Could not install dependencies from { PYTHON_PACKAGES_ZIP } . \" msg += \"Trying to install them from pypi.org. The resulting Python environment \" msg += \"may be significantly different than the targeted Python environment on the Edge Device!\" _logger . warning ( msg ) second_install_result = self . _install_from_pypi_org ( component , package_dir ) if 0 != second_install_result . returncode : self . cleanup = False raise RuntimeError ( f \"Error installing requirements: \\n { second_install_result . stderr } \" ) @staticmethod def _install_logmodule ( bin_path , env_dir ): _logger . info ( \"Installing LogModule...\" ) try : package_paths = importlib_metadata . files ( \"simaticai\" ) assert package_paths is not None logger_wheel = [ p for p in package_paths if 'log_module' in str ( p )][ 0 ] . locate () except Exception : from importlib.metadata import Distribution direct_url = Distribution . from_name ( \"simaticai\" ) . read_text ( \"direct_url.json\" ) assert direct_url is not None direct_url = json . loads ( direct_url )[ 'url' ] direct_url = direct_url . replace ( 'file://' , '' ) direct_url = Path ( direct_url ) / 'simaticai' / 'data' paths = list ( direct_url . rglob ( '*.whl' )) logger_wheel = [ p for p in paths if 'log_module' in str ( p )][ 0 ] . resolve () cmd = [ f \" { bin_path / 'pip' } \" , \"install\" , \"--no-warn-script-location\" , logger_wheel , \"joblib\" ] return subprocess . run ( cmd , cwd = env_dir , text = True , stderr = subprocess . PIPE ) def run_component ( self , name : str , data : Optional [ Union [ dict , list , DataStream ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Runs the component in its virtual environment with the given input. This environment is created according to `requirements.txt` in the package. Additionally 'joblib' and the mock `log_module` is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as `inputs.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: name (str): The name of the component to be executed. data (dict or list): One or more input records for the component. Returns: dict / list: A list of dictionaries for outputs if there were no errors and field `ready` is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. \"\"\" assert name in self . components , f \"Invalid component name: { name } \" component = self . components [ name ] assert component [ \"runtime\" ][ \"type\" ] in [ \"python\" , \"gpuruntime\" ], f \"Can not run component ' { name } ': Runtime type is nor 'python' or 'gpuruntime'\" input_payload_path : Path = component [ \"env_dir\" ] / \"input.joblib\" output_payload_path : Path = component [ \"env_dir\" ] / \"output.joblib\" batch_input : bool = component [ \"batch\" ][ \"inputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False batch_output : bool = component [ \"batch\" ][ \"outputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False # Validate and serialize input payload assert data is not None , f \"Can not run component ' { name } ' without input.\" result_is_list = True if isinstance ( data , list ): input_payload = data elif isinstance ( data , DataStream ): input_payload = [ item for item in data ] else : result_is_list = False input_payload = [ data ] validate_payload ( input_payload , component [ \"inputType\" ], batch_input ) joblib . dump ( input_payload , input_payload_path ) self . report_writer . set_input_payload_length ( name , len ( input_payload )) _logger . info ( f \"Input payload saved as ' { input_payload_path } '\" ) # Assemble command for runnig component if component [ 'runtime' ][ 'type' ] == 'python' : # Version check for Python e_major , e_minor , _ , _ , _ = sys . version_info c_major , c_minor , * _ = tuple ( str ( component [ \"runtime\" ][ \"version\" ]) . split ( '.' )) if f \" { e_major } . { e_minor } \" != f \" { c_major } . { c_minor } \" : msg = f \"The local python version ( { e_major } . { e_minor } ) and the python version defined for the component ( { c_major } . { c_minor } ) are different.\" msg += \" Testing will be done using dependencies that corresponds to the python version of your development environment.\" msg += \" Pipeline behavior on AI Inference Server might be different.\" _logger . warning ( msg ) if component [ \"context\" ] is None : self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_component.py' , component [ \"env_dir\" ]) req_list_path = _relative_to ( component [ \"env_dir\" ] / \"requirements.list\" , component [ \"env_dir\" ]) json_params = json . dumps ({ param [ \"name\" ]: param [ \"defaultValue\" ] for param in self . parameters . values ()}) args = [ \"-m\" , 'run_component' , \"-m\" , Path ( component [ \"entrypoint\" ]) . stem , \"-p\" , f \" { json_params } \" , \"-r\" , f \" { req_list_path } \" , ] else : # gpuruntime step requires Python environment with onnxruntime installed # TODO: check gpuruntime version if needed if component [ \"context\" ] is None : shutil . copy ( _runner_path / 'gpuruntime_requirements.txt' , component [ \"env_dir\" ] / REQUIREMENTS_TXT ) self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_gpuruntime_component.py' , component [ \"env_dir\" ]) shutil . copy ( _runner_path . parent / 'model_config_pb2.py' , component [ \"env_dir\" ]) args = [ \"-m\" , 'run_gpuruntime_component' , \"-m\" , component [ \"entrypoint\" ], \"-c\" , \"config.pbtxt\" , ] args += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . log_level ) ] # Run the component in the created Python environment executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \" { executable_path } \" ] + args _logger . info ( f \"Running command: { subprocess . list2cmdline ( cmd ) } \" ) p = subprocess . Popen ( cmd , cwd = component [ \"env_dir\" ], stdout = subprocess . PIPE , stderr = subprocess . STDOUT , text = True ) if p . stdout is not None : for line in p . stdout : _logger . info ( line . strip ()) returncode = p . wait () if returncode not in [ RETURN_CODE_OK , RETURN_CODE_DEPRECATED ]: self . cleanup = False raise RuntimeError ( f \"\"\" \\ There was an error while running component ' { name } '. You can check the test results in directory ' { component [ 'env_dir' ] } ' \"\"\" ) else : response_wrapped = True if returncode == RETURN_CODE_DEPRECATED else False # Deserialize and validate output payload _logger . info ( f \"Loading output payload from ' { output_payload_path } '\" ) output_payload = joblib . load ( output_payload_path ) output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] self . report_writer . set_output_payload_length ( name , len ( output_payload )) if response_wrapped : output_payload = [ json . loads ( item [ 'output' ]) for item in output_payload if item [ 'ready' ]] else : output_payload = [ output for output in output_payload if output is not None ] validate_payload ( output_payload , component [ \"outputType\" ], batch_output ) # Only return the first item if the input was a single item, or None if there are no valid results. if result_is_list : return output_payload return output_payload [ 0 ] if len ( output_payload ) > 0 else None def update_parameters ( self , parameters : dict ): \"\"\" Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Args: parameters (dict): names and values of parameters to update Raises: AssertionError: When: - either `name` is not in the configured parameters, - or `defaultValue` type is different from the configured one \"\"\" for key , value in parameters . items (): if key not in self . parameters . keys () or self . parameters [ key ][ \"type\" ] != type_map . get ( type ( value ) . __name__ ): raise AssertionError ( f \"Pipeline has no parameters with the name ' { key } ' and type ' { type_map . get ( type ( value ) . __name__ ) } '\" ) for key , value in parameters . items (): self . parameters [ key ][ \"defaultValue\" ] = value def run_pipeline ( self , payload : Optional [ Union [ dict , list , DataStream ]] = {}) -> Optional [ Union [ dict , list ]]: \"\"\" Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as `input.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: payload (dict or list): One or more input records for the pipeline. Returns: The output of the last component. \"\"\" for name in self . components . keys (): payload = self . run_component ( name , payload ) return payload def validate_payload ( data : list , variables : list , batch : bool , logger = _logger ): \"\"\" Validates that data is a valid list of input or output payload items. Variables list what variables each playload item has. Batch indicates if the payload items are themselves batches of items or not. \"\"\" assert isinstance ( data , list ), \"payload data must be a 'list'\" for i in data : if batch : assert isinstance ( i , list ), \"batch payload items must be 'list' instances\" else : i = [ i ] for j in i : validate_payload_item ( j , variables , logger ) def validate_payload_item ( data : dict , variables : list , logger ): \"\"\" Validates that data is a valid payload item. Variables listed must have a corresponding field in data. The types of the values must match their declared type. \"\"\" assert isinstance ( data , dict ), \"payload items must be 'dict' isntances\" for variable in variables : name = variable [ \"name\" ] value = data . get ( name , None ) if value is None : logger . warning ( f \"WARNING! Variable ' { name } ' is missing from input, output or metric\" ) continue if variable [ \"type\" ] == \"String\" : assert isinstance ( value , str ), \"'String' value must be an 'str'\" if variable [ \"type\" ] == \"StringArray\" : assert isinstance ( value , list ), \"'StringArray' value must be a 'list'\" assert all ( isinstance ( i , str ) for i in value ), \"'StringArray' items must be 'str' isntances\" if variable [ \"type\" ] == \"Object\" : assert isinstance ( value , dict ), \"'Object' value must be a 'dict'\" values = list ( value . values ()) assert len ( values ) == 2 , \"'Object' value must have exactly 2 items\" ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) assert ok , \"'Object' value must have exactly one 'str' and one 'bytes' field\" payload_names = set ( data . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: { extra_variables } \" )","title":"Module simaticai.testing.pipeline_runner"},{"location":"reference/simaticai/testing/pipeline_runner.html#variables","text":"INVALID_PIPELINE_PACKAGE_MESSAGE MSG_NOT_FOUND PYTHON_PACKAGES PYTHON_PACKAGES_ZIP REQUIREMENTS_TXT RETURN_CODE_DEPRECATED RETURN_CODE_OK TELEMETRY_YAML type_map","title":"Variables"},{"location":"reference/simaticai/testing/pipeline_runner.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/testing/pipeline_runner.html#validate_payload","text":"def validate_payload ( data : list , variables : list , batch : bool , logger =< Logger simaticai . testing . pipeline_runner ( INFO ) > ) Validates that data is a valid list of input or output payload items. Variables list what variables each playload item has. Batch indicates if the payload items are themselves batches of items or not. View Source def validate_payload ( data : list , variables : list , batch : bool , logger = _logger ): \"\"\" Validates that data is a valid list of input or output payload items. Variables list what variables each playload item has. Batch indicates if the payload items are themselves batches of items or not. \"\"\" assert isinstance ( data , list ), \"payload data must be a 'list'\" for i in data : if batch : assert isinstance ( i , list ), \"batch payload items must be 'list' instances\" else : i = [ i ] for j in i : validate_payload_item ( j , variables , logger )","title":"validate_payload"},{"location":"reference/simaticai/testing/pipeline_runner.html#validate_payload_item","text":"def validate_payload_item ( data : dict , variables : list , logger ) Validates that data is a valid payload item. Variables listed must have a corresponding field in data. The types of the values must match their declared type. View Source def validate_payload_item ( data : dict , variables : list , logger ): \"\"\" Validates that data is a valid payload item. Variables listed must have a corresponding field in data. The types of the values must match their declared type. \"\"\" assert isinstance ( data , dict ), \"payload items must be 'dict' isntances\" for variable in variables : name = variable [ \"name\" ] value = data . get ( name , None ) if value is None : logger . warning ( f \"WARNING! Variable '{name}' is missing from input, output or metric\" ) continue if variable [ \"type\" ] == \"String\" : assert isinstance ( value , str ), \"'String' value must be an 'str'\" if variable [ \"type\" ] == \"StringArray\" : assert isinstance ( value , list ), \"'StringArray' value must be a 'list'\" assert all ( isinstance ( i , str ) for i in value ), \"'StringArray' items must be 'str' isntances\" if variable [ \"type\" ] == \"Object\" : assert isinstance ( value , dict ), \"'Object' value must be a 'dict'\" values = list ( value . values ()) assert len ( values ) == 2 , \"'Object' value must have exactly 2 items\" ok = isinstance ( values [ 0 ], str ) and isinstance ( values [ 1 ], bytes ) or isinstance ( values [ 1 ], str ) and isinstance ( values [ 0 ], bytes ) assert ok , \"'Object' value must have exactly one 'str' and one 'bytes' field\" payload_names = set ( data . keys ()) variable_names = { variable [ \"name\" ] for variable in variables } variable_names . add ( 'timestamp' ) extra_variables = payload_names - variable_names if len ( extra_variables ): logger . warning ( f \"WARNING! These variables are not declared but are part of the payload: {extra_variables}\" )","title":"validate_payload_item"},{"location":"reference/simaticai/testing/pipeline_runner.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/testing/pipeline_runner.html#localpipelinerunner","text":"Simulates the execution of a pipeline in a local Python environment. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Restriction: only linear pipelines are supported where the pipeline input variables are only used by one component, each component uses only the outputs of the previous components, and the pipeline output only consists of variables from the last component. If the caller specifies no path , the working directory is temporary and is removed unless an error occurs. If the caller specifies a working directory with a path argument, the working directory is kept. This behavior can be overridden using boolean parameter cleanup . Currently, the pipeline runner supports both the current process_input(data: dict) entrypoint signature and the legacy run(data: str) signature. If both entrypoints are present, process_input() takes precedence. Please note however that run() is deprecated, and support for it will be removed in future versions of the pipeline runner. class LocalPipelineRunner ( packageZip : os . PathLike , path : Optional [ os . PathLike ] = None , cleanup : Optional [ bool ] = None , loglevel = 20 )","title":"LocalPipelineRunner"},{"location":"reference/simaticai/testing/pipeline_runner.html#attributes","text":"Name Type Description Default packageZip path-like Path to the pipeline configuration package. None path path-like Path to the working directory. If unset, a temporary directory is created. None cleanup bool If set, the working directory is kept when True, and deleted when False. If unset, a temporary working directory is removed, and an explicit working directory is kept. When an error occurs in a component, the working directory is kept regardless of this value. None View Source class LocalPipelineRunner : \"\"\" Simulates the execution of a pipeline in a local Python environment. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Restriction: only linear pipelines are supported where the pipeline input variables are only used by one component, each component uses only the outputs of the previous components, and the pipeline output only consists of variables from the last component. If the caller specifies no `path`, the working directory is temporary and is removed unless an error occurs. If the caller specifies a working directory with a `path` argument, the working directory is kept. This behavior can be overridden using boolean parameter `cleanup`. Currently, the pipeline runner supports both the current `process_input(data: dict)` entrypoint signature and the legacy `run(data: str)` signature. If both entrypoints are present, `process_input()` takes precedence. Please note however that `run()` is deprecated, and support for it will be removed in future versions of the pipeline runner. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset, a temporary directory is created. cleanup (bool): If set, the working directory is kept when True, and deleted when False. \\ If unset, a temporary working directory is removed, and an explicit working directory is kept. \\ When an error occurs in a component, the working directory is kept regardless of this value. \"\"\" def __init__ ( self , packageZip : os . PathLike , path : Optional [ os . PathLike ] = None , cleanup : Optional [ bool ] = None , loglevel = logging . INFO ): \"\"\" Creates a new component LocalPipelineRunner for the provided pipeline configuration package. Only works with a pipeline configuration package. Does not work with e.g. an edge configuration package. Args: packageZip (path-like): Path to the pipeline configuration package. path (path-like): Path to the working directory. If unset a temporary directory will be created. cleanup (bool): If set, the working directory will be kept when True, and deleted when False. \\ If unset, a temporary working directory will be removed, and an explicit working directory will be kept. \\ When an error occurs in a component, the working directory will be kept regardless of this value. \"\"\" self . package_zip : Path = Path ( packageZip ) self . path = path self . components = {} self . parameters = {} self . cleanup = cleanup self . log_level = loglevel self . workdir : Path self . report_writer = PipelineRunnerReportWriter () report_writer_handler = ReportWriterHandler ( self . report_writer ) _logger . addHandler ( report_writer_handler ) def __enter__ ( self ): self . report_writer . set_package_zip_path ( self . package_zip ) timestamp = re . sub ( r \"[-:]\" , \"\" , datetime . datetime . utcnow () . isoformat ( sep = \"_\" , timespec = \"seconds\" )) if self . path is not None : self . workdir = Path ( self . path ) self . workdir . mkdir ( parents = True , exist_ok = True ) self . cleanup = self . cleanup if self . cleanup is not None else False else : self . workdir = Path ( tempfile . mkdtemp ( prefix = f \"LocalPipelineRunner_{timestamp}_\" )) self . cleanup = self . cleanup if self . cleanup is not None else True unzip_components = False with zipfile . ZipFile ( self . package_zip ) as zf : if 'runtime_config.yml' in zf . namelist (): self . workdir = self . workdir / self . package_zip . stem zf . extractall ( path = self . workdir ) unzip_components = True else : zf . extractall ( path = self . workdir ) self . workdir = self . workdir / zf . namelist ()[ 0 ] try : with open ( self . workdir / \"pipeline_config.yml\" ) as cf : config = yaml . load ( cf , Loader = yaml . FullLoader ) components = config [ \"dataFlowPipeline\" ] . get ( \"components\" , []) for component in components : component [ \"context\" ] = None component [ \"env_dir\" ] = self . workdir / component [ 'name' ] self . components [ component [ \"name\" ]] = component if unzip_components : for component in components : component_zip = f \"{component['name']}_{component['version']}.zip\" with zipfile . ZipFile ( self . workdir / 'components' / component_zip ) as zf : zf . extractall ( path = self . workdir / component [ \"name\" ]) for parameter in config [ \"dataFlowPipeline\" ] . get ( \"pipelineParameters\" , {}): self . parameters [ parameter [ \"name\" ]] = parameter except Exception : raise RuntimeError ( INVALID_PIPELINE_PACKAGE_MESSAGE ) return self def __exit__ ( self , exception_type , value , traceback ): self . update_telemetry_data () self . report_writer . write_report () if self . cleanup : _logger . info ( \"Removing local pipeline runner environment...\" ) shutil . rmtree ( self . workdir . parent ) else : _logger . info ( f \"Leaving local pipeline runner environment in its final state at '{self.workdir}'\" ) def collect_telemetry_data ( self ): \"\"\" Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: dict: A dictionary containing the telemetry data. \"\"\" telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\" , \"get_ipython\" ]) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ) . version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ][ 'runtime' ][ 'version' ] for component in self . components if self . components [ component ][ 'runtime' ][ 'type' ] == 'python' )) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = [] for component_dir in [ Path ( self . workdir ) / c for c in Path ( self . workdir ) . rglob ( \"*\" ) if c . name in self . components . keys ()]: excluded_dirs = set ([ component_dir / '.venv' , component_dir / '__pyache__' ]) suffixes = list ( set ( f . suffix for f in component_dir . rglob ( \"*\" ) if not ( any ( excluded_dirs . intersection ( f . parents )) or f . suffix in [ \"\" , \".zip\" , \".yml\" , \".yaml\" , \".html\" ]))) for suffix in suffixes : if suffix not in telemetry_data [ \"pipeline\" ][ \"file_extensions\" ]: telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] . append ( suffix ) return telemetry_data def update_telemetry_data ( self ): \"\"\" Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: None \"\"\" _logger . info ( \"Updating telemetry data and the package\" ) telemetry_path = self . workdir / \"telemetry_data.yml\" if telemetry_path . is_file (): telemetry_data = yaml . safe_load ( telemetry_path . read_text ()) else : telemetry_data = self . collect_telemetry_data () telemetry_data [ \"pipeline\" ][ \"last_run\" ] = datetime . datetime . now () . isoformat () telemetry_path . write_text ( yaml . dump ( telemetry_data )) config_package = False with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : for file in zip_read . namelist (): if TELEMETRY_YAML in file and file != TELEMETRY_YAML : config_package = True break new_zip_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.zip\" with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : with zipfile . ZipFile ( new_zip_path , 'w' ) as zip_write : for file in zip_read . namelist (): if TELEMETRY_YAML not in file : filepath = zip_read . extract ( file , path = self . workdir ) zip_write . write ( filepath , arcname = file ) else : zip_write . write ( telemetry_path , arcname = file ) if config_package : shutil . copy ( new_zip_path , self . package_zip ) else : new_sha_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.sha256\" new_sha_path . write_text ( calc_sha ( new_zip_path )) def _install_requirements ( self , component , package_dir , no_index : bool = True ): _logger . info ( \"Installing requirements...\" ) pip_report_file = component [ \"env_dir\" ] / \"pip_report.json\" executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) package_dir_path = _relative_to ( package_dir , component [ \"env_dir\" ]) pip_report_file_path = _relative_to ( pip_report_file , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" , \"-m\" , \"pip\" , \"install\" , \"--no-warn-script-location\" , \"-f\" , f \"{package_dir_path}\" , \"-r\" , REQUIREMENTS_TXT , \"--report\" , f \"{pip_report_file_path}\" ] if no_index : cmd += [ \"--no-index\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if 0 == result . returncode and pip_report_file . is_file (): self . report_writer . add_installed_packages ( component [ \"name\" ], pip_report_file ) return result def _install_from_packages_zip ( self , component , package_dir ): result = self . _install_requirements ( component , package_dir , True ) return 0 == result . returncode def _install_from_pypi_org ( self , component , package_dir ): return self . _install_requirements ( component , package_dir , False ) def _init_component_venv ( self , component : dict ): \"\"\" Creates a virtual environment in which the given component can run. Args: component (str): name of the selected component. \"\"\" _logger . info ( f \"Creating virtual environment for component '{component['name']}'...\" ) context_dir = component [ \"env_dir\" ] / \".venv\" builder = venv . EnvBuilder ( with_pip = True ) builder . create ( str ( context_dir )) component [ \"context\" ] = builder . ensure_directories ( context_dir ) component [ \"bin_path\" ] = Path ( component [ \"context\" ] . bin_path ) . absolute () _logger . debug ( f \"Component bin path: {component['bin_path']}\" ) _logger . info ( \"Upgrading pip...\" ) executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" , \"-m\" , \"pip\" , \"install\" , \"pip\" , \"--upgrade\" ] result = subprocess . run ( cmd , cwd = component [ \"env_dir\" ], text = True , stderr = subprocess . PIPE ) if result . returncode != 0 : self . cleanup = False _logger . warning ( f \"Error upgrading pip: \\n {result.stderr}\" ) try : result = self . _install_logmodule ( component [ \"bin_path\" ], component [ \"env_dir\" ]) except Exception as err : _logger . error ( err ) self . cleanup = False raise RuntimeError ( \"The 'simaticai' Python package is either not installed or does not contain package 'log_module'.\" ) from None if result . returncode != 0 : self . cleanup = False raise RuntimeError ( f \"Error installing log_module: \\n {result.stderr}\" ) req_list = Path ( component [ \"env_dir\" ] / \"requirements.list\" ) req_list . touch ( exist_ok = True ) if Path ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) . is_file (): dependencies , extra_index , index_url = parse_requirements ( component [ \"env_dir\" ] / REQUIREMENTS_TXT ) requirements = \"#\" . join ( dependencies . keys ()) req_list . write_text ( requirements ) else : _logger . info ( f \"'{REQUIREMENTS_TXT}' was not found. No additional dependencies were installed.\" ) return package_dir = component [ \"env_dir\" ] / PYTHON_PACKAGES package_zip = component [ \"env_dir\" ] / PYTHON_PACKAGES_ZIP _logger . info ( f \"Extracting {PYTHON_PACKAGES_ZIP}\" ) if package_zip . is_file (): with zipfile . ZipFile ( package_zip ) as zf : zf . extractall ( path = package_dir . absolute ()) else : _logger . info ( f \"There is no {PYTHON_PACKAGES_ZIP} to extract.\" ) package_dir . mkdir ( parents = True , exist_ok = True ) success = self . _install_from_packages_zip ( component , package_dir ) if not success : msg = f \"Warning! Could not install dependencies from {PYTHON_PACKAGES_ZIP}. \" msg += \"Trying to install them from pypi.org. The resulting Python environment \" msg += \"may be significantly different than the targeted Python environment on the Edge Device!\" _logger . warning ( msg ) second_install_result = self . _install_from_pypi_org ( component , package_dir ) if 0 != second_install_result . returncode : self . cleanup = False raise RuntimeError ( f \"Error installing requirements: \\n {second_install_result.stderr}\" ) @ staticmethod def _install_logmodule ( bin_path , env_dir ): _logger . info ( \"Installing LogModule...\" ) try : package_paths = importlib_metadata . files ( \"simaticai\" ) assert package_paths is not None logger_wheel = [ p for p in package_paths if 'log_module' in str ( p )][ 0 ] . locate () except Exception : from importlib . metadata import Distribution direct_url = Distribution . from_name ( \"simaticai\" ) . read_text ( \"direct_url.json\" ) assert direct_url is not None direct_url = json . loads ( direct_url )[ 'url' ] direct_url = direct_url . replace ( 'file://' , '' ) direct_url = Path ( direct_url ) / 'simaticai' / 'data' paths = list ( direct_url . rglob ( '*.whl' )) logger_wheel = [ p for p in paths if 'log_module' in str ( p )][ 0 ] . resolve () cmd = [ f \"{bin_path / 'pip'}\" , \"install\" , \"--no-warn-script-location\" , logger_wheel , \"joblib\" ] return subprocess . run ( cmd , cwd = env_dir , text = True , stderr = subprocess . PIPE ) def run_component ( self , name : str , data : Optional [ Union [ dict , list , DataStream ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Runs the component in its virtual environment with the given input. This environment is created according to `requirements.txt` in the package. Additionally 'joblib' and the mock `log_module` is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as `inputs.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: name (str): The name of the component to be executed. data (dict or list): One or more input records for the component. Returns: dict / list: A list of dictionaries for outputs if there were no errors and field `ready` is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. \"\"\" assert name in self . components , f \"Invalid component name: {name}\" component = self . components [ name ] assert component [ \"runtime\" ][ \"type\" ] in [ \"python\" , \"gpuruntime\" ], f \"Can not run component '{name}': Runtime type is nor 'python' or 'gpuruntime'\" input_payload_path : Path = component [ \"env_dir\" ] / \"input.joblib\" output_payload_path : Path = component [ \"env_dir\" ] / \"output.joblib\" batch_input : bool = component [ \"batch\" ][ \"inputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False batch_output : bool = component [ \"batch\" ][ \"outputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False # Validate and serialize input payload assert data is not None , f \"Can not run component '{name}' without input.\" result_is_list = True if isinstance ( data , list ): input_payload = data elif isinstance ( data , DataStream ): input_payload = [ item for item in data ] else : result_is_list = False input_payload = [ data ] validate_payload ( input_payload , component [ \"inputType\" ], batch_input ) joblib . dump ( input_payload , input_payload_path ) self . report_writer . set_input_payload_length ( name , len ( input_payload )) _logger . info ( f \"Input payload saved as '{input_payload_path}'\" ) # Assemble command for runnig component if component [ 'runtime' ][ 'type' ] == 'python' : # Version check for Python e_major , e_minor , _ , _ , _ = sys . version_info c_major , c_minor , * _ = tuple ( str ( component [ \"runtime\" ][ \"version\" ]) . split ( '.' )) if f \"{e_major}.{e_minor}\" != f \"{c_major}.{c_minor}\" : msg = f \"The local python version ({e_major}.{e_minor}) and the python version defined for the component ({c_major}.{c_minor}) are different.\" msg += \" Testing will be done using dependencies that corresponds to the python version of your development environment.\" msg += \" Pipeline behavior on AI Inference Server might be different.\" _logger . warning ( msg ) if component [ \"context\" ] is None : self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_component.py' , component [ \"env_dir\" ]) req_list_path = _relative_to ( component [ \"env_dir\" ] / \"requirements.list\" , component [ \"env_dir\" ]) json_params = json . dumps ({ param [ \"name\" ]: param [ \"defaultValue\" ] for param in self . parameters . values ()}) args = [ \"-m\" , 'run_component' , \"-m\" , Path ( component [ \"entrypoint\" ]) . stem , \"-p\" , f \"{json_params}\" , \"-r\" , f \"{req_list_path}\" , ] else : # gpuruntime step requires Python environment with onnxruntime installed # TODO: check gpuruntime version if needed if component [ \"context\" ] is None : shutil . copy ( _runner_path / 'gpuruntime_requirements.txt' , component [ \"env_dir\" ] / REQUIREMENTS_TXT ) self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_gpuruntime_component.py' , component [ \"env_dir\" ]) shutil . copy ( _runner_path . parent / 'model_config_pb2.py' , component [ \"env_dir\" ]) args = [ \"-m\" , 'run_gpuruntime_component' , \"-m\" , component [ \"entrypoint\" ], \"-c\" , \"config.pbtxt\" , ] args += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . log_level ) ] # Run the component in the created Python environment executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" ] + args _logger . info ( f \"Running command: {subprocess.list2cmdline(cmd)}\" ) p = subprocess . Popen ( cmd , cwd = component [ \"env_dir\" ], stdout = subprocess . PIPE , stderr = subprocess . STDOUT , text = True ) if p . stdout is not None : for line in p . stdout : _logger . info ( line . strip ()) returncode = p . wait () if returncode not in [ RETURN_CODE_OK , RETURN_CODE_DEPRECATED ]: self . cleanup = False raise RuntimeError ( f \"\"\" \\ There was an error while running component '{name}'. You can check the test results in directory '{component['env_dir']}' \"\"\" ) else : response_wrapped = True if returncode == RETURN_CODE_DEPRECATED else False # Deserialize and validate output payload _logger . info ( f \"Loading output payload from '{output_payload_path}'\" ) output_payload = joblib . load ( output_payload_path ) output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] self . report_writer . set_output_payload_length ( name , len ( output_payload )) if response_wrapped : output_payload = [ json . loads ( item [ 'output' ]) for item in output_payload if item [ 'ready' ]] else : output_payload = [ output for output in output_payload if output is not None ] validate_payload ( output_payload , component [ \"outputType\" ], batch_output ) # Only return the first item if the input was a single item, or None if there are no valid results. if result_is_list : return output_payload return output_payload [ 0 ] if len ( output_payload ) > 0 else None def update_parameters ( self , parameters : dict ): \"\"\" Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Args: parameters (dict): names and values of parameters to update Raises: AssertionError: When: - either `name` is not in the configured parameters, - or `defaultValue` type is different from the configured one \"\"\" for key , value in parameters . items (): if key not in self . parameters . keys () or self . parameters [ key ][ \"type\" ] != type_map . get ( type ( value ) . __name__ ): raise AssertionError ( f \"Pipeline has no parameters with the name '{key}' and type '{type_map.get(type(value).__name__)}'\" ) for key , value in parameters . items (): self . parameters [ key ][ \"defaultValue\" ] = value def run_pipeline ( self , payload : Optional [ Union [ dict , list , DataStream ]] = {}) -> Optional [ Union [ dict , list ]]: \"\"\" Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as `input.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: payload (dict or list): One or more input records for the pipeline. Returns: The output of the last component. \"\"\" for name in self . components . keys (): payload = self . run_component ( name , payload ) return payload","title":"Attributes"},{"location":"reference/simaticai/testing/pipeline_runner.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/testing/pipeline_runner.html#collect_telemetry_data","text":"def collect_telemetry_data ( self ) Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: Type Description dict A dictionary containing the telemetry data. View Source def collect_telemetry_data ( self ) : \"\"\" Collects telemetry data about the platform, environment, industrial AI packages, and pipeline. Returns: dict: A dictionary containing the telemetry data. \"\"\" telemetry_data = {} telemetry_data [ \"platform\" ] = {} telemetry_data [ \"platform\" ][ \"os\" ] = platform . system () telemetry_data [ \"platform\" ][ \"release\" ] = platform . release () telemetry_data [ \"platform\" ][ \"python_version\" ] = platform . python_version () _logger . info ( f \"locals: {locals()}\" ) telemetry_data [ \"environment\" ] = {} telemetry_data [ \"environment\" ][ \"jupyter\" ] = any ( k for k in locals () if k in [ \"__IPYTHON__\", \"get_ipython\" ] ) telemetry_data [ \"environment\" ][ \"gitlab_ci\" ] = True if \"GITLAB_CI\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"azure_devops\" ] = True if \"TF_BUILD\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"environment\" ][ \"github_actions\" ] = True if \"GITHUB_ACTIONS\" in os . environ else MSG_NOT_FOUND telemetry_data [ \"industrial_ai\" ] = {} try : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = pkg_resources . get_distribution ( \"simaticai\" ). version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"simaticai\" ] = MSG_NOT_FOUND try : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = pkg_resources . get_distribution ( \"vep-template-sdk\" ). version except pkg_resources . DistributionNotFound : telemetry_data [ \"industrial_ai\" ][ \"vep-template-sdk\" ] = MSG_NOT_FOUND telemetry_data [ \"pipeline\" ] = {} telemetry_data [ \"pipeline\" ][ \"python_versions\" ] = list ( set ( self . components [ component ][ 'runtime' ][ 'version' ] for component in self . components if self . components [ component ][ 'runtime' ][ 'type' ] == 'python' )) telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] = [] for component_dir in [ Path(self.workdir) / c for c in Path(self.workdir).rglob(\"*\") if c.name in self.components.keys() ] : excluded_dirs = set ( [ component_dir / '.venv', component_dir / '__pyache__' ] ) suffixes = list ( set ( f . suffix for f in component_dir . rglob ( \"*\" ) if not ( any ( excluded_dirs . intersection ( f . parents )) or f . suffix in [ \"\", \".zip\", \".yml\", \".yaml\", \".html\" ] ))) for suffix in suffixes : if suffix not in telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] : telemetry_data [ \"pipeline\" ][ \"file_extensions\" ] . append ( suffix ) return telemetry_data","title":"collect_telemetry_data"},{"location":"reference/simaticai/testing/pipeline_runner.html#run_component","text":"def run_component ( self , name : str , data : Union [ dict , list , simaticai . testing . data_stream . DataStream , NoneType ] ) -> Union [ dict , list , NoneType ] Runs the component in its virtual environment with the given input. This environment is created according to requirements.txt in the package. Additionally 'joblib' and the mock log_module is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as inputs.joblib in the component runtime directory, and the output is saved as output.joblib . Parameters: Name Type Description Default name str The name of the component to be executed. None data dict or list One or more input records for the component. None Returns: Type Description None dict / list: A list of dictionaries for outputs if there were no errors and field ready is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. View Source def run_component ( self , name : str , data : Optional [ Union [ dict , list , DataStream ]]) -> Optional [ Union [ dict , list ]]: \"\"\" Runs the component in its virtual environment with the given input. This environment is created according to `requirements.txt` in the package. Additionally 'joblib' and the mock `log_module` is automatically installed in this virtual environment. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries, or a DataStream object which will produce the appropriate input data. The supplied input data is saved as `inputs.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: name (str): The name of the component to be executed. data (dict or list): One or more input records for the component. Returns: dict / list: A list of dictionaries for outputs if there were no errors and field `ready` is true. If the input was a single dict, then a single dict (the first item of the list) or None if there is no output. \"\"\" assert name in self . components , f \"Invalid component name: {name}\" component = self . components [ name ] assert component [ \"runtime\" ][ \"type\" ] in [ \"python\" , \"gpuruntime\" ], f \"Can not run component '{name}': Runtime type is nor 'python' or 'gpuruntime'\" input_payload_path : Path = component [ \"env_dir\" ] / \"input.joblib\" output_payload_path : Path = component [ \"env_dir\" ] / \"output.joblib\" batch_input : bool = component [ \"batch\" ][ \"inputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False batch_output : bool = component [ \"batch\" ][ \"outputBatch\" ] == \"Yes\" if component . get ( \"batch\" ) is not None else False # Validate and serialize input payload assert data is not None , f \"Can not run component '{name}' without input.\" result_is_list = True if isinstance ( data , list ): input_payload = data elif isinstance ( data , DataStream ): input_payload = [ item for item in data ] else : result_is_list = False input_payload = [ data ] validate_payload ( input_payload , component [ \"inputType\" ], batch_input ) joblib . dump ( input_payload , input_payload_path ) self . report_writer . set_input_payload_length ( name , len ( input_payload )) _logger . info ( f \"Input payload saved as '{input_payload_path}'\" ) # Assemble command for runnig component if component [ 'runtime' ][ 'type' ] == 'python' : # Version check for Python e_major , e_minor , _ , _ , _ = sys . version_info c_major , c_minor , * _ = tuple ( str ( component [ \"runtime\" ][ \"version\" ]) . split ( '.' )) if f \"{e_major}.{e_minor}\" != f \"{c_major}.{c_minor}\" : msg = f \"The local python version ({e_major}.{e_minor}) and the python version defined for the component ({c_major}.{c_minor}) are different.\" msg += \" Testing will be done using dependencies that corresponds to the python version of your development environment.\" msg += \" Pipeline behavior on AI Inference Server might be different.\" _logger . warning ( msg ) if component [ \"context\" ] is None : self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_component.py' , component [ \"env_dir\" ]) req_list_path = _relative_to ( component [ \"env_dir\" ] / \"requirements.list\" , component [ \"env_dir\" ]) json_params = json . dumps ({ param [ \"name\" ]: param [ \"defaultValue\" ] for param in self . parameters . values ()}) args = [ \"-m\" , 'run_component' , \"-m\" , Path ( component [ \"entrypoint\" ]) . stem , \"-p\" , f \"{json_params}\" , \"-r\" , f \"{req_list_path}\" , ] else : # gpuruntime step requires Python environment with onnxruntime installed # TODO: check gpuruntime version if needed if component [ \"context\" ] is None : shutil . copy ( _runner_path / 'gpuruntime_requirements.txt' , component [ \"env_dir\" ] / REQUIREMENTS_TXT ) self . _init_component_venv ( component ) shutil . copy ( _runner_path / 'run_gpuruntime_component.py' , component [ \"env_dir\" ]) shutil . copy ( _runner_path . parent / 'model_config_pb2.py' , component [ \"env_dir\" ]) args = [ \"-m\" , 'run_gpuruntime_component' , \"-m\" , component [ \"entrypoint\" ], \"-c\" , \"config.pbtxt\" , ] args += [ \"-i\" , input_payload_path . name , \"-o\" , output_payload_path . name , \"-ll\" , logging . getLevelName ( self . log_level ) ] # Run the component in the created Python environment executable_path = _relative_to ( component [ 'bin_path' ] / 'python' , component [ \"env_dir\" ]) cmd = [ f \"{executable_path}\" ] + args _logger . info ( f \"Running command: {subprocess.list2cmdline(cmd)}\" ) p = subprocess . Popen ( cmd , cwd = component [ \"env_dir\" ], stdout = subprocess . PIPE , stderr = subprocess . STDOUT , text = True ) if p . stdout is not None : for line in p . stdout : _logger . info ( line . strip ()) returncode = p . wait () if returncode not in [ RETURN_CODE_OK , RETURN_CODE_DEPRECATED ]: self . cleanup = False raise RuntimeError ( f \"\"\" \\ There was an error while running component '{name}'. You can check the test results in directory '{component['env_dir']}' \"\"\" ) else : response_wrapped = True if returncode == RETURN_CODE_DEPRECATED else False # Deserialize and validate output payload _logger . info ( f \"Loading output payload from '{output_payload_path}'\" ) output_payload = joblib . load ( output_payload_path ) output_payload = output_payload if isinstance ( output_payload , list ) else [ output_payload ] self . report_writer . set_output_payload_length ( name , len ( output_payload )) if response_wrapped : output_payload = [ json . loads ( item [ 'output' ]) for item in output_payload if item [ 'ready' ]] else : output_payload = [ output for output in output_payload if output is not None ] validate_payload ( output_payload , component [ \"outputType\" ], batch_output ) # Only return the first item if the input was a single item, or None if there are no valid results. if result_is_list : return output_payload return output_payload [ 0 ] if len ( output_payload ) > 0 else None","title":"run_component"},{"location":"reference/simaticai/testing/pipeline_runner.html#run_pipeline","text":"def run_pipeline ( self , payload : Union [ dict , list , simaticai . testing . data_stream . DataStream , NoneType ] = {} ) -> Union [ dict , list , NoneType ] Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as input.joblib in the component runtime directory, and the output is saved as output.joblib . Parameters: Name Type Description Default payload dict or list One or more input records for the pipeline. None Returns: Type Description None The output of the last component. View Source def run_pipeline ( self , payload : Optional [ Union [ dict , list , DataStream ]] = {}) -> Optional [ Union [ dict , list ]]: \"\"\" Runs all the components sequentially, assuming the output of a component is only consumed by the next. The input data can be a single input record in a dictionary or a batch of input records in a list of dictionaries. For each component the supplied input data is saved as `input.joblib` in the component runtime directory, and the output is saved as `output.joblib`. Args: payload (dict or list): One or more input records for the pipeline. Returns: The output of the last component. \"\"\" for name in self . components . keys (): payload = self . run_component ( name , payload ) return payload","title":"run_pipeline"},{"location":"reference/simaticai/testing/pipeline_runner.html#update_parameters","text":"def update_parameters ( self , parameters : dict ) Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Parameters: Name Type Description Default parameters dict names and values of parameters to update None Raises: Type Description AssertionError When: - either name is not in the configured parameters, - or defaultValue type is different from the configured one View Source def update_parameters ( self , parameters : dict ): \"\"\" Validates and updates pipeline parameters. The elements of the dictionary must match the parameters specified in the pipeline configuration package. If any of the names or types does not match, all parameters will remain untouched. Args: parameters (dict): names and values of parameters to update Raises: AssertionError: When: - either `name` is not in the configured parameters, - or `defaultValue` type is different from the configured one \"\"\" for key , value in parameters . items (): if key not in self . parameters . keys () or self . parameters [ key ][ \"type\" ] != type_map . get ( type ( value ). __name__ ): raise AssertionError ( f \"Pipeline has no parameters with the name '{key}' and type '{type_map.get(type(value).__name__)}'\" ) for key , value in parameters . items (): self . parameters [ key ][ \"defaultValue\" ] = value","title":"update_parameters"},{"location":"reference/simaticai/testing/pipeline_runner.html#update_telemetry_data","text":"def update_telemetry_data ( self ) Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: Type Description None None View Source def update_telemetry_data ( self ): \"\"\" Update the telemetry data and the package. This method updates the telemetry data by loading the existing data from a YAML file, or collecting new telemetry data if the file doesn't exist. It then updates the \"last_run\" field of the telemetry data with the current timestamp. The updated telemetry data is then written back to the YAML file. If the package contains a different version of the telemetry data file, a new package is created with the updated telemetry data. Otherwise, the existing package is overwritten with the new package containing the updated telemetry data. Returns: None \"\"\" _logger . info ( \"Updating telemetry data and the package\" ) telemetry_path = self . workdir / \"telemetry_data.yml\" if telemetry_path . is_file (): telemetry_data = yaml . safe_load ( telemetry_path . read_text ()) else : telemetry_data = self . collect_telemetry_data () telemetry_data [ \"pipeline\" ][ \"last_run\" ] = datetime . datetime . now () . isoformat () telemetry_path . write_text ( yaml . dump ( telemetry_data )) config_package = False with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : for file in zip_read . namelist (): if TELEMETRY_YAML in file and file != TELEMETRY_YAML : config_package = True break new_zip_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.zip\" with zipfile . ZipFile ( self . package_zip , 'r' ) as zip_read : with zipfile . ZipFile ( new_zip_path , 'w' ) as zip_write : for file in zip_read . namelist (): if TELEMETRY_YAML not in file : filepath = zip_read . extract ( file , path = self . workdir ) zip_write . write ( filepath , arcname = file ) else : zip_write . write ( telemetry_path , arcname = file ) if config_package : shutil . copy ( new_zip_path , self . package_zip ) else : new_sha_path = Path ( self . package_zip ) . parent / f \"{Path(self.package_zip).stem}_tested.sha256\" new_sha_path . write_text ( calc_sha ( new_zip_path ))","title":"update_telemetry_data"},{"location":"reference/simaticai/testing/pipeline_validator.html","text":"Module simaticai.testing.pipeline_validator Static validation of pipeline packages. Executes static checks on a pipeline configuration package including: Verifying that the Python version required in the package is supported by a known version of the AI Inference Server. Verifying that all the required Python packages are either included in the pipeline package itself or available on pypi.org for the target platform. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Static validation of pipeline packages. Executes static checks on a pipeline configuration package including: - Verifying that the Python version required in the package is supported by a known version of the AI Inference Server. - Verifying that all the required Python packages are either included in the pipeline package itself or available on `pypi.org` for the target platform. \"\"\" import subprocess from pathlib import Path import zipfile import logging from simaticai import deployment from simaticai.helpers import tempfiles , yaml_helper from simaticai.packaging.constants import REQUIREMENTS_TXT , PYTHON_PACKAGES_ZIP from simaticai.packaging.wheelhouse import create_wheelhouse logging . basicConfig () _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) INVALID_PIPELINE_PACKAGE_MESSAGE = \"Invalid pipeline configuration package. Perhaps you have passed an Edge configuration package instead?\" class PipelineValidationError ( Exception ): \"\"\" Represents a problem with the pipeline configuration. Args: description (str): Description of the error. Mandatory argument. \"\"\" def __init__ ( self , description : str ) -> None : assert description is not None self . value = description def __str__ ( self ): return self . value def validate_pipeline_dependencies ( zip_path ): \"\"\" @Deprecated, reason: In the future only the edge package will be generated and the same validation is performed during edge package creation. Validates an already built pipeline configuration package to check if it is compatible with the AI Inference Server. This method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. If the required dependency for the target platform is not available on pypi.org and not present in `PythonPackages.zip` it will log the problem at the ERROR level. Args: zip_path (path-like): Path to the pipeline configuration package zip file Raises: `pipeline_validator.PipelineValidationError` if the validation fails. See the logger output for details. \"\"\" with tempfiles . OpenZipInTemp ( zip_path ) as package_dir : package_dir = next ( package_dir . iterdir ()) error = read_config_and_download_deps ( package_dir ) if error : raise PipelineValidationError ( \"Requirements of one or more components can not be satisfied.\" ) _logger . info ( f \"Validating pipeline package ' { zip_path } ' was successful.\" ) def download_component_dependencies ( component : dict , package_dir : Path ): \"\"\" Download the dependencies of a pipeline component. Args: component (dict): A single component from the parsed `pipeline_configuration.yml`. package_dir (Path): The directory where the component was extracted \"\"\" _logger . info ( f \"Validating requirements of component: { component [ 'name' ] } \" ) try : deployment . python_version_validator ( component [ 'runtime' ][ 'version' ]) except ValueError as error : raise PipelineValidationError ( error ) component_dir = package_dir / component [ 'name' ] requirements_file_path = component_dir / REQUIREMENTS_TXT python_packages_folder = _build_python_packages_folder ( component_dir ) return not _are_dependencies_available ( requirements_file_path , component [ 'runtime' ][ 'version' ], python_packages_folder ) def read_config_and_download_deps ( package_dir : Path ): \"\"\" Reads the pipeline configuration from the package directory and downloads its dependencies Args: package_dir (Path): The directory where the pipeline configuration package was extracted. Returns: boolean: True if there was an error during the download of the components, False otherwise \"\"\" try : config = yaml_helper . read_yaml ( package_dir / 'pipeline_config.yml' ) error = False for component in config [ 'dataFlowPipeline' ][ 'components' ]: error = download_component_dependencies ( component , package_dir ) or error return error except Exception : raise PipelineValidationError ( INVALID_PIPELINE_PACKAGE_MESSAGE ) def _are_dependencies_available ( requirements_file_path : Path , python_version : str , python_packages_folder : Path ): try : if Path ( requirements_file_path ) . is_file (): try : create_wheelhouse ( requirements_file_path , python_version , python_packages_folder ) except RuntimeError as error : _logger . error ( f \"Error occurred while creating wheelhouse \\n { error . __str__ () } \" ) return False except AssertionError as error : _logger . error ( f \"Could not find a package that satisfies the requirements \\n { error . __str__ () } \" ) return False return True else : _logger . info ( f \"' { REQUIREMENTS_TXT } ' was not found.\" ) except subprocess . TimeoutExpired : _logger . error ( \"TimeoutExpired occurred during download\" ) return False except Exception as e : _logger . error ( f \"Unexpected exception occurred while downloading packages \\n { e . __str__ () } \" ) return False return True def _build_python_packages_folder ( component_dir : Path ) -> Path : assert component_dir is not None python_packages_folder = component_dir / 'packages' packages_file = component_dir / PYTHON_PACKAGES_ZIP python_packages_folder . mkdir ( exist_ok = True ) if packages_file . is_file (): with zipfile . ZipFile ( packages_file ) as zip_file : zip_file . extractall ( python_packages_folder ) return python_packages_folder Variables INVALID_PIPELINE_PACKAGE_MESSAGE PYTHON_PACKAGES_ZIP REQUIREMENTS_TXT Functions download_component_dependencies def download_component_dependencies ( component : dict , package_dir : pathlib . Path ) Download the dependencies of a pipeline component. Parameters: Name Type Description Default component dict A single component from the parsed pipeline_configuration.yml . None package_dir Path The directory where the component was extracted None View Source def download_component_dependencies ( component : dict , package_dir : Path ): \"\"\" Download the dependencies of a pipeline component. Args: component (dict): A single component from the parsed `pipeline_configuration.yml`. package_dir (Path): The directory where the component was extracted \"\"\" _logger . info ( f \"Validating requirements of component: {component['name']}\" ) try : deployment . python_version_validator ( component [ 'runtime' ][ 'version' ]) except ValueError as error : raise PipelineValidationError ( error ) component_dir = package_dir / component [ 'name' ] requirements_file_path = component_dir / REQUIREMENTS_TXT python_packages_folder = _build_python_packages_folder ( component_dir ) return not _are_dependencies_available ( requirements_file_path , component [ 'runtime' ][ 'version' ], python_packages_folder ) read_config_and_download_deps def read_config_and_download_deps ( package_dir : pathlib . Path ) Reads the pipeline configuration from the package directory and downloads its dependencies Parameters: Name Type Description Default package_dir Path The directory where the pipeline configuration package was extracted. None Returns: Type Description boolean True if there was an error during the download of the components, False otherwise View Source def read_config_and_download_deps ( package_dir : Path ): \"\"\" Reads the pipeline configuration from the package directory and downloads its dependencies Args: package_dir (Path): The directory where the pipeline configuration package was extracted. Returns: boolean: True if there was an error during the download of the components, False otherwise \"\"\" try : config = yaml_helper . read_yaml ( package_dir / 'pipeline_config.yml' ) error = False for component in config [ 'dataFlowPipeline' ][ 'components' ]: error = download_component_dependencies ( component , package_dir ) or error return error except Exception : raise PipelineValidationError ( INVALID_PIPELINE_PACKAGE_MESSAGE ) validate_pipeline_dependencies def validate_pipeline_dependencies ( zip_path ) View Source def validate_pipeline_dependencies ( zip_path ): \"\"\" @Deprecated, reason: In the future only the edge package will be generated and the same validation is performed during edge package creation. Validates an already built pipeline configuration package to check if it is compatible with the AI Inference Server. This method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. If the required dependency for the target platform is not available on pypi.org and not present in `PythonPackages.zip` it will log the problem at the ERROR level. Args: zip_path (path-like): Path to the pipeline configuration package zip file Raises: `pipeline_validator.PipelineValidationError` if the validation fails. See the logger output for details. \"\"\" with tempfiles . OpenZipInTemp ( zip_path ) as package_dir : package_dir = next ( package_dir . iterdir ()) error = read_config_and_download_deps ( package_dir ) if error : raise PipelineValidationError ( \"Requirements of one or more components can not be satisfied.\" ) _logger . info ( f \"Validating pipeline package '{zip_path}' was successful.\" ) Classes PipelineValidationError Represents a problem with the pipeline configuration. class PipelineValidationError ( description : str ) Attributes Name Type Description Default description str Description of the error. Mandatory argument. None View Source class PipelineValidationError ( Exception ): \"\"\" Represents a problem with the pipeline configuration. Args: description (str): Description of the error. Mandatory argument. \"\"\" def __init__ ( self , description: str ) -> None: assert description is not None self . value = description def __str__ ( self ): return self . value Ancestors (in MRO) builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"Pipeline Validator"},{"location":"reference/simaticai/testing/pipeline_validator.html#module-simaticaitestingpipeline_validator","text":"Static validation of pipeline packages. Executes static checks on a pipeline configuration package including: Verifying that the Python version required in the package is supported by a known version of the AI Inference Server. Verifying that all the required Python packages are either included in the pipeline package itself or available on pypi.org for the target platform. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Static validation of pipeline packages. Executes static checks on a pipeline configuration package including: - Verifying that the Python version required in the package is supported by a known version of the AI Inference Server. - Verifying that all the required Python packages are either included in the pipeline package itself or available on `pypi.org` for the target platform. \"\"\" import subprocess from pathlib import Path import zipfile import logging from simaticai import deployment from simaticai.helpers import tempfiles , yaml_helper from simaticai.packaging.constants import REQUIREMENTS_TXT , PYTHON_PACKAGES_ZIP from simaticai.packaging.wheelhouse import create_wheelhouse logging . basicConfig () _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) INVALID_PIPELINE_PACKAGE_MESSAGE = \"Invalid pipeline configuration package. Perhaps you have passed an Edge configuration package instead?\" class PipelineValidationError ( Exception ): \"\"\" Represents a problem with the pipeline configuration. Args: description (str): Description of the error. Mandatory argument. \"\"\" def __init__ ( self , description : str ) -> None : assert description is not None self . value = description def __str__ ( self ): return self . value def validate_pipeline_dependencies ( zip_path ): \"\"\" @Deprecated, reason: In the future only the edge package will be generated and the same validation is performed during edge package creation. Validates an already built pipeline configuration package to check if it is compatible with the AI Inference Server. This method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. If the required dependency for the target platform is not available on pypi.org and not present in `PythonPackages.zip` it will log the problem at the ERROR level. Args: zip_path (path-like): Path to the pipeline configuration package zip file Raises: `pipeline_validator.PipelineValidationError` if the validation fails. See the logger output for details. \"\"\" with tempfiles . OpenZipInTemp ( zip_path ) as package_dir : package_dir = next ( package_dir . iterdir ()) error = read_config_and_download_deps ( package_dir ) if error : raise PipelineValidationError ( \"Requirements of one or more components can not be satisfied.\" ) _logger . info ( f \"Validating pipeline package ' { zip_path } ' was successful.\" ) def download_component_dependencies ( component : dict , package_dir : Path ): \"\"\" Download the dependencies of a pipeline component. Args: component (dict): A single component from the parsed `pipeline_configuration.yml`. package_dir (Path): The directory where the component was extracted \"\"\" _logger . info ( f \"Validating requirements of component: { component [ 'name' ] } \" ) try : deployment . python_version_validator ( component [ 'runtime' ][ 'version' ]) except ValueError as error : raise PipelineValidationError ( error ) component_dir = package_dir / component [ 'name' ] requirements_file_path = component_dir / REQUIREMENTS_TXT python_packages_folder = _build_python_packages_folder ( component_dir ) return not _are_dependencies_available ( requirements_file_path , component [ 'runtime' ][ 'version' ], python_packages_folder ) def read_config_and_download_deps ( package_dir : Path ): \"\"\" Reads the pipeline configuration from the package directory and downloads its dependencies Args: package_dir (Path): The directory where the pipeline configuration package was extracted. Returns: boolean: True if there was an error during the download of the components, False otherwise \"\"\" try : config = yaml_helper . read_yaml ( package_dir / 'pipeline_config.yml' ) error = False for component in config [ 'dataFlowPipeline' ][ 'components' ]: error = download_component_dependencies ( component , package_dir ) or error return error except Exception : raise PipelineValidationError ( INVALID_PIPELINE_PACKAGE_MESSAGE ) def _are_dependencies_available ( requirements_file_path : Path , python_version : str , python_packages_folder : Path ): try : if Path ( requirements_file_path ) . is_file (): try : create_wheelhouse ( requirements_file_path , python_version , python_packages_folder ) except RuntimeError as error : _logger . error ( f \"Error occurred while creating wheelhouse \\n { error . __str__ () } \" ) return False except AssertionError as error : _logger . error ( f \"Could not find a package that satisfies the requirements \\n { error . __str__ () } \" ) return False return True else : _logger . info ( f \"' { REQUIREMENTS_TXT } ' was not found.\" ) except subprocess . TimeoutExpired : _logger . error ( \"TimeoutExpired occurred during download\" ) return False except Exception as e : _logger . error ( f \"Unexpected exception occurred while downloading packages \\n { e . __str__ () } \" ) return False return True def _build_python_packages_folder ( component_dir : Path ) -> Path : assert component_dir is not None python_packages_folder = component_dir / 'packages' packages_file = component_dir / PYTHON_PACKAGES_ZIP python_packages_folder . mkdir ( exist_ok = True ) if packages_file . is_file (): with zipfile . ZipFile ( packages_file ) as zip_file : zip_file . extractall ( python_packages_folder ) return python_packages_folder","title":"Module simaticai.testing.pipeline_validator"},{"location":"reference/simaticai/testing/pipeline_validator.html#variables","text":"INVALID_PIPELINE_PACKAGE_MESSAGE PYTHON_PACKAGES_ZIP REQUIREMENTS_TXT","title":"Variables"},{"location":"reference/simaticai/testing/pipeline_validator.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/testing/pipeline_validator.html#download_component_dependencies","text":"def download_component_dependencies ( component : dict , package_dir : pathlib . Path ) Download the dependencies of a pipeline component. Parameters: Name Type Description Default component dict A single component from the parsed pipeline_configuration.yml . None package_dir Path The directory where the component was extracted None View Source def download_component_dependencies ( component : dict , package_dir : Path ): \"\"\" Download the dependencies of a pipeline component. Args: component (dict): A single component from the parsed `pipeline_configuration.yml`. package_dir (Path): The directory where the component was extracted \"\"\" _logger . info ( f \"Validating requirements of component: {component['name']}\" ) try : deployment . python_version_validator ( component [ 'runtime' ][ 'version' ]) except ValueError as error : raise PipelineValidationError ( error ) component_dir = package_dir / component [ 'name' ] requirements_file_path = component_dir / REQUIREMENTS_TXT python_packages_folder = _build_python_packages_folder ( component_dir ) return not _are_dependencies_available ( requirements_file_path , component [ 'runtime' ][ 'version' ], python_packages_folder )","title":"download_component_dependencies"},{"location":"reference/simaticai/testing/pipeline_validator.html#read_config_and_download_deps","text":"def read_config_and_download_deps ( package_dir : pathlib . Path ) Reads the pipeline configuration from the package directory and downloads its dependencies Parameters: Name Type Description Default package_dir Path The directory where the pipeline configuration package was extracted. None Returns: Type Description boolean True if there was an error during the download of the components, False otherwise View Source def read_config_and_download_deps ( package_dir : Path ): \"\"\" Reads the pipeline configuration from the package directory and downloads its dependencies Args: package_dir (Path): The directory where the pipeline configuration package was extracted. Returns: boolean: True if there was an error during the download of the components, False otherwise \"\"\" try : config = yaml_helper . read_yaml ( package_dir / 'pipeline_config.yml' ) error = False for component in config [ 'dataFlowPipeline' ][ 'components' ]: error = download_component_dependencies ( component , package_dir ) or error return error except Exception : raise PipelineValidationError ( INVALID_PIPELINE_PACKAGE_MESSAGE )","title":"read_config_and_download_deps"},{"location":"reference/simaticai/testing/pipeline_validator.html#validate_pipeline_dependencies","text":"def validate_pipeline_dependencies ( zip_path ) View Source def validate_pipeline_dependencies ( zip_path ): \"\"\" @Deprecated, reason: In the future only the edge package will be generated and the same validation is performed during edge package creation. Validates an already built pipeline configuration package to check if it is compatible with the AI Inference Server. This method verifies that the requirements identified by name and version are either included in `PythonPackages.zip` or available on pypi.org for the target platform. If the required dependency for the target platform is not available on pypi.org and not present in `PythonPackages.zip` it will log the problem at the ERROR level. Args: zip_path (path-like): Path to the pipeline configuration package zip file Raises: `pipeline_validator.PipelineValidationError` if the validation fails. See the logger output for details. \"\"\" with tempfiles . OpenZipInTemp ( zip_path ) as package_dir : package_dir = next ( package_dir . iterdir ()) error = read_config_and_download_deps ( package_dir ) if error : raise PipelineValidationError ( \"Requirements of one or more components can not be satisfied.\" ) _logger . info ( f \"Validating pipeline package '{zip_path}' was successful.\" )","title":"validate_pipeline_dependencies"},{"location":"reference/simaticai/testing/pipeline_validator.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/testing/pipeline_validator.html#pipelinevalidationerror","text":"Represents a problem with the pipeline configuration. class PipelineValidationError ( description : str )","title":"PipelineValidationError"},{"location":"reference/simaticai/testing/pipeline_validator.html#attributes","text":"Name Type Description Default description str Description of the error. Mandatory argument. None View Source class PipelineValidationError ( Exception ): \"\"\" Represents a problem with the pipeline configuration. Args: description (str): Description of the error. Mandatory argument. \"\"\" def __init__ ( self , description: str ) -> None: assert description is not None self . value = description def __str__ ( self ): return self . value","title":"Attributes"},{"location":"reference/simaticai/testing/pipeline_validator.html#ancestors-in-mro","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/testing/pipeline_validator.html#class-variables","text":"args","title":"Class variables"},{"location":"reference/simaticai/testing/pipeline_validator.html#methods","text":"","title":"Methods"},{"location":"reference/simaticai/testing/pipeline_validator.html#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/simaticai/testing/run_component.html","text":"Module simaticai.testing.run_component Utility script for running an entrypoint Python script in a given virtual Python environment. It is designed to be executed from simaticai.testing.PipelineRunner class. It consumes input data from a joblib file and produces output data into a joblib file. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Utility script for running an `entrypoint` Python script in a given virtual Python environment. It is designed to be executed from `simaticai.testing.PipelineRunner` class. It consumes input data from a joblib file and produces output data into a joblib file. \"\"\" import os import sys import json import argparse import joblib import logging import importlib import importlib.metadata import inspect from pathlib import Path logging . basicConfig () _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) def warn_about_unusued_dependencies ( requirements_list ): \"\"\" Raises a warning if some declared dependencies were not used during test execution This method compares the imported modules after test execution with the dependency list in the package's requirements.txt file. If the requirements contains more than what is required for execution, it raises a warning. Also warns the user if the `sqlite3` module is in use, because the current AI Inference Server does not support it. A third party replacement library can be used instead. \"\"\" imported_packages = [] packages = importlib . metadata . packages_distributions () for key , _ in sys . modules . items (): if key in packages : pkgs = packages . get ( key ) for pkg in pkgs : imported_packages . append ( pkg ) imported_packages = set ([ pkg . replace ( '-' , '_' ) . lower () for pkg in imported_packages ]) requirements_list = set ([ pkg . replace ( '-' , '_' ) . lower () for pkg in requirements_list ]) diff = requirements_list . difference ( imported_packages ) if 0 < len ( diff ): _logger . warning ( f \"WARNING! The following dependencies were not used during execution: { ', ' . join ( diff ) } . Consider removing them from the pipeline package.\" ) if 'sqlite3' in sys . modules : _logger . warning ( \"WARNING! sqlite3 is not part of AI Inference Server's Python runtime. Make sure to include a third party replacement library for sqlite3 in your pipeline package.\" ) def main ( args : argparse . Namespace ) -> int : \"\"\" Feeds input to the entrypoint and captures output. Imports entrypoint module given with its name, and triggers its `run(...)` function with the prepared data in the input file. If pipeline_parameters dictionary is not empty, before triggering `run(..)` method, the `update_parameters(..)` method of entrypoint will be called with the dictionary. The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. One dictionary represents one input for the component with the required variable names and values, which is directly passed to `run()`. The output file is a dumped joblib result which is a list containing outputs of the component, in the structure returned from `run()`. Args: module_name (str): Name of the entrypoint Python script input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored pipeline-parameters (json-string): json formatted dictionary defining configurable parameters with their names as key and their values \"\"\" entrypoint = importlib . import_module ( args . module_name ) trigger_method = None try : inspect . signature ( entrypoint . process_input ) trigger_method = \"process_input\" except AttributeError : try : inspect . signature ( entrypoint . run ) trigger_method = \"run\" except AttributeError : _logger . warning ( \"Method run not found\" ) if trigger_method is None : raise RuntimeError ( \"Neither 'run(data: str)' nor 'process_input(data: dict)' entrypoint method can be found.\" ) # configure Pipeline parameters if args . pipeline_parameters is not None and args . pipeline_parameters != \" {} \" : _logger . debug ( f \"Calling `update_parameters(..)` with: { args . pipeline_parameters } \" ) entrypoint . update_parameters ( json . loads ( args . pipeline_parameters )) input_list = joblib . load ( args . input_file ) if not isinstance ( input_list , list ): raise ValueError ( \"Component input must be supplied as a list.\" ) if trigger_method == \"process_input\" : _logger . debug ( \"Calling `process_input(..)`\" ) else : _logger . debug ( \"Calling `run(..)`\" ) output_list = [] for input_data in input_list : if trigger_method == \"process_input\" : output_list . append ( entrypoint . process_input ( input_data )) else : output_list . append ( entrypoint . run ( json . dumps ( input_data ))) if args . requirements_file is not None : requirements_list = Path ( args . requirements_file ) . read_text () . split ( '#' ) warn_about_unusued_dependencies ( requirements_list ) joblib . dump ( output_list , args . output_file ) if trigger_method == \"process_input\" : return 0 else : _logger . warning ( \"Trigger method `run(data: str)` is deprecated and will be removed in the future. Please refer the user manual.\" ) return 0b10001 # binary return code means deprecated run method was triggered if __name__ == '__main__' : _parser = argparse . ArgumentParser () _parser . add_argument ( \"-m\" , \"--module-name\" , type = str , help = \"The module which is implemented in the entrypoint Python script.\" ) _parser . add_argument ( \"-i\" , \"--input-file\" , type = str , help = \"The file which contains input data to test with component.\" ) _parser . add_argument ( \"-o\" , \"--output-file\" , type = str , help = \"The file which contains calculated output data.\" ) _parser . add_argument ( \"-ll\" , \"--log-level\" , default = \"INFO\" , type = str , help = \"Log Level using `logging` class' enum values.\" ) _parser . add_argument ( \"-p\" , \"--pipeline-parameters\" , type = str , help = \"Dict of configurable parameters with their values\" ) _parser . add_argument ( \"-r\" , \"--requirements-file\" , type = str , help = \"The file which contains the required dependencies.\" ) _args = _parser . parse_args () _logger . setLevel ( logging . getLevelName ( _args . log_level )) _logger . info ( f \"workdir: { os . path . abspath ( '.' ) } \" ) _logger . info ( f \"arguments: { _args } \" ) sys . exit ( main ( _args )) Functions main def main ( args : argparse . Namespace ) -> int Feeds input to the entrypoint and captures output. Imports entrypoint module given with its name, and triggers its run(...) function with the prepared data in the input file. If pipeline_parameters dictionary is not empty, before triggering run(..) method, the update_parameters(..) method of entrypoint will be called with the dictionary. The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. One dictionary represents one input for the component with the required variable names and values, which is directly passed to run() . The output file is a dumped joblib result which is a list containing outputs of the component, in the structure returned from run() . Parameters: Name Type Description Default module_name str Name of the entrypoint Python script None input_file os.Pathlike Path of the joblib file containing the input payloads None output_file os.Pathlike Path of the joblib file where the outputs will be stored None pipeline-parameters json-string json formatted dictionary defining configurable parameters with their names as key and their values None View Source def main ( args : argparse . Namespace ) -> int : \"\"\" Feeds input to the entrypoint and captures output. Imports entrypoint module given with its name, and triggers its `run(...)` function with the prepared data in the input file. If pipeline_parameters dictionary is not empty, before triggering `run(..)` method, the `update_parameters(..)` method of entrypoint will be called with the dictionary. The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. One dictionary represents one input for the component with the required variable names and values, which is directly passed to `run()`. The output file is a dumped joblib result which is a list containing outputs of the component, in the structure returned from `run()`. Args: module_name (str): Name of the entrypoint Python script input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored pipeline-parameters (json-string): json formatted dictionary defining configurable parameters with their names as key and their values \"\"\" entrypoint = importlib . import_module ( args . module_name ) trigger_method = None try : inspect . signature ( entrypoint . process_input ) trigger_method = \"process_input\" except AttributeError : try : inspect . signature ( entrypoint . run ) trigger_method = \"run\" except AttributeError : _logger . warning ( \"Method run not found\" ) if trigger_method is None : raise RuntimeError ( \"Neither 'run(data: str)' nor 'process_input(data: dict)' entrypoint method can be found.\" ) # configure Pipeline parameters if args . pipeline_parameters is not None and args . pipeline_parameters != \"{}\" : _logger . debug ( f \"Calling `update_parameters(..)` with: {args.pipeline_parameters}\" ) entrypoint . update_parameters ( json . loads ( args . pipeline_parameters )) input_list = joblib . load ( args . input_file ) if not isinstance ( input_list , list ): raise ValueError ( \"Component input must be supplied as a list.\" ) if trigger_method == \"process_input\" : _logger . debug ( \"Calling `process_input(..)`\" ) else : _logger . debug ( \"Calling `run(..)`\" ) output_list = [] for input_data in input_list : if trigger_method == \"process_input\" : output_list . append ( entrypoint . process_input ( input_data )) else : output_list . append ( entrypoint . run ( json . dumps ( input_data ))) if args . requirements_file is not None : requirements_list = Path ( args . requirements_file ) . read_text () . split ( '#' ) warn_about_unusued_dependencies ( requirements_list ) joblib . dump ( output_list , args . output_file ) if trigger_method == \"process_input\" : return 0 else : _logger . warning ( \"Trigger method `run(data: str)` is deprecated and will be removed in the future. Please refer the user manual.\" ) return 0 b10001 # binary return code means deprecated run method was triggered warn_about_unusued_dependencies def warn_about_unusued_dependencies ( requirements_list ) Raises a warning if some declared dependencies were not used during test execution This method compares the imported modules after test execution with the dependency list in the package's requirements.txt file. If the requirements contains more than what is required for execution, it raises a warning. Also warns the user if the sqlite3 module is in use, because the current AI Inference Server does not support it. A third party replacement library can be used instead. View Source def warn_about_unusued_dependencies ( requirements_list ) : \" \"\" Raises a warning if some declared dependencies were not used during test execution This method compares the imported modules after test execution with the dependency list in the package's requirements.txt file. If the requirements contains more than what is required for execution, it raises a warning. Also warns the user if the `sqlite3` module is in use, because the current AI Inference Server does not support it. A third party replacement library can be used instead. \"\" \" imported_packages = [] packages = importlib . metadata . packages_distributions () for key , _ in sys . modules . items () : if key in packages : pkgs = packages . get ( key ) for pkg in pkgs : imported_packages . append ( pkg ) imported_packages = set ( [ pkg . replace ( '-' , '_' ). lower () for pkg in imported_packages ] ) requirements_list = set ( [ pkg . replace ( '-' , '_' ). lower () for pkg in requirements_list ] ) diff = requirements_list . difference ( imported_packages ) if 0 < len ( diff ) : _logger . warning ( f \"WARNING! The following dependencies were not used during execution: {', '.join(diff)}. Consider removing them from the pipeline package.\" ) if 'sqlite3' in sys . modules : _logger . warning ( \"WARNING! sqlite3 is not part of AI Inference Server's Python runtime. Make sure to include a third party replacement library for sqlite3 in your pipeline package.\" )","title":"Run Component"},{"location":"reference/simaticai/testing/run_component.html#module-simaticaitestingrun_component","text":"Utility script for running an entrypoint Python script in a given virtual Python environment. It is designed to be executed from simaticai.testing.PipelineRunner class. It consumes input data from a joblib file and produces output data into a joblib file. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Utility script for running an `entrypoint` Python script in a given virtual Python environment. It is designed to be executed from `simaticai.testing.PipelineRunner` class. It consumes input data from a joblib file and produces output data into a joblib file. \"\"\" import os import sys import json import argparse import joblib import logging import importlib import importlib.metadata import inspect from pathlib import Path logging . basicConfig () _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) def warn_about_unusued_dependencies ( requirements_list ): \"\"\" Raises a warning if some declared dependencies were not used during test execution This method compares the imported modules after test execution with the dependency list in the package's requirements.txt file. If the requirements contains more than what is required for execution, it raises a warning. Also warns the user if the `sqlite3` module is in use, because the current AI Inference Server does not support it. A third party replacement library can be used instead. \"\"\" imported_packages = [] packages = importlib . metadata . packages_distributions () for key , _ in sys . modules . items (): if key in packages : pkgs = packages . get ( key ) for pkg in pkgs : imported_packages . append ( pkg ) imported_packages = set ([ pkg . replace ( '-' , '_' ) . lower () for pkg in imported_packages ]) requirements_list = set ([ pkg . replace ( '-' , '_' ) . lower () for pkg in requirements_list ]) diff = requirements_list . difference ( imported_packages ) if 0 < len ( diff ): _logger . warning ( f \"WARNING! The following dependencies were not used during execution: { ', ' . join ( diff ) } . Consider removing them from the pipeline package.\" ) if 'sqlite3' in sys . modules : _logger . warning ( \"WARNING! sqlite3 is not part of AI Inference Server's Python runtime. Make sure to include a third party replacement library for sqlite3 in your pipeline package.\" ) def main ( args : argparse . Namespace ) -> int : \"\"\" Feeds input to the entrypoint and captures output. Imports entrypoint module given with its name, and triggers its `run(...)` function with the prepared data in the input file. If pipeline_parameters dictionary is not empty, before triggering `run(..)` method, the `update_parameters(..)` method of entrypoint will be called with the dictionary. The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. One dictionary represents one input for the component with the required variable names and values, which is directly passed to `run()`. The output file is a dumped joblib result which is a list containing outputs of the component, in the structure returned from `run()`. Args: module_name (str): Name of the entrypoint Python script input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored pipeline-parameters (json-string): json formatted dictionary defining configurable parameters with their names as key and their values \"\"\" entrypoint = importlib . import_module ( args . module_name ) trigger_method = None try : inspect . signature ( entrypoint . process_input ) trigger_method = \"process_input\" except AttributeError : try : inspect . signature ( entrypoint . run ) trigger_method = \"run\" except AttributeError : _logger . warning ( \"Method run not found\" ) if trigger_method is None : raise RuntimeError ( \"Neither 'run(data: str)' nor 'process_input(data: dict)' entrypoint method can be found.\" ) # configure Pipeline parameters if args . pipeline_parameters is not None and args . pipeline_parameters != \" {} \" : _logger . debug ( f \"Calling `update_parameters(..)` with: { args . pipeline_parameters } \" ) entrypoint . update_parameters ( json . loads ( args . pipeline_parameters )) input_list = joblib . load ( args . input_file ) if not isinstance ( input_list , list ): raise ValueError ( \"Component input must be supplied as a list.\" ) if trigger_method == \"process_input\" : _logger . debug ( \"Calling `process_input(..)`\" ) else : _logger . debug ( \"Calling `run(..)`\" ) output_list = [] for input_data in input_list : if trigger_method == \"process_input\" : output_list . append ( entrypoint . process_input ( input_data )) else : output_list . append ( entrypoint . run ( json . dumps ( input_data ))) if args . requirements_file is not None : requirements_list = Path ( args . requirements_file ) . read_text () . split ( '#' ) warn_about_unusued_dependencies ( requirements_list ) joblib . dump ( output_list , args . output_file ) if trigger_method == \"process_input\" : return 0 else : _logger . warning ( \"Trigger method `run(data: str)` is deprecated and will be removed in the future. Please refer the user manual.\" ) return 0b10001 # binary return code means deprecated run method was triggered if __name__ == '__main__' : _parser = argparse . ArgumentParser () _parser . add_argument ( \"-m\" , \"--module-name\" , type = str , help = \"The module which is implemented in the entrypoint Python script.\" ) _parser . add_argument ( \"-i\" , \"--input-file\" , type = str , help = \"The file which contains input data to test with component.\" ) _parser . add_argument ( \"-o\" , \"--output-file\" , type = str , help = \"The file which contains calculated output data.\" ) _parser . add_argument ( \"-ll\" , \"--log-level\" , default = \"INFO\" , type = str , help = \"Log Level using `logging` class' enum values.\" ) _parser . add_argument ( \"-p\" , \"--pipeline-parameters\" , type = str , help = \"Dict of configurable parameters with their values\" ) _parser . add_argument ( \"-r\" , \"--requirements-file\" , type = str , help = \"The file which contains the required dependencies.\" ) _args = _parser . parse_args () _logger . setLevel ( logging . getLevelName ( _args . log_level )) _logger . info ( f \"workdir: { os . path . abspath ( '.' ) } \" ) _logger . info ( f \"arguments: { _args } \" ) sys . exit ( main ( _args ))","title":"Module simaticai.testing.run_component"},{"location":"reference/simaticai/testing/run_component.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/testing/run_component.html#main","text":"def main ( args : argparse . Namespace ) -> int Feeds input to the entrypoint and captures output. Imports entrypoint module given with its name, and triggers its run(...) function with the prepared data in the input file. If pipeline_parameters dictionary is not empty, before triggering run(..) method, the update_parameters(..) method of entrypoint will be called with the dictionary. The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. One dictionary represents one input for the component with the required variable names and values, which is directly passed to run() . The output file is a dumped joblib result which is a list containing outputs of the component, in the structure returned from run() . Parameters: Name Type Description Default module_name str Name of the entrypoint Python script None input_file os.Pathlike Path of the joblib file containing the input payloads None output_file os.Pathlike Path of the joblib file where the outputs will be stored None pipeline-parameters json-string json formatted dictionary defining configurable parameters with their names as key and their values None View Source def main ( args : argparse . Namespace ) -> int : \"\"\" Feeds input to the entrypoint and captures output. Imports entrypoint module given with its name, and triggers its `run(...)` function with the prepared data in the input file. If pipeline_parameters dictionary is not empty, before triggering `run(..)` method, the `update_parameters(..)` method of entrypoint will be called with the dictionary. The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. One dictionary represents one input for the component with the required variable names and values, which is directly passed to `run()`. The output file is a dumped joblib result which is a list containing outputs of the component, in the structure returned from `run()`. Args: module_name (str): Name of the entrypoint Python script input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored pipeline-parameters (json-string): json formatted dictionary defining configurable parameters with their names as key and their values \"\"\" entrypoint = importlib . import_module ( args . module_name ) trigger_method = None try : inspect . signature ( entrypoint . process_input ) trigger_method = \"process_input\" except AttributeError : try : inspect . signature ( entrypoint . run ) trigger_method = \"run\" except AttributeError : _logger . warning ( \"Method run not found\" ) if trigger_method is None : raise RuntimeError ( \"Neither 'run(data: str)' nor 'process_input(data: dict)' entrypoint method can be found.\" ) # configure Pipeline parameters if args . pipeline_parameters is not None and args . pipeline_parameters != \"{}\" : _logger . debug ( f \"Calling `update_parameters(..)` with: {args.pipeline_parameters}\" ) entrypoint . update_parameters ( json . loads ( args . pipeline_parameters )) input_list = joblib . load ( args . input_file ) if not isinstance ( input_list , list ): raise ValueError ( \"Component input must be supplied as a list.\" ) if trigger_method == \"process_input\" : _logger . debug ( \"Calling `process_input(..)`\" ) else : _logger . debug ( \"Calling `run(..)`\" ) output_list = [] for input_data in input_list : if trigger_method == \"process_input\" : output_list . append ( entrypoint . process_input ( input_data )) else : output_list . append ( entrypoint . run ( json . dumps ( input_data ))) if args . requirements_file is not None : requirements_list = Path ( args . requirements_file ) . read_text () . split ( '#' ) warn_about_unusued_dependencies ( requirements_list ) joblib . dump ( output_list , args . output_file ) if trigger_method == \"process_input\" : return 0 else : _logger . warning ( \"Trigger method `run(data: str)` is deprecated and will be removed in the future. Please refer the user manual.\" ) return 0 b10001 # binary return code means deprecated run method was triggered","title":"main"},{"location":"reference/simaticai/testing/run_component.html#warn_about_unusued_dependencies","text":"def warn_about_unusued_dependencies ( requirements_list ) Raises a warning if some declared dependencies were not used during test execution This method compares the imported modules after test execution with the dependency list in the package's requirements.txt file. If the requirements contains more than what is required for execution, it raises a warning. Also warns the user if the sqlite3 module is in use, because the current AI Inference Server does not support it. A third party replacement library can be used instead. View Source def warn_about_unusued_dependencies ( requirements_list ) : \" \"\" Raises a warning if some declared dependencies were not used during test execution This method compares the imported modules after test execution with the dependency list in the package's requirements.txt file. If the requirements contains more than what is required for execution, it raises a warning. Also warns the user if the `sqlite3` module is in use, because the current AI Inference Server does not support it. A third party replacement library can be used instead. \"\" \" imported_packages = [] packages = importlib . metadata . packages_distributions () for key , _ in sys . modules . items () : if key in packages : pkgs = packages . get ( key ) for pkg in pkgs : imported_packages . append ( pkg ) imported_packages = set ( [ pkg . replace ( '-' , '_' ). lower () for pkg in imported_packages ] ) requirements_list = set ( [ pkg . replace ( '-' , '_' ). lower () for pkg in requirements_list ] ) diff = requirements_list . difference ( imported_packages ) if 0 < len ( diff ) : _logger . warning ( f \"WARNING! The following dependencies were not used during execution: {', '.join(diff)}. Consider removing them from the pipeline package.\" ) if 'sqlite3' in sys . modules : _logger . warning ( \"WARNING! sqlite3 is not part of AI Inference Server's Python runtime. Make sure to include a third party replacement library for sqlite3 in your pipeline package.\" )","title":"warn_about_unusued_dependencies"},{"location":"reference/simaticai/testing/run_gpuruntime_component.html","text":"Module simaticai.testing.run_gpuruntime_component Utility script for running an ONNX model in a given virtual Python environment. It is designed to be executed from simaticai.testing.PipelineRunner class. It consumes input data from a joblib file and produces output data into a joblib file. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Utility script for running an ONNX model in a given virtual Python environment. It is designed to be executed from `simaticai.testing.PipelineRunner` class. It consumes input data from a joblib file and produces output data into a joblib file. \"\"\" import argparse import joblib import logging import numpy import os import sys from onnxruntime import InferenceSession from google.protobuf import text_format from google.protobuf import json_format try : import model_config_pb2 except ImportError : import simaticai.model_config_pb2 as model_config_pb2 # for documentation build logging . basicConfig () _logger = logging . getLogger ( \"simaticai.testing.pipeline_runner.run_gpuruntime_component\" ) _logger . setLevel ( logging . INFO ) def _get_proto_config ( config_path ): \"\"\"Reads model configuration from config.pbtxt into a dictionary Args: config_path (os.PathLike): path to `config.pbtxt` file, generally model_path.parents[1] / \"config.pbtxt\" \"\"\" with open ( config_path , 'r' ) as file : config_msg = text_format . Parse ( file . read (), model_config_pb2 . ModelConfig ()) config_dict = json_format . MessageToDict ( config_msg ) return config_dict def _get_new_shape ( config_input , model_input , input_name ): \"\"\"Calculates the input shape if it is a batch of data. In case of batched input, the number of input arrays must be max_batch_size or less. The input shape is a flat array and must be reshaped based on one input shape, and the size of batch can be calculated. Args: config_input (dict): dictionary with information of the input type and shape from configuration file model_input (dict): standard input format for GPU Runtime, the input tensor is flattened into numpy array input_name (str): name of the actual model_input to search in config_input \"\"\" config_shape = numpy . array ([ i [ \"dims\" ] for i in config_input if i [ \"name\" ] == input_name ]) . flatten () . astype ( numpy . int32 ) img_size = config_shape . prod () input_size = numpy . prod ( model_input . shape ) batch_size = input_size // img_size return numpy . concatenate (([ batch_size ], config_shape )) def main ( model_path , config_path , input_file , output_file ): \"\"\"Feeds input to the ML Model saved in ONNX format and captures output. Reads the given model and creates an onnxruntime Session to The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. The output file is a dumped joblib result list containing the input dictionary extended with the generated predictions. Args: model_path (str): File path for the stored ML Model in ONNX format input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored \"\"\" input_list = joblib . load ( input_file ) input_list = input_list if type ( input_list ) is list else [ input_list ] output_list = input_list . copy () session = InferenceSession ( model_path ) model_config = _get_proto_config ( config_path ) max_batch_size = model_config . get ( \"maxBatchSize\" , 0 ) inputs = [ input for input in session . get_inputs ()] outputs = [ output for output in session . get_outputs ()] predictions = [ output . name for output in outputs ] for _output in output_list : _input_tensor = {} for _input in inputs : if max_batch_size > 0 : input_shape = _get_new_shape ( model_config [ \"input\" ], _output [ _input . name ], _input . name ) if input_shape [ 0 ] > max_batch_size : _logger . warning ( f \"Received input batch size ( { input_shape [ 0 ] } ) is greater than max_batch_size ( { max_batch_size } )!\" ) _input_tensor [ _input . name ] = _output [ _input . name ] . reshape ( input_shape ) else : _input_tensor [ _input . name ] = _output [ _input . name ] . reshape ( _input . shape ) _predictions = session . run ( predictions , _input_tensor ) for i in range ( len ( _predictions )): _output [ predictions [ i ]] = _predictions [ i ] joblib . dump ( output_list , output_file ) return 0 if __name__ == '__main__' : _parser = argparse . ArgumentParser () _parser . add_argument ( \"-m\" , \"--model-path\" , type = str , help = \"The path of ONNX file.\" ) _parser . add_argument ( \"-c\" , \"--config-path\" , type = str , help = \"The path of config.pbtxt\" ) _parser . add_argument ( \"-i\" , \"--input-file\" , type = str , help = \"The file which contains input data to test with component.\" ) _parser . add_argument ( \"-o\" , \"--output-file\" , type = str , help = \"The file which contains calculated output data.\" ) _parser . add_argument ( \"-ll\" , \"--log-level\" , default = \"INFO\" , type = str , help = \"Log Level using `logging` class' enum values.\" ) _args = _parser . parse_args () _logger . setLevel ( logging . getLevelName ( _args . log_level )) _logger . info ( f \"arguments: \\t { _args } \" ) _logger . info ( f \"workdir: \\t { os . path . abspath ( '.' ) } \" ) sys . exit ( main ( _args . model_path , _args . config_path , _args . input_file , _args . output_file )) Functions main def main ( model_path , config_path , input_file , output_file ) Feeds input to the ML Model saved in ONNX format and captures output. Reads the given model and creates an onnxruntime Session to The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. The output file is a dumped joblib result list containing the input dictionary extended with the generated predictions. Parameters: Name Type Description Default model_path str File path for the stored ML Model in ONNX format None input_file os.Pathlike Path of the joblib file containing the input payloads None output_file os.Pathlike Path of the joblib file where the outputs will be stored None View Source def main ( model_path , config_path , input_file , output_file ) : \"\"\"Feeds input to the ML Model saved in ONNX format and captures output. Reads the given model and creates an onnxruntime Session to The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. The output file is a dumped joblib result list containing the input dictionary extended with the generated predictions. Args: model_path (str): File path for the stored ML Model in ONNX format input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored \"\"\" input_list = joblib . load ( input_file ) input_list = input_list if type ( input_list ) is list else [ input_list ] output_list = input_list . copy () session = InferenceSession ( model_path ) model_config = _get_proto_config ( config_path ) max_batch_size = model_config . get ( \"maxBatchSize\" , 0 ) inputs = [ input for input in session.get_inputs() ] outputs = [ output for output in session.get_outputs() ] predictions = [ output.name for output in outputs ] for _output in output_list : _input_tensor = {} for _input in inputs : if max_batch_size > 0 : input_shape = _get_new_shape ( model_config [ \"input\" ] , _output [ _input.name ] , _input . name ) if input_shape [ 0 ] > max_batch_size : _logger . warning ( f \"Received input batch size ({input_shape[0]}) is greater than max_batch_size ({max_batch_size})!\" ) _input_tensor [ _input.name ] = _output [ _input.name ] . reshape ( input_shape ) else : _input_tensor [ _input.name ] = _output [ _input.name ] . reshape ( _input . shape ) _predictions = session . run ( predictions , _input_tensor ) for i in range ( len ( _predictions )) : _output [ predictions[i ] ] = _predictions [ i ] joblib . dump ( output_list , output_file ) return 0","title":"Run Gpuruntime Component"},{"location":"reference/simaticai/testing/run_gpuruntime_component.html#module-simaticaitestingrun_gpuruntime_component","text":"Utility script for running an ONNX model in a given virtual Python environment. It is designed to be executed from simaticai.testing.PipelineRunner class. It consumes input data from a joblib file and produces output data into a joblib file. View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. \"\"\" Utility script for running an ONNX model in a given virtual Python environment. It is designed to be executed from `simaticai.testing.PipelineRunner` class. It consumes input data from a joblib file and produces output data into a joblib file. \"\"\" import argparse import joblib import logging import numpy import os import sys from onnxruntime import InferenceSession from google.protobuf import text_format from google.protobuf import json_format try : import model_config_pb2 except ImportError : import simaticai.model_config_pb2 as model_config_pb2 # for documentation build logging . basicConfig () _logger = logging . getLogger ( \"simaticai.testing.pipeline_runner.run_gpuruntime_component\" ) _logger . setLevel ( logging . INFO ) def _get_proto_config ( config_path ): \"\"\"Reads model configuration from config.pbtxt into a dictionary Args: config_path (os.PathLike): path to `config.pbtxt` file, generally model_path.parents[1] / \"config.pbtxt\" \"\"\" with open ( config_path , 'r' ) as file : config_msg = text_format . Parse ( file . read (), model_config_pb2 . ModelConfig ()) config_dict = json_format . MessageToDict ( config_msg ) return config_dict def _get_new_shape ( config_input , model_input , input_name ): \"\"\"Calculates the input shape if it is a batch of data. In case of batched input, the number of input arrays must be max_batch_size or less. The input shape is a flat array and must be reshaped based on one input shape, and the size of batch can be calculated. Args: config_input (dict): dictionary with information of the input type and shape from configuration file model_input (dict): standard input format for GPU Runtime, the input tensor is flattened into numpy array input_name (str): name of the actual model_input to search in config_input \"\"\" config_shape = numpy . array ([ i [ \"dims\" ] for i in config_input if i [ \"name\" ] == input_name ]) . flatten () . astype ( numpy . int32 ) img_size = config_shape . prod () input_size = numpy . prod ( model_input . shape ) batch_size = input_size // img_size return numpy . concatenate (([ batch_size ], config_shape )) def main ( model_path , config_path , input_file , output_file ): \"\"\"Feeds input to the ML Model saved in ONNX format and captures output. Reads the given model and creates an onnxruntime Session to The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. The output file is a dumped joblib result list containing the input dictionary extended with the generated predictions. Args: model_path (str): File path for the stored ML Model in ONNX format input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored \"\"\" input_list = joblib . load ( input_file ) input_list = input_list if type ( input_list ) is list else [ input_list ] output_list = input_list . copy () session = InferenceSession ( model_path ) model_config = _get_proto_config ( config_path ) max_batch_size = model_config . get ( \"maxBatchSize\" , 0 ) inputs = [ input for input in session . get_inputs ()] outputs = [ output for output in session . get_outputs ()] predictions = [ output . name for output in outputs ] for _output in output_list : _input_tensor = {} for _input in inputs : if max_batch_size > 0 : input_shape = _get_new_shape ( model_config [ \"input\" ], _output [ _input . name ], _input . name ) if input_shape [ 0 ] > max_batch_size : _logger . warning ( f \"Received input batch size ( { input_shape [ 0 ] } ) is greater than max_batch_size ( { max_batch_size } )!\" ) _input_tensor [ _input . name ] = _output [ _input . name ] . reshape ( input_shape ) else : _input_tensor [ _input . name ] = _output [ _input . name ] . reshape ( _input . shape ) _predictions = session . run ( predictions , _input_tensor ) for i in range ( len ( _predictions )): _output [ predictions [ i ]] = _predictions [ i ] joblib . dump ( output_list , output_file ) return 0 if __name__ == '__main__' : _parser = argparse . ArgumentParser () _parser . add_argument ( \"-m\" , \"--model-path\" , type = str , help = \"The path of ONNX file.\" ) _parser . add_argument ( \"-c\" , \"--config-path\" , type = str , help = \"The path of config.pbtxt\" ) _parser . add_argument ( \"-i\" , \"--input-file\" , type = str , help = \"The file which contains input data to test with component.\" ) _parser . add_argument ( \"-o\" , \"--output-file\" , type = str , help = \"The file which contains calculated output data.\" ) _parser . add_argument ( \"-ll\" , \"--log-level\" , default = \"INFO\" , type = str , help = \"Log Level using `logging` class' enum values.\" ) _args = _parser . parse_args () _logger . setLevel ( logging . getLevelName ( _args . log_level )) _logger . info ( f \"arguments: \\t { _args } \" ) _logger . info ( f \"workdir: \\t { os . path . abspath ( '.' ) } \" ) sys . exit ( main ( _args . model_path , _args . config_path , _args . input_file , _args . output_file ))","title":"Module simaticai.testing.run_gpuruntime_component"},{"location":"reference/simaticai/testing/run_gpuruntime_component.html#functions","text":"","title":"Functions"},{"location":"reference/simaticai/testing/run_gpuruntime_component.html#main","text":"def main ( model_path , config_path , input_file , output_file ) Feeds input to the ML Model saved in ONNX format and captures output. Reads the given model and creates an onnxruntime Session to The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. The output file is a dumped joblib result list containing the input dictionary extended with the generated predictions. Parameters: Name Type Description Default model_path str File path for the stored ML Model in ONNX format None input_file os.Pathlike Path of the joblib file containing the input payloads None output_file os.Pathlike Path of the joblib file where the outputs will be stored None View Source def main ( model_path , config_path , input_file , output_file ) : \"\"\"Feeds input to the ML Model saved in ONNX format and captures output. Reads the given model and creates an onnxruntime Session to The input file must be a joblib dump, and the joblib must be a dictionary or list of dictionaries. The output file is a dumped joblib result list containing the input dictionary extended with the generated predictions. Args: model_path (str): File path for the stored ML Model in ONNX format input_file (os.Pathlike): Path of the joblib file containing the input payloads output_file (os.Pathlike): Path of the joblib file where the outputs will be stored \"\"\" input_list = joblib . load ( input_file ) input_list = input_list if type ( input_list ) is list else [ input_list ] output_list = input_list . copy () session = InferenceSession ( model_path ) model_config = _get_proto_config ( config_path ) max_batch_size = model_config . get ( \"maxBatchSize\" , 0 ) inputs = [ input for input in session.get_inputs() ] outputs = [ output for output in session.get_outputs() ] predictions = [ output.name for output in outputs ] for _output in output_list : _input_tensor = {} for _input in inputs : if max_batch_size > 0 : input_shape = _get_new_shape ( model_config [ \"input\" ] , _output [ _input.name ] , _input . name ) if input_shape [ 0 ] > max_batch_size : _logger . warning ( f \"Received input batch size ({input_shape[0]}) is greater than max_batch_size ({max_batch_size})!\" ) _input_tensor [ _input.name ] = _output [ _input.name ] . reshape ( input_shape ) else : _input_tensor [ _input.name ] = _output [ _input.name ] . reshape ( _input . shape ) _predictions = session . run ( predictions , _input_tensor ) for i in range ( len ( _predictions )) : _output [ predictions[i ] ] = _predictions [ i ] joblib . dump ( output_list , output_file ) return 0","title":"main"},{"location":"reference/simaticai/testing/timeseries_stream.html","text":"Module simaticai.testing.timeseries_stream None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os import re import sys import csv import logging from pathlib import Path from simaticai.testing.data_stream import DataStream from typing import Optional logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) class TimeSeriesStream ( DataStream ): \"\"\" This class creates a generator from a csv file. The generate function returns a generator that reads the csv file line by line and converts each line to an input dictionary, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , csv_path : os . PathLike , * , fields : Optional [ list ] = None , count : Optional [ int ] = None , offset : Optional [ int ] = None , batch_size : Optional [ int ] = None ): \"\"\" Creates a new TimeSeriesStream object Args: data (os.Pathlike): Path to the csv file \"\"\" self . csv_path = Path ( csv_path ) if fields is None : self . fields = [] else : self . fields = fields with open ( self . csv_path , \"r\" ) as file : self . header = file . readline () . strip () . split ( \",\" ) if any ( key for key in self . fields if key not in self . header ): raise KeyError ( \"The CSV file must contain variable names in the first row.\" ) if self . fields == []: self . fields = self . header fields_len = len ( self . fields ) filtered_len = len ( list ( k for k in self . fields if re . match ( \"[a-zA-Z]+\" , k ))) if filtered_len != fields_len : raise KeyError ( \"Column headers should start with a letter.\" ) if ( not isinstance ( count , int )) or count < 0 : self . count = 0 else : self . count = count if ( not isinstance ( offset , int )) or offset < 0 : self . offset = 0 else : self . offset = offset if ( not isinstance ( batch_size , int )) or batch_size < 0 : self . batch_size = 0 else : self . batch_size = batch_size def _read_value ( self , value ): try : return int ( value ) except ValueError : try : return float ( value ) except ValueError : return value def _read_csv ( self ): with open ( self . csv_path , 'r' , encoding = 'UTF-8' ) as csv_file : csv_reader = csv . DictReader ( csv_file ) for line in csv_reader : if 0 < len ( self . fields ): key_holder = self . fields else : key_holder = line . keys () result = {} for k in key_holder : result [ k ] = self . _read_value ( line [ k ]) yield result def _limit ( self ): _max = float ( 'inf' ) if 0 < self . count : _max = self . offset + self . count counter = 0 for line in self . _read_csv (): if _max <= counter : break counter += 1 if counter <= self . offset : continue yield line def _batch ( self ): aggregate = [] for line in self . _limit (): if len ( aggregate ) < self . batch_size : aggregate += [ line ] continue yield aggregate aggregate = [ line ] if self . batch_size == len ( aggregate ): yield aggregate else : _logger . warning ( f \"WARNING! The length of the given dataset is not divisible by { self . batch_size } . There are { len ( aggregate ) } inputs remaining in the buffer.\" ) def __iter__ ( self ): \"\"\" Creates the input data generator. Returns: a generator \"\"\" if 0 < self . batch_size : return self . _batch () else : return self . _limit () Classes TimeSeriesStream This class creates a generator from a csv file. The generate function returns a generator that reads the csv file line by line and converts each line to an input dictionary, as if it were received from AI Inference Server. class TimeSeriesStream ( csv_path : os . PathLike , * , fields : Optional [ list ] = None , count : Optional [ int ] = None , offset : Optional [ int ] = None , batch_size : Optional [ int ] = None ) View Source class TimeSeriesStream ( DataStream ) : \"\"\" This class creates a generator from a csv file. The generate function returns a generator that reads the csv file line by line and converts each line to an input dictionary, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , csv_path : os . PathLike , * , fields : Optional [ list ] = None , count : Optional [ int ] = None , offset : Optional [ int ] = None , batch_size : Optional [ int ] = None ) : \"\"\" Creates a new TimeSeriesStream object Args: data (os.Pathlike): Path to the csv file \"\"\" self . csv_path = Path ( csv_path ) if fields is None : self . fields = [] else : self . fields = fields with open ( self . csv_path , \"r\" ) as file : self . header = file . readline (). strip (). split ( \",\" ) if any ( key for key in self . fields if key not in self . header ) : raise KeyError ( \"The CSV file must contain variable names in the first row.\" ) if self . fields == []: self . fields = self . header fields_len = len ( self . fields ) filtered_len = len ( list ( k for k in self . fields if re . match ( \"[a-zA-Z]+\" , k ))) if filtered_len != fields_len : raise KeyError ( \"Column headers should start with a letter.\" ) if ( not isinstance ( count , int )) or count < 0 : self . count = 0 else : self . count = count if ( not isinstance ( offset , int )) or offset < 0 : self . offset = 0 else : self . offset = offset if ( not isinstance ( batch_size , int )) or batch_size < 0 : self . batch_size = 0 else : self . batch_size = batch_size def _read_value ( self , value ) : try : return int ( value ) except ValueError : try : return float ( value ) except ValueError : return value def _read_csv ( self ) : with open ( self . csv_path , 'r' , encoding = 'UTF-8' ) as csv_file : csv_reader = csv . DictReader ( csv_file ) for line in csv_reader : if 0 < len ( self . fields ) : key_holder = self . fields else : key_holder = line . keys () result = {} for k in key_holder : result [ k ] = self . _read_value ( line [ k ] ) yield result def _limit ( self ) : _max = float ( 'inf' ) if 0 < self . count : _max = self . offset + self . count counter = 0 for line in self . _read_csv () : if _max <= counter : break counter += 1 if counter <= self . offset : continue yield line def _batch ( self ) : aggregate = [] for line in self . _limit () : if len ( aggregate ) < self . batch_size : aggregate += [ line ] continue yield aggregate aggregate = [ line ] if self . batch_size == len ( aggregate ) : yield aggregate else : _logger . warning ( f \"WARNING! The length of the given dataset is not divisible by {self.batch_size}. There are {len(aggregate)} inputs remaining in the buffer.\" ) def __iter__ ( self ) : \"\"\" Creates the input data generator. Returns: a generator \"\"\" if 0 < self . batch_size : return self . _batch () else : return self . _limit () Ancestors (in MRO) simaticai.testing.data_stream.DataStream","title":"Timeseries Stream"},{"location":"reference/simaticai/testing/timeseries_stream.html#module-simaticaitestingtimeseries_stream","text":"None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os import re import sys import csv import logging from pathlib import Path from simaticai.testing.data_stream import DataStream from typing import Optional logging . basicConfig () logging . getLogger () . handlers = [ logging . StreamHandler ( sys . stdout )] _logger = logging . getLogger ( __name__ ) _logger . setLevel ( logging . INFO ) class TimeSeriesStream ( DataStream ): \"\"\" This class creates a generator from a csv file. The generate function returns a generator that reads the csv file line by line and converts each line to an input dictionary, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , csv_path : os . PathLike , * , fields : Optional [ list ] = None , count : Optional [ int ] = None , offset : Optional [ int ] = None , batch_size : Optional [ int ] = None ): \"\"\" Creates a new TimeSeriesStream object Args: data (os.Pathlike): Path to the csv file \"\"\" self . csv_path = Path ( csv_path ) if fields is None : self . fields = [] else : self . fields = fields with open ( self . csv_path , \"r\" ) as file : self . header = file . readline () . strip () . split ( \",\" ) if any ( key for key in self . fields if key not in self . header ): raise KeyError ( \"The CSV file must contain variable names in the first row.\" ) if self . fields == []: self . fields = self . header fields_len = len ( self . fields ) filtered_len = len ( list ( k for k in self . fields if re . match ( \"[a-zA-Z]+\" , k ))) if filtered_len != fields_len : raise KeyError ( \"Column headers should start with a letter.\" ) if ( not isinstance ( count , int )) or count < 0 : self . count = 0 else : self . count = count if ( not isinstance ( offset , int )) or offset < 0 : self . offset = 0 else : self . offset = offset if ( not isinstance ( batch_size , int )) or batch_size < 0 : self . batch_size = 0 else : self . batch_size = batch_size def _read_value ( self , value ): try : return int ( value ) except ValueError : try : return float ( value ) except ValueError : return value def _read_csv ( self ): with open ( self . csv_path , 'r' , encoding = 'UTF-8' ) as csv_file : csv_reader = csv . DictReader ( csv_file ) for line in csv_reader : if 0 < len ( self . fields ): key_holder = self . fields else : key_holder = line . keys () result = {} for k in key_holder : result [ k ] = self . _read_value ( line [ k ]) yield result def _limit ( self ): _max = float ( 'inf' ) if 0 < self . count : _max = self . offset + self . count counter = 0 for line in self . _read_csv (): if _max <= counter : break counter += 1 if counter <= self . offset : continue yield line def _batch ( self ): aggregate = [] for line in self . _limit (): if len ( aggregate ) < self . batch_size : aggregate += [ line ] continue yield aggregate aggregate = [ line ] if self . batch_size == len ( aggregate ): yield aggregate else : _logger . warning ( f \"WARNING! The length of the given dataset is not divisible by { self . batch_size } . There are { len ( aggregate ) } inputs remaining in the buffer.\" ) def __iter__ ( self ): \"\"\" Creates the input data generator. Returns: a generator \"\"\" if 0 < self . batch_size : return self . _batch () else : return self . _limit ()","title":"Module simaticai.testing.timeseries_stream"},{"location":"reference/simaticai/testing/timeseries_stream.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/testing/timeseries_stream.html#timeseriesstream","text":"This class creates a generator from a csv file. The generate function returns a generator that reads the csv file line by line and converts each line to an input dictionary, as if it were received from AI Inference Server. class TimeSeriesStream ( csv_path : os . PathLike , * , fields : Optional [ list ] = None , count : Optional [ int ] = None , offset : Optional [ int ] = None , batch_size : Optional [ int ] = None ) View Source class TimeSeriesStream ( DataStream ) : \"\"\" This class creates a generator from a csv file. The generate function returns a generator that reads the csv file line by line and converts each line to an input dictionary, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , csv_path : os . PathLike , * , fields : Optional [ list ] = None , count : Optional [ int ] = None , offset : Optional [ int ] = None , batch_size : Optional [ int ] = None ) : \"\"\" Creates a new TimeSeriesStream object Args: data (os.Pathlike): Path to the csv file \"\"\" self . csv_path = Path ( csv_path ) if fields is None : self . fields = [] else : self . fields = fields with open ( self . csv_path , \"r\" ) as file : self . header = file . readline (). strip (). split ( \",\" ) if any ( key for key in self . fields if key not in self . header ) : raise KeyError ( \"The CSV file must contain variable names in the first row.\" ) if self . fields == []: self . fields = self . header fields_len = len ( self . fields ) filtered_len = len ( list ( k for k in self . fields if re . match ( \"[a-zA-Z]+\" , k ))) if filtered_len != fields_len : raise KeyError ( \"Column headers should start with a letter.\" ) if ( not isinstance ( count , int )) or count < 0 : self . count = 0 else : self . count = count if ( not isinstance ( offset , int )) or offset < 0 : self . offset = 0 else : self . offset = offset if ( not isinstance ( batch_size , int )) or batch_size < 0 : self . batch_size = 0 else : self . batch_size = batch_size def _read_value ( self , value ) : try : return int ( value ) except ValueError : try : return float ( value ) except ValueError : return value def _read_csv ( self ) : with open ( self . csv_path , 'r' , encoding = 'UTF-8' ) as csv_file : csv_reader = csv . DictReader ( csv_file ) for line in csv_reader : if 0 < len ( self . fields ) : key_holder = self . fields else : key_holder = line . keys () result = {} for k in key_holder : result [ k ] = self . _read_value ( line [ k ] ) yield result def _limit ( self ) : _max = float ( 'inf' ) if 0 < self . count : _max = self . offset + self . count counter = 0 for line in self . _read_csv () : if _max <= counter : break counter += 1 if counter <= self . offset : continue yield line def _batch ( self ) : aggregate = [] for line in self . _limit () : if len ( aggregate ) < self . batch_size : aggregate += [ line ] continue yield aggregate aggregate = [ line ] if self . batch_size == len ( aggregate ) : yield aggregate else : _logger . warning ( f \"WARNING! The length of the given dataset is not divisible by {self.batch_size}. There are {len(aggregate)} inputs remaining in the buffer.\" ) def __iter__ ( self ) : \"\"\" Creates the input data generator. Returns: a generator \"\"\" if 0 < self . batch_size : return self . _batch () else : return self . _limit ()","title":"TimeSeriesStream"},{"location":"reference/simaticai/testing/timeseries_stream.html#ancestors-in-mro","text":"simaticai.testing.data_stream.DataStream","title":"Ancestors (in MRO)"},{"location":"reference/simaticai/testing/vca_stream.html","text":"Module simaticai.testing.vca_stream None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os import datetime import cv2 import numpy as np import uuid from pathlib import Path from simaticai.testing.data_stream import DataStream _supported_image_formats = [ 'BayerRG8' , 'BGR' ] class VCAStream ( DataStream ): \"\"\" This class creates a generator from a folder of images. The generate function returns a generator that walks over the image folder and converts each image into the specified format, BayerRG8 by default. The resulting object is in the ImageSet format, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , data : os . PathLike , variable_name : str = 'vision_payload' , image_format : str = 'BayerRG8' , filter = None ): \"\"\" Creates a new VCAStream object Args: data (os.Pathlike): Path to the directory of images variable_name (str): Name of the variable to store the images (default: 'vision_payload') image_format (str): Supported image formats: 'BayerRG8' or 'BGR' filter (rglob_pattern): Pattern to filter the images (see also: pathlib.rglob()) \"\"\" self . seq = 0 self . data = data if filter is None or \"\" == filter . strip (): self . filter = \"**/*.[jJpP][pPnN][gGeE]*\" else : self . filter = filter if variable_name is None or \"\" == variable_name . strip (): self . variable_name = 'vision_payload' else : self . variable_name = variable_name if image_format not in _supported_image_formats : raise AssertionError ( f 'ERROR Provided image format is not supported. image_format must be one of { _supported_image_formats } ' ) self . image_format = image_format self . camera_id = uuid . uuid4 () def __iter__ ( self ): \"\"\" Creates the input data generator. Walks recursively the image folder and converts each image into an ImageSet variable. Returns: a generator \"\"\" for image_path in Path ( self . data ) . rglob ( self . filter ): yield self . _create_imageset ( image_path ) def _to_BGR ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] return im , width , height def _to_bayerRG8 ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] ( R , G , B ) = cv2 . split ( im ) bayerrg8 = np . zeros (( height , width ), np . uint8 ) bayerrg8 [ 0 :: 2 , 0 :: 2 ] = R [ 0 :: 2 , 1 :: 2 ] # top left bayerrg8 [ 0 :: 2 , 1 :: 2 ] = G [ 0 :: 2 , 0 :: 2 ] # top right bayerrg8 [ 1 :: 2 , 0 :: 2 ] = G [ 1 :: 2 , 1 :: 2 ] # bottom left bayerrg8 [ 1 :: 2 , 1 :: 2 ] = B [ 1 :: 2 , 0 :: 2 ] # bottom right bayerrg8 = bayerrg8 . ravel () . tobytes () return bayerrg8 , width , height def _create_imageset ( self , image_path ): timestamp = f \" { datetime . datetime . now ( datetime . timezone . utc ) . strftime ( '%Y-%m- %d T%H:%M:%S. %f ' )[: - 3 ] } Z\" if 'BayerRG8' == self . image_format : image_array , width , height = self . _to_bayerRG8 ( image_path ) else : # default to BGR image_array , width , height = self . _to_BGR ( image_path ) result = { self . variable_name : { 'version' : '1' , 'cameraid' : str ( self . camera_id ), 'timestamp' : timestamp , 'customfields' : '' , 'detail' : [{ 'id' : f 'VCA Stream : { image_path } ' , 'seq' : self . seq , 'timestamp' : timestamp , 'format' : self . image_format , 'width' : width , 'height' : height , 'metadata' : '{\"ptpstatus\":\"Disabled\",\"ptptimestamp\":\"0\"}' , 'image' : image_array , }]} } self . seq += 1 return result Classes VCAStream This class creates a generator from a folder of images. The generate function returns a generator that walks over the image folder and converts each image into the specified format, BayerRG8 by default. The resulting object is in the ImageSet format, as if it were received from AI Inference Server. class VCAStream ( data : os . PathLike , variable_name : str = 'vision_payload' , image_format : str = 'BayerRG8' , filter = None ) View Source class VCAStream ( DataStream ): \"\"\" This class creates a generator from a folder of images. The generate function returns a generator that walks over the image folder and converts each image into the specified format, BayerRG8 by default. The resulting object is in the ImageSet format, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , data : os . PathLike , variable_name : str = 'vision_payload' , image_format : str = 'BayerRG8' , filter = None ): \"\"\" Creates a new VCAStream object Args: data (os.Pathlike): Path to the directory of images variable_name (str): Name of the variable to store the images (default: 'vision_payload') image_format (str): Supported image formats: 'BayerRG8' or 'BGR' filter (rglob_pattern): Pattern to filter the images (see also: pathlib.rglob()) \"\"\" self . seq = 0 self . data = data if filter is None or \"\" == filter . strip (): self . filter = \"**/*.[jJpP][pPnN][gGeE]*\" else : self . filter = filter if variable_name is None or \"\" == variable_name . strip (): self . variable_name = 'vision_payload' else : self . variable_name = variable_name if image_format not in _supported_image_formats : raise AssertionError ( f 'ERROR Provided image format is not supported. image_format must be one of {_supported_image_formats}' ) self . image_format = image_format self . camera_id = uuid . uuid4 () def __iter__ ( self ): \"\"\" Creates the input data generator. Walks recursively the image folder and converts each image into an ImageSet variable. Returns: a generator \"\"\" for image_path in Path ( self . data ) . rglob ( self . filter ): yield self . _create_imageset ( image_path ) def _to_BGR ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] return im , width , height def _to_bayerRG8 ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] ( R , G , B ) = cv2 . split ( im ) bayerrg8 = np . zeros (( height , width ), np . uint8 ) bayerrg8 [ 0 :: 2 , 0 :: 2 ] = R [ 0 :: 2 , 1 :: 2 ] # top left bayerrg8 [ 0 :: 2 , 1 :: 2 ] = G [ 0 :: 2 , 0 :: 2 ] # top right bayerrg8 [ 1 :: 2 , 0 :: 2 ] = G [ 1 :: 2 , 1 :: 2 ] # bottom left bayerrg8 [ 1 :: 2 , 1 :: 2 ] = B [ 1 :: 2 , 0 :: 2 ] # bottom right bayerrg8 = bayerrg8 . ravel () . tobytes () return bayerrg8 , width , height def _create_imageset ( self , image_path ): timestamp = f \"{datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m- %d T%H:%M:%S. %f ')[:-3]}Z\" if 'BayerRG8' == self . image_format : image_array , width , height = self . _to_bayerRG8 ( image_path ) else : # default to BGR image_array , width , height = self . _to_BGR ( image_path ) result = { self . variable_name : { 'version' : '1' , 'cameraid' : str ( self . camera_id ), 'timestamp' : timestamp , 'customfields' : '' , 'detail' : [{ 'id' : f 'VCA Stream : {image_path}' , 'seq' : self . seq , 'timestamp' : timestamp , 'format' : self . image_format , 'width' : width , 'height' : height , 'metadata' : '{\"ptpstatus\":\"Disabled\",\"ptptimestamp\":\"0\"}' , 'image' : image_array , }]} } self . seq += 1 return result Ancestors (in MRO) simaticai.testing.data_stream.DataStream","title":"Vca Stream"},{"location":"reference/simaticai/testing/vca_stream.html#module-simaticaitestingvca_stream","text":"None None View Source # Copyright (C) Siemens AG 2021. All Rights Reserved. Confidential. import os import datetime import cv2 import numpy as np import uuid from pathlib import Path from simaticai.testing.data_stream import DataStream _supported_image_formats = [ 'BayerRG8' , 'BGR' ] class VCAStream ( DataStream ): \"\"\" This class creates a generator from a folder of images. The generate function returns a generator that walks over the image folder and converts each image into the specified format, BayerRG8 by default. The resulting object is in the ImageSet format, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , data : os . PathLike , variable_name : str = 'vision_payload' , image_format : str = 'BayerRG8' , filter = None ): \"\"\" Creates a new VCAStream object Args: data (os.Pathlike): Path to the directory of images variable_name (str): Name of the variable to store the images (default: 'vision_payload') image_format (str): Supported image formats: 'BayerRG8' or 'BGR' filter (rglob_pattern): Pattern to filter the images (see also: pathlib.rglob()) \"\"\" self . seq = 0 self . data = data if filter is None or \"\" == filter . strip (): self . filter = \"**/*.[jJpP][pPnN][gGeE]*\" else : self . filter = filter if variable_name is None or \"\" == variable_name . strip (): self . variable_name = 'vision_payload' else : self . variable_name = variable_name if image_format not in _supported_image_formats : raise AssertionError ( f 'ERROR Provided image format is not supported. image_format must be one of { _supported_image_formats } ' ) self . image_format = image_format self . camera_id = uuid . uuid4 () def __iter__ ( self ): \"\"\" Creates the input data generator. Walks recursively the image folder and converts each image into an ImageSet variable. Returns: a generator \"\"\" for image_path in Path ( self . data ) . rglob ( self . filter ): yield self . _create_imageset ( image_path ) def _to_BGR ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] return im , width , height def _to_bayerRG8 ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] ( R , G , B ) = cv2 . split ( im ) bayerrg8 = np . zeros (( height , width ), np . uint8 ) bayerrg8 [ 0 :: 2 , 0 :: 2 ] = R [ 0 :: 2 , 1 :: 2 ] # top left bayerrg8 [ 0 :: 2 , 1 :: 2 ] = G [ 0 :: 2 , 0 :: 2 ] # top right bayerrg8 [ 1 :: 2 , 0 :: 2 ] = G [ 1 :: 2 , 1 :: 2 ] # bottom left bayerrg8 [ 1 :: 2 , 1 :: 2 ] = B [ 1 :: 2 , 0 :: 2 ] # bottom right bayerrg8 = bayerrg8 . ravel () . tobytes () return bayerrg8 , width , height def _create_imageset ( self , image_path ): timestamp = f \" { datetime . datetime . now ( datetime . timezone . utc ) . strftime ( '%Y-%m- %d T%H:%M:%S. %f ' )[: - 3 ] } Z\" if 'BayerRG8' == self . image_format : image_array , width , height = self . _to_bayerRG8 ( image_path ) else : # default to BGR image_array , width , height = self . _to_BGR ( image_path ) result = { self . variable_name : { 'version' : '1' , 'cameraid' : str ( self . camera_id ), 'timestamp' : timestamp , 'customfields' : '' , 'detail' : [{ 'id' : f 'VCA Stream : { image_path } ' , 'seq' : self . seq , 'timestamp' : timestamp , 'format' : self . image_format , 'width' : width , 'height' : height , 'metadata' : '{\"ptpstatus\":\"Disabled\",\"ptptimestamp\":\"0\"}' , 'image' : image_array , }]} } self . seq += 1 return result","title":"Module simaticai.testing.vca_stream"},{"location":"reference/simaticai/testing/vca_stream.html#classes","text":"","title":"Classes"},{"location":"reference/simaticai/testing/vca_stream.html#vcastream","text":"This class creates a generator from a folder of images. The generate function returns a generator that walks over the image folder and converts each image into the specified format, BayerRG8 by default. The resulting object is in the ImageSet format, as if it were received from AI Inference Server. class VCAStream ( data : os . PathLike , variable_name : str = 'vision_payload' , image_format : str = 'BayerRG8' , filter = None ) View Source class VCAStream ( DataStream ): \"\"\" This class creates a generator from a folder of images. The generate function returns a generator that walks over the image folder and converts each image into the specified format, BayerRG8 by default. The resulting object is in the ImageSet format, as if it were received from AI Inference Server. \"\"\" def __init__ ( self , data : os . PathLike , variable_name : str = 'vision_payload' , image_format : str = 'BayerRG8' , filter = None ): \"\"\" Creates a new VCAStream object Args: data (os.Pathlike): Path to the directory of images variable_name (str): Name of the variable to store the images (default: 'vision_payload') image_format (str): Supported image formats: 'BayerRG8' or 'BGR' filter (rglob_pattern): Pattern to filter the images (see also: pathlib.rglob()) \"\"\" self . seq = 0 self . data = data if filter is None or \"\" == filter . strip (): self . filter = \"**/*.[jJpP][pPnN][gGeE]*\" else : self . filter = filter if variable_name is None or \"\" == variable_name . strip (): self . variable_name = 'vision_payload' else : self . variable_name = variable_name if image_format not in _supported_image_formats : raise AssertionError ( f 'ERROR Provided image format is not supported. image_format must be one of {_supported_image_formats}' ) self . image_format = image_format self . camera_id = uuid . uuid4 () def __iter__ ( self ): \"\"\" Creates the input data generator. Walks recursively the image folder and converts each image into an ImageSet variable. Returns: a generator \"\"\" for image_path in Path ( self . data ) . rglob ( self . filter ): yield self . _create_imageset ( image_path ) def _to_BGR ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] return im , width , height def _to_bayerRG8 ( self , image_path ): im = cv2 . imread ( str ( image_path )) ( height , width ) = im . shape [: 2 ] ( R , G , B ) = cv2 . split ( im ) bayerrg8 = np . zeros (( height , width ), np . uint8 ) bayerrg8 [ 0 :: 2 , 0 :: 2 ] = R [ 0 :: 2 , 1 :: 2 ] # top left bayerrg8 [ 0 :: 2 , 1 :: 2 ] = G [ 0 :: 2 , 0 :: 2 ] # top right bayerrg8 [ 1 :: 2 , 0 :: 2 ] = G [ 1 :: 2 , 1 :: 2 ] # bottom left bayerrg8 [ 1 :: 2 , 1 :: 2 ] = B [ 1 :: 2 , 0 :: 2 ] # bottom right bayerrg8 = bayerrg8 . ravel () . tobytes () return bayerrg8 , width , height def _create_imageset ( self , image_path ): timestamp = f \"{datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m- %d T%H:%M:%S. %f ')[:-3]}Z\" if 'BayerRG8' == self . image_format : image_array , width , height = self . _to_bayerRG8 ( image_path ) else : # default to BGR image_array , width , height = self . _to_BGR ( image_path ) result = { self . variable_name : { 'version' : '1' , 'cameraid' : str ( self . camera_id ), 'timestamp' : timestamp , 'customfields' : '' , 'detail' : [{ 'id' : f 'VCA Stream : {image_path}' , 'seq' : self . seq , 'timestamp' : timestamp , 'format' : self . image_format , 'width' : width , 'height' : height , 'metadata' : '{\"ptpstatus\":\"Disabled\",\"ptptimestamp\":\"0\"}' , 'image' : image_array , }]} } self . seq += 1 return result","title":"VCAStream"},{"location":"reference/simaticai/testing/vca_stream.html#ancestors-in-mro","text":"simaticai.testing.data_stream.DataStream","title":"Ancestors (in MRO)"}]}